{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66edca07-f26f-46a8-be72-d71947390541",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "I start by importing the full set of libraries and internal project modules needed for the end-to-end workflow: data acquisition, cleaning, PostgreSQL I/O, panel regression, and reporting. Centralizing imports up front keeps the pipeline reproducible and makes each downstream cell single-purpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db033aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import io\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import math\n",
    "import logging\n",
    "from pathlib import Path    \n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timezone\n",
    "import subprocess\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import hashlib\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "from linearmodels import PanelOLS\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy import text\n",
    "import psycopg\n",
    "from psycopg.rows import dict_row\n",
    "\n",
    "from reportlab.lib.pagesizes import LETTER\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.platypus import (\n",
    "    SimpleDocTemplate, Paragraph, Spacer, PageBreak, Image,\n",
    "    Table, TableStyle, Preformatted, KeepTogether\n",
    ")\n",
    "from reportlab.pdfbase import pdfmetrics\n",
    "from reportlab.pdfbase.ttfonts import TTFont\n",
    "\n",
    "from dc_prices.config import ProjectPaths, load_settings\n",
    "from dc_prices.http import make_cached_session\n",
    "from dc_prices.eia import fetch_eia_v2_all_pages\n",
    "from dc_prices.fractracker import load_fractracker_csv\n",
    "from dc_prices.qa import qa_overview, qa_column_profile, assert_required_columns, check_state_codes\n",
    "from dc_prices.eda import write_profile_report\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14356584-f282-4341-be36-7f2c55d20645",
   "metadata": {},
   "source": [
    "## Project configuration\n",
    "\n",
    "I initialize consistent project paths (raw/interim/outputs/reports) and load runtime settings (e.g., API paging and timeouts) from a single configuration layer. I also set up a cached HTTP session so repeated API pulls are deterministic and less likely to trigger rate/timeout issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6de28c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-10 17:17:51,854 | INFO | PROJECT_ROOT = C:\\Users\\amand\\OneDrive\\Documents\\School\\Assignments\\WGU\\Data-Engineering\\Capstone\\datacenter-electricity-prices-us\n",
      "2026-01-10 17:17:51,860 | INFO | SQL_DIR       = C:\\Users\\amand\\OneDrive\\Documents\\School\\Assignments\\WGU\\Data-Engineering\\Capstone\\datacenter-electricity-prices-us\\sql\n"
     ]
    }
   ],
   "source": [
    "# Cell: Logging + project paths\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(\"capstone\")\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent \n",
    "\n",
    "# Define directories relative to the true Project Root\n",
    "SQL_DIR = PROJECT_ROOT / \"sql\"\n",
    "REPORTS_DIR = PROJECT_ROOT / \"reports\"\n",
    "FIG_DIR = REPORTS_DIR / \"figures\"\n",
    "TABLE_DIR = REPORTS_DIR / \"tables\"\n",
    "DATA_RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "FRAC_PATH = DATA_RAW / \"Data_Centers_Database - FracTracker Data Centers.csv\"\n",
    "\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TABLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logger.info(f\"PROJECT_ROOT = {PROJECT_ROOT}\")\n",
    "logger.info(f\"SQL_DIR       = {SQL_DIR}\")\n",
    "\n",
    "REPO_ROOT = Path.cwd().parents[0]\n",
    "paths = ProjectPaths.from_repo_root(REPO_ROOT)\n",
    "paths.data_interim.mkdir(parents=True, exist_ok=True)\n",
    "paths.reports_qa.mkdir(parents=True, exist_ok=True)\n",
    "paths.reports_eda.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "settings = load_settings()\n",
    "session = make_cached_session(cache_name=str(paths.data_interim / \"requests_cache\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0eb3af-8578-46db-8ed7-364c4f1bc64a",
   "metadata": {},
   "source": [
    "## Database connectivity\n",
    "\n",
    "I build the PostgreSQL connection using environment variables so credentials never live in the notebook. I then validate connectivity and define lightweight SQL helpers that let me execute small verification queries as I move from raw ingestion to curated marts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdce93b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql+psycopg://postgres:***@localhost:5432/dc_capstone\n",
      "PostgreSQL 18.1 on x86_64-windows, compiled by msvc-19.44.35221, 64-bit\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ROOT = Path.cwd()\n",
    "load_dotenv(PROJECT_ROOT / \".env\", override=True)\n",
    "\n",
    "# Build URL programmatically (avoids URL-encoding issues)\n",
    "\n",
    "\n",
    "url = sa.URL.create(\n",
    "    \"postgresql+psycopg\",\n",
    "    username=os.environ[\"PGUSER\"],\n",
    "    password=os.environ[\"PGPASSWORD\"],\n",
    "    host=os.environ.get(\"PGHOST\", \"localhost\"),\n",
    "    port=int(os.environ.get(\"PGPORT\", \"5432\")),\n",
    "    database=os.environ[\"PGDATABASE\"],\n",
    ")\n",
    "print(url)\n",
    "engine = sa.create_engine(url, pool_pre_ping=True, future=True)\n",
    "\n",
    "# Quick connectivity test\n",
    "with engine.connect() as conn:\n",
    "    v = conn.execute(text(\"select version();\")).scalar_one()\n",
    "print(v)\n",
    "\n",
    "def sql_exec(stmt: str, params: dict | None = None):\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(text(stmt), params or {})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd36a94-af7b-48f3-961f-84913b3a50cf",
   "metadata": {},
   "source": [
    "## Database schema bootstrap\n",
    "\n",
    "I initialize the database objects from my SQL DDL (schemas/tables/indexes) to ensure the database is the system of record for the pipeline. I intentionally execute the DDL programmatically so the environment can be rebuilt consistently from scratch when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "976eda36-f16f-4d00-b9db-427c0d842c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDL: 00_schemas.sql  (statements: 3)\n",
      "Running DDL: 01_raw_tables.sql  (statements: 9)\n",
      "Running DDL: 02_stage_tables.sql  (statements: 8)\n",
      "Running DDL: 03_mart_dimensions.sql  (statements: 4)\n",
      "Running DDL: 04_mart_facts.sql  (statements: 5)\n",
      "Running DDL: 05_mart_analysis.sql  (statements: 2)\n",
      "✅ DDL bootstrap complete (schemas + raw/stage/mart objects ensured).\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Bootstrap: ALWAYS run DDL (sql/ddl) near the top of the notebook\n",
    "# =========================\n",
    "assert \"engine\" in globals(), \"Define your SQLAlchemy `engine` before running DDL bootstrap.\"\n",
    "assert \"SQL_DIR\" in globals(), \"Define `SQL_DIR` (e.g., PROJECT_ROOT / 'sql') before running DDL bootstrap.\"\n",
    "\n",
    "SQL_DDL_DIR = Path(SQL_DIR) / \"ddl\"\n",
    "\n",
    "DDL_SQL_FILES = [\n",
    "    \"00_schemas.sql\",\n",
    "    \"01_raw_tables.sql\",\n",
    "    \"02_stage_tables.sql\",\n",
    "    \"03_mart_dimensions.sql\",\n",
    "    \"04_mart_facts.sql\",\n",
    "    \"05_mart_analysis.sql\",\n",
    "]\n",
    "\n",
    "_BEGIN = re.compile(r\"(?im)^\\s*BEGIN;\\s*$\")\n",
    "_COMMIT = re.compile(r\"(?im)^\\s*COMMIT;\\s*$\")\n",
    "_PCT = re.compile(r\"%(?![sbt%])\", flags=re.IGNORECASE)  # escape stray % for psycopg\n",
    "\n",
    "def _split_sql_statements(sql: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Split SQL on semicolons while respecting:\n",
    "      - single-quoted strings\n",
    "      - dollar-quoted blocks ($$...$$ or $tag$...$tag$)\n",
    "      - -- line comments\n",
    "      - /* block comments */\n",
    "    \"\"\"\n",
    "    stmts, buf = [], []\n",
    "    i, n = 0, len(sql)\n",
    "\n",
    "    in_sq = False\n",
    "    in_line_comment = False\n",
    "    in_block_comment = False\n",
    "    dollar_tag = None\n",
    "\n",
    "    def startswith_at(s: str, pos: int) -> bool:\n",
    "        return sql.startswith(s, pos)\n",
    "\n",
    "    while i < n:\n",
    "        ch = sql[i]\n",
    "\n",
    "        # line comments\n",
    "        if in_line_comment:\n",
    "            buf.append(ch)\n",
    "            if ch == \"\\n\":\n",
    "                in_line_comment = False\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # block comments\n",
    "        if in_block_comment:\n",
    "            buf.append(ch)\n",
    "            if startswith_at(\"*/\", i):\n",
    "                buf.append(\"*\")\n",
    "                i += 2\n",
    "                in_block_comment = False\n",
    "            else:\n",
    "                i += 1\n",
    "            continue\n",
    "        if not in_sq and dollar_tag is None:\n",
    "            if startswith_at(\"--\", i):\n",
    "                buf.append(\"--\")\n",
    "                i += 2\n",
    "                in_line_comment = True\n",
    "                continue\n",
    "            if startswith_at(\"/*\", i):\n",
    "                buf.append(\"/*\")\n",
    "                i += 2\n",
    "                in_block_comment = True\n",
    "                continue\n",
    "\n",
    "        # dollar-quote start/end (only when not in single quotes)\n",
    "        if not in_sq:\n",
    "            if dollar_tag is None and ch == \"$\":\n",
    "                j = i + 1\n",
    "                while j < n and sql[j] != \"$\" and (sql[j].isalnum() or sql[j] == \"_\"):\n",
    "                    j += 1\n",
    "                if j < n and sql[j] == \"$\":\n",
    "                    dollar_tag = sql[i : j + 1]  # e.g., $$ or $func$\n",
    "                    buf.append(dollar_tag)\n",
    "                    i = j + 1\n",
    "                    continue\n",
    "            elif dollar_tag is not None and startswith_at(dollar_tag, i):\n",
    "                buf.append(dollar_tag)\n",
    "                i += len(dollar_tag)\n",
    "                dollar_tag = None\n",
    "                continue\n",
    "\n",
    "        # single quotes\n",
    "        if dollar_tag is None and ch == \"'\":\n",
    "            buf.append(ch)\n",
    "            if in_sq:\n",
    "                # escaped ''\n",
    "                if i + 1 < n and sql[i + 1] == \"'\":\n",
    "                    buf.append(\"'\")\n",
    "                    i += 2\n",
    "                    continue\n",
    "                in_sq = False\n",
    "            else:\n",
    "                in_sq = True\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # statement terminator\n",
    "        if ch == \";\" and (not in_sq) and (dollar_tag is None):\n",
    "            stmt = \"\".join(buf).strip()\n",
    "            if stmt:\n",
    "                stmts.append(stmt)\n",
    "            buf = []\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        buf.append(ch)\n",
    "        i += 1\n",
    "\n",
    "    tail = \"\".join(buf).strip()\n",
    "    if tail:\n",
    "        stmts.append(tail)\n",
    "\n",
    "    return stmts\n",
    "\n",
    "\n",
    "def run_sql_file_bootstrap(path: Path) -> None:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing SQL file: {path.resolve()}\")\n",
    "\n",
    "    sql_text = path.read_text(encoding=\"utf-8-sig\")  # strips BOM if present\n",
    "    sql_text = _BEGIN.sub(\"\", sql_text)\n",
    "    sql_text = _COMMIT.sub(\"\", sql_text)\n",
    "    sql_text = _PCT.sub(\"%%\", sql_text)\n",
    "\n",
    "    statements = _split_sql_statements(sql_text)\n",
    "\n",
    "    print(f\"Running DDL: {path.name}  (statements: {len(statements)})\")\n",
    "    with engine.begin() as conn:\n",
    "        conn.exec_driver_sql(\"SET search_path TO public;\")\n",
    "\n",
    "        for k, stmt in enumerate(statements, start=1):\n",
    "            s = stmt.strip()\n",
    "            if not s or s.startswith(\"\\\\\"):  # ignore psql meta-commands if any\n",
    "                continue\n",
    "            try:\n",
    "                conn.exec_driver_sql(stmt)\n",
    "            except Exception as e:\n",
    "                preview = (stmt[:500] + \" ...\") if len(stmt) > 500 else stmt\n",
    "                raise RuntimeError(\n",
    "                    f\"DDL failed in {path.name} (statement {k}/{len(statements)}):\\n{preview}\"\n",
    "                ) from e\n",
    "\n",
    "\n",
    "# Run DDL scripts in a deterministic order\n",
    "for fname in DDL_SQL_FILES:\n",
    "    run_sql_file_bootstrap(SQL_DDL_DIR / fname)\n",
    "\n",
    "print(\"✅ DDL bootstrap complete (schemas + raw/stage/mart objects ensured).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8896cd-ebbc-48e5-ae4f-3d4e23467e3d",
   "metadata": {},
   "source": [
    "## Ingest data center facilities (FracTracker)\n",
    "\n",
    "I load the FracTracker data center facilities CSV into a raw DataFrame and validate that the structural keys I depend on (facility ID, state, coordinates, status) are present. This is my facility-level source that will later be aggregated into state-level data center load intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62bf5e27-add4-4564-8027-d817db948916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'fractracker_datacenters',\n",
       " 'rows': 1260,\n",
       " 'cols': 40,\n",
       " 'duplicate_rows': 0,\n",
       " 'missing_cells': 25840,\n",
       " 'missing_pct': 0.5126984126984127}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fractracker_csv = paths.data_raw / \"Data_Centers_Database - FracTracker Data Centers.csv\"\n",
    "FRAC_PATH = fractracker_csv\n",
    "df_dc = load_fractracker_csv(fractracker_csv)\n",
    "\n",
    "# Basic structural expectations\n",
    "required_dc_cols = [\n",
    "    \"facility_id\", \"name\", \"state\", \"status\", \"county\", \"lat\", \"long\"\n",
    "]\n",
    "assert_required_columns(df_dc, required_dc_cols, \"FracTracker Data Centers\")\n",
    "\n",
    "overview_dc = qa_overview(df_dc, \"fractracker_datacenters\")\n",
    "overview_dc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be87910-a740-440c-bd30-a6462c5dc34d",
   "metadata": {},
   "source": [
    "## Facility profiling\n",
    "\n",
    "I generate a column-level profile and explicitly inspect missingness for the MW- and size-related fields that drive my exposure measures. This gives me an evidence-based view of where imputation is required and which features are available to support it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a48d401-923d-46e8-b7a5-82dd46de6e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mw_high missing %: 0.6801587301587302\n",
      "facility_size_sq_ft missing %: 0.3761904761904762\n",
      "sizerank missing %: 0.0\n",
      "sizerank_numeric missing %: 0.002380952380952381\n"
     ]
    }
   ],
   "source": [
    "dc_col_profile = qa_column_profile(df_dc)\n",
    "dc_col_profile.head(20)\n",
    "df_dc[\"status\"].value_counts(dropna=False).head(25)\n",
    "for col in [\"mw_high\", \"facility_size_sq_ft\", \"sizerank\", \"sizerank_numeric\"]:\n",
    "    if col in df_dc.columns:\n",
    "        print(col, \"missing %:\", float(df_dc[col].isna().mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21294466-899f-434e-bf7a-f662dece5880",
   "metadata": {},
   "source": [
    "## Persist raw facilities + QA evidence\n",
    "\n",
    "I save a raw parquet snapshot of the facility dataset and write QA summaries to disk. These artifacts make the pipeline auditable: I can prove what I ingested and what the data quality looked like before any transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3f76100-2589-4a86-b36e-a296b0d7e069",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_dc\n",
    " .to_parquet(paths.data_interim / \"fractracker_datacenters_raw.parquet\", index=False))\n",
    "\n",
    "dc_col_profile.to_csv(paths.reports_qa / \"fractracker_column_profile.csv\")\n",
    "\n",
    "with open(paths.reports_qa / \"fractracker_overview.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(overview_dc, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c033e9f-4d6f-4b1c-a98e-214903176e7d",
   "metadata": {},
   "source": [
    "## Extract retail electricity outcomes (EIA API)\n",
    "\n",
    "I pull monthly retail sales metrics from the EIA API for the full state list and the project date window. This dataset provides my primary outcome measure—residential price (cents/kWh)—along with sales, revenue, and customer counts at a consistent reporting grain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a75572b-3ab0-47d0-b121-d4d7fa45f734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rows': 10404, 'total_reported': 10404, 'cols': 13}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EIA_RETAIL_URL = \"https://api.eia.gov/v2/electricity/retail-sales/data/\"\n",
    "\n",
    "retail_params = {\n",
    "    \"frequency\": \"monthly\",\n",
    "    \"data[0]\": \"customers\",\n",
    "    \"data[1]\": \"price\",\n",
    "    \"data[2]\": \"revenue\",\n",
    "    \"data[3]\": \"sales\",\n",
    "    \"start\": \"2023-01\",\n",
    "    \"end\": \"2025-10\",\n",
    "    \"sort[0][column]\": \"period\",\n",
    "    \"sort[0][direction]\": \"asc\",  \n",
    "    \"facets[stateid][]\": [\"AK\",\"AL\",\"AR\",\"AZ\",\"CA\",\"CO\",\"CT\",\"DC\",\"DE\",\"FL\",\"GA\",\"HI\",\"IA\",\"ID\",\"IL\",\"IN\",\"KS\",\"KY\",\"LA\",\n",
    "                          \"MA\",\"MD\",\"ME\",\"MI\",\"MN\",\"MO\",\"MS\",\"MT\",\"NC\",\"ND\",\"NE\",\"NH\",\"NJ\",\"NM\",\"NV\",\"NY\",\"OH\",\"OK\",\"OR\",\n",
    "                          \"PA\",\"RI\",\"SC\",\"SD\",\"TN\",\"TX\",\"UT\",\"VA\",\"VT\",\"WA\",\"WI\",\"WV\",\"WY\"],\n",
    "}\n",
    "\n",
    "retail_res = fetch_eia_v2_all_pages(\n",
    "    session=session,\n",
    "    base_url=EIA_RETAIL_URL,\n",
    "    base_params=retail_params,\n",
    "    api_key=settings.eia_api_key,\n",
    "    page_size=settings.eia_page_size,\n",
    "    timeout_s=settings.eia_timeout_s,\n",
    ")\n",
    "\n",
    "df_retail = retail_res.df\n",
    "{\"rows\": len(df_retail), \"total_reported\": retail_res.total, \"cols\": df_retail.shape[1]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdd5783-4ee8-4709-98bc-9fe7d97a869c",
   "metadata": {},
   "source": [
    "## Retail extract sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da5f3f78-0fb4-4824-91ef-1cb8c6215c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period</th>\n",
       "      <th>stateid</th>\n",
       "      <th>stateDescription</th>\n",
       "      <th>sectorid</th>\n",
       "      <th>sectorName</th>\n",
       "      <th>customers</th>\n",
       "      <th>price</th>\n",
       "      <th>revenue</th>\n",
       "      <th>sales</th>\n",
       "      <th>customers-units</th>\n",
       "      <th>price-units</th>\n",
       "      <th>revenue-units</th>\n",
       "      <th>sales-units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>ALL</td>\n",
       "      <td>all sectors</td>\n",
       "      <td>352907</td>\n",
       "      <td>20.28</td>\n",
       "      <td>119.22997</td>\n",
       "      <td>587.91571</td>\n",
       "      <td>number of customers</td>\n",
       "      <td>cents per kilowatt-hour</td>\n",
       "      <td>million dollars</td>\n",
       "      <td>million kilowatt hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>COM</td>\n",
       "      <td>commercial</td>\n",
       "      <td>56637</td>\n",
       "      <td>20.22</td>\n",
       "      <td>48.06171</td>\n",
       "      <td>237.6831</td>\n",
       "      <td>number of customers</td>\n",
       "      <td>cents per kilowatt-hour</td>\n",
       "      <td>million dollars</td>\n",
       "      <td>million kilowatt hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>IND</td>\n",
       "      <td>industrial</td>\n",
       "      <td>1119</td>\n",
       "      <td>17.61</td>\n",
       "      <td>21.08354</td>\n",
       "      <td>119.73471</td>\n",
       "      <td>number of customers</td>\n",
       "      <td>cents per kilowatt-hour</td>\n",
       "      <td>million dollars</td>\n",
       "      <td>million kilowatt hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>OTH</td>\n",
       "      <td>other</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>number of customers</td>\n",
       "      <td>cents per kilowatt-hour</td>\n",
       "      <td>million dollars</td>\n",
       "      <td>million kilowatt hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>RES</td>\n",
       "      <td>residential</td>\n",
       "      <td>295151</td>\n",
       "      <td>21.73</td>\n",
       "      <td>50.08472</td>\n",
       "      <td>230.4979</td>\n",
       "      <td>number of customers</td>\n",
       "      <td>cents per kilowatt-hour</td>\n",
       "      <td>million dollars</td>\n",
       "      <td>million kilowatt hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>TRA</td>\n",
       "      <td>transportation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>number of customers</td>\n",
       "      <td>cents per kilowatt-hour</td>\n",
       "      <td>million dollars</td>\n",
       "      <td>million kilowatt hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AL</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>ALL</td>\n",
       "      <td>all sectors</td>\n",
       "      <td>2729635</td>\n",
       "      <td>11.4</td>\n",
       "      <td>803.02308</td>\n",
       "      <td>7043.38231</td>\n",
       "      <td>number of customers</td>\n",
       "      <td>cents per kilowatt-hour</td>\n",
       "      <td>million dollars</td>\n",
       "      <td>million kilowatt hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AL</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>COM</td>\n",
       "      <td>commercial</td>\n",
       "      <td>381998</td>\n",
       "      <td>13.27</td>\n",
       "      <td>227.42971</td>\n",
       "      <td>1713.24371</td>\n",
       "      <td>number of customers</td>\n",
       "      <td>cents per kilowatt-hour</td>\n",
       "      <td>million dollars</td>\n",
       "      <td>million kilowatt hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AL</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>IND</td>\n",
       "      <td>industrial</td>\n",
       "      <td>7208</td>\n",
       "      <td>6.81</td>\n",
       "      <td>172.33699</td>\n",
       "      <td>2531.80572</td>\n",
       "      <td>number of customers</td>\n",
       "      <td>cents per kilowatt-hour</td>\n",
       "      <td>million dollars</td>\n",
       "      <td>million kilowatt hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AL</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>OTH</td>\n",
       "      <td>other</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>number of customers</td>\n",
       "      <td>cents per kilowatt-hour</td>\n",
       "      <td>million dollars</td>\n",
       "      <td>million kilowatt hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AL</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>RES</td>\n",
       "      <td>residential</td>\n",
       "      <td>2340429</td>\n",
       "      <td>14.41</td>\n",
       "      <td>403.25638</td>\n",
       "      <td>2798.33288</td>\n",
       "      <td>number of customers</td>\n",
       "      <td>cents per kilowatt-hour</td>\n",
       "      <td>million dollars</td>\n",
       "      <td>million kilowatt hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AL</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>TRA</td>\n",
       "      <td>transportation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>number of customers</td>\n",
       "      <td>cents per kilowatt-hour</td>\n",
       "      <td>million dollars</td>\n",
       "      <td>million kilowatt hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AR</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>ALL</td>\n",
       "      <td>all sectors</td>\n",
       "      <td>1690490</td>\n",
       "      <td>9.54</td>\n",
       "      <td>400.75745</td>\n",
       "      <td>4199.15748</td>\n",
       "      <td>number of customers</td>\n",
       "      <td>cents per kilowatt-hour</td>\n",
       "      <td>million dollars</td>\n",
       "      <td>million kilowatt hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AR</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>COM</td>\n",
       "      <td>commercial</td>\n",
       "      <td>205193</td>\n",
       "      <td>10.01</td>\n",
       "      <td>92.2866</td>\n",
       "      <td>921.83839</td>\n",
       "      <td>number of customers</td>\n",
       "      <td>cents per kilowatt-hour</td>\n",
       "      <td>million dollars</td>\n",
       "      <td>million kilowatt hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AR</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>IND</td>\n",
       "      <td>industrial</td>\n",
       "      <td>34707</td>\n",
       "      <td>6.94</td>\n",
       "      <td>101.83575</td>\n",
       "      <td>1467.99916</td>\n",
       "      <td>number of customers</td>\n",
       "      <td>cents per kilowatt-hour</td>\n",
       "      <td>million dollars</td>\n",
       "      <td>million kilowatt hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AR</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>OTH</td>\n",
       "      <td>other</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>number of customers</td>\n",
       "      <td>cents per kilowatt-hour</td>\n",
       "      <td>million dollars</td>\n",
       "      <td>million kilowatt hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AR</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>RES</td>\n",
       "      <td>residential</td>\n",
       "      <td>1450588</td>\n",
       "      <td>11.42</td>\n",
       "      <td>206.63194</td>\n",
       "      <td>1809.29724</td>\n",
       "      <td>number of customers</td>\n",
       "      <td>cents per kilowatt-hour</td>\n",
       "      <td>million dollars</td>\n",
       "      <td>million kilowatt hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AR</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>TRA</td>\n",
       "      <td>transportation</td>\n",
       "      <td>2</td>\n",
       "      <td>13.94</td>\n",
       "      <td>.00316</td>\n",
       "      <td>.02269</td>\n",
       "      <td>number of customers</td>\n",
       "      <td>cents per kilowatt-hour</td>\n",
       "      <td>million dollars</td>\n",
       "      <td>million kilowatt hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>ALL</td>\n",
       "      <td>all sectors</td>\n",
       "      <td>3402912</td>\n",
       "      <td>11.11</td>\n",
       "      <td>691.25326</td>\n",
       "      <td>6220.41845</td>\n",
       "      <td>number of customers</td>\n",
       "      <td>cents per kilowatt-hour</td>\n",
       "      <td>million dollars</td>\n",
       "      <td>million kilowatt hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>COM</td>\n",
       "      <td>commercial</td>\n",
       "      <td>342008</td>\n",
       "      <td>10.54</td>\n",
       "      <td>246.2977</td>\n",
       "      <td>2337.83154</td>\n",
       "      <td>number of customers</td>\n",
       "      <td>cents per kilowatt-hour</td>\n",
       "      <td>million dollars</td>\n",
       "      <td>million kilowatt hours</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     period stateid stateDescription sectorid      sectorName customers  price    revenue       sales      customers-units  \\\n",
       "0   2023-01      AK           Alaska      ALL     all sectors    352907  20.28  119.22997   587.91571  number of customers   \n",
       "1   2023-01      AK           Alaska      COM      commercial     56637  20.22   48.06171    237.6831  number of customers   \n",
       "2   2023-01      AK           Alaska      IND      industrial      1119  17.61   21.08354   119.73471  number of customers   \n",
       "3   2023-01      AK           Alaska      OTH           other      None   None       None        None  number of customers   \n",
       "4   2023-01      AK           Alaska      RES     residential    295151  21.73   50.08472    230.4979  number of customers   \n",
       "5   2023-01      AK           Alaska      TRA  transportation         0      0          0           0  number of customers   \n",
       "6   2023-01      AL          Alabama      ALL     all sectors   2729635   11.4  803.02308  7043.38231  number of customers   \n",
       "7   2023-01      AL          Alabama      COM      commercial    381998  13.27  227.42971  1713.24371  number of customers   \n",
       "8   2023-01      AL          Alabama      IND      industrial      7208   6.81  172.33699  2531.80572  number of customers   \n",
       "9   2023-01      AL          Alabama      OTH           other      None   None       None        None  number of customers   \n",
       "10  2023-01      AL          Alabama      RES     residential   2340429  14.41  403.25638  2798.33288  number of customers   \n",
       "11  2023-01      AL          Alabama      TRA  transportation         0      0          0           0  number of customers   \n",
       "12  2023-01      AR         Arkansas      ALL     all sectors   1690490   9.54  400.75745  4199.15748  number of customers   \n",
       "13  2023-01      AR         Arkansas      COM      commercial    205193  10.01    92.2866   921.83839  number of customers   \n",
       "14  2023-01      AR         Arkansas      IND      industrial     34707   6.94  101.83575  1467.99916  number of customers   \n",
       "15  2023-01      AR         Arkansas      OTH           other      None   None       None        None  number of customers   \n",
       "16  2023-01      AR         Arkansas      RES     residential   1450588  11.42  206.63194  1809.29724  number of customers   \n",
       "17  2023-01      AR         Arkansas      TRA  transportation         2  13.94     .00316      .02269  number of customers   \n",
       "18  2023-01      AZ          Arizona      ALL     all sectors   3402912  11.11  691.25326  6220.41845  number of customers   \n",
       "19  2023-01      AZ          Arizona      COM      commercial    342008  10.54   246.2977  2337.83154  number of customers   \n",
       "\n",
       "                price-units    revenue-units             sales-units  \n",
       "0   cents per kilowatt-hour  million dollars  million kilowatt hours  \n",
       "1   cents per kilowatt-hour  million dollars  million kilowatt hours  \n",
       "2   cents per kilowatt-hour  million dollars  million kilowatt hours  \n",
       "3   cents per kilowatt-hour  million dollars  million kilowatt hours  \n",
       "4   cents per kilowatt-hour  million dollars  million kilowatt hours  \n",
       "5   cents per kilowatt-hour  million dollars  million kilowatt hours  \n",
       "6   cents per kilowatt-hour  million dollars  million kilowatt hours  \n",
       "7   cents per kilowatt-hour  million dollars  million kilowatt hours  \n",
       "8   cents per kilowatt-hour  million dollars  million kilowatt hours  \n",
       "9   cents per kilowatt-hour  million dollars  million kilowatt hours  \n",
       "10  cents per kilowatt-hour  million dollars  million kilowatt hours  \n",
       "11  cents per kilowatt-hour  million dollars  million kilowatt hours  \n",
       "12  cents per kilowatt-hour  million dollars  million kilowatt hours  \n",
       "13  cents per kilowatt-hour  million dollars  million kilowatt hours  \n",
       "14  cents per kilowatt-hour  million dollars  million kilowatt hours  \n",
       "15  cents per kilowatt-hour  million dollars  million kilowatt hours  \n",
       "16  cents per kilowatt-hour  million dollars  million kilowatt hours  \n",
       "17  cents per kilowatt-hour  million dollars  million kilowatt hours  \n",
       "18  cents per kilowatt-hour  million dollars  million kilowatt hours  \n",
       "19  cents per kilowatt-hour  million dollars  million kilowatt hours  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_retail.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b62535-eaa5-421b-897c-f05eb6043e8b",
   "metadata": {},
   "source": [
    "## Validate retail dataset structure\n",
    "\n",
    "I enforce required column presence and compute an overview QA summary (row counts, missingness, and basic distribution indicators). This checkpoint ensures I’m not silently modeling on incomplete or malformed API responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1736e4a6-7a9a-4f0e-90a8-ae8e9a940bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'eia_retail_sales',\n",
       " 'rows': 10404,\n",
       " 'cols': 13,\n",
       " 'duplicate_rows': 0,\n",
       " 'missing_cells': 6936,\n",
       " 'missing_pct': 0.05128205128205128}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "required_retail_cols = [\"period\",\"stateid\",\"sectorid\",\"customers\",\"price\",\"revenue\",\"sales\"]\n",
    "assert_required_columns(df_retail, required_retail_cols, \"EIA retail-sales\")\n",
    "\n",
    "overview_retail = qa_overview(df_retail, \"eia_retail_sales\")\n",
    "overview_retail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f92f6c-daea-4ccc-bb7f-97ac649aa58f",
   "metadata": {},
   "source": [
    "## Validate state identifiers\n",
    "\n",
    "I validate that state codes conform to the expected two-letter format and are within the intended domain. Keeping state identifiers clean is critical because the database joins and the panel index both depend on this key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2d17459-aee4-4554-8518-828457ddce85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: stateid, dtype: object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate state code shape\n",
    "bad_states = check_state_codes(df_retail[\"stateid\"], \"retail.stateid\")\n",
    "bad_states.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe6510b-77ce-41bd-817b-d501e1e3d2e6",
   "metadata": {},
   "source": [
    "## Confirm sector coverage\n",
    "\n",
    "I inspect sector distribution to confirm the dataset includes the sectors I expect (with residential as the primary focus). This also helps me detect any unexpected sector codes early, before loading into Postgres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29f4dcdb-e878-47d0-9709-39908a38f183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sectorid\n",
       "ALL    1734\n",
       "COM    1734\n",
       "IND    1734\n",
       "OTH    1734\n",
       "RES    1734\n",
       "TRA    1734\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sector distribution\n",
    "df_retail[\"sectorid\"].value_counts(dropna=False).head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d46d1d2-29e0-4345-9cfa-73a92d666f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>sales</th>\n",
       "      <th>revenue</th>\n",
       "      <th>customers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8670</td>\n",
       "      <td>8670</td>\n",
       "      <td>8670</td>\n",
       "      <td>8670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2097</td>\n",
       "      <td>7920</td>\n",
       "      <td>7912</td>\n",
       "      <td>6748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>750</td>\n",
       "      <td>750</td>\n",
       "      <td>750</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       price sales revenue customers\n",
       "count   8670  8670    8670      8670\n",
       "unique  2097  7920    7912      6748\n",
       "top        0     0       0         0\n",
       "freq     750   750     750       750"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic numeric sanity checks\n",
    "(df_retail[[\"price\",\"sales\",\"revenue\",\"customers\"]]\n",
    " .describe(percentiles=[0.01,0.05,0.5,0.95,0.99]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923b6dc8-aeb0-4247-bdde-864db94b370e",
   "metadata": {},
   "source": [
    "## Persist retail snapshot + QA evidence\n",
    "\n",
    "I store a parquet snapshot of the raw EIA retail dataset and write QA summaries to disk. This creates a reproducible “raw landing” record and supports traceability from model outputs back to the ingested source.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eefaa36d-a78c-4ac7-b2fb-2c4a154bcc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retail.to_parquet(paths.data_interim / \"eia_retail_sales_raw.parquet\", index=False)\n",
    "qa_column_profile(df_retail).to_csv(paths.reports_qa / \"eia_retail_column_profile.csv\")\n",
    "\n",
    "with open(paths.reports_qa / \"eia_retail_overview.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(overview_retail, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfe447f-423c-46de-bd16-a766bb400920",
   "metadata": {},
   "source": [
    "## Extract generation and fuel controls (EIA API)\n",
    "\n",
    "I pull monthly electric power operational data from the EIA API over the same state list and date window. These fields become my supply-side controls (generation and fuel cost proxies) to help separate demand-side association from supply-driven price movements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "baa54557-3322-41c2-9994-01bc03d6b87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rows': 429522, 'total_reported': 429522, 'cols': 19}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EIA_OPS_URL = \"https://api.eia.gov/v2/electricity/electric-power-operational-data/data/\"\n",
    "\n",
    "ops_params = {\n",
    "    \"frequency\": \"monthly\",\n",
    "    \"data[0]\": \"consumption-for-eg\",\n",
    "    \"data[1]\": \"consumption-for-eg-btu\",\n",
    "    \"data[2]\": \"cost\",\n",
    "    \"data[3]\": \"generation\",\n",
    "    \"data[4]\": \"total-consumption\",\n",
    "    \"data[5]\": \"total-consumption-btu\",\n",
    "    \"start\": \"2023-01\",\n",
    "    \"end\": \"2025-10\",\n",
    "    \"sort[0][column]\": \"period\",\n",
    "    \"sort[0][direction]\": \"asc\",\n",
    "    \"facets[location][]\": [\"AK\",\"AL\",\"AR\",\"AZ\",\"CA\",\"CO\",\"CT\",\"DC\",\"DE\",\"FL\",\"GA\",\"HI\",\"IA\",\"ID\",\"IL\",\"IN\",\"KS\",\"KY\",\"LA\",\n",
    "                           \"MA\",\"MD\",\"ME\",\"MI\",\"MN\",\"MO\",\"MS\",\"MT\",\"NC\",\"ND\",\"NE\",\"NH\",\"NJ\",\"NM\",\"NV\",\"NY\",\"OH\",\"OK\",\"OR\",\n",
    "                           \"PA\",\"RI\",\"SC\",\"SD\",\"TN\",\"TX\",\"UT\",\"VA\",\"VT\",\"WA\",\"WI\",\"WV\",\"WY\"],\n",
    "}\n",
    "\n",
    "ops_res = fetch_eia_v2_all_pages(\n",
    "    session=session,\n",
    "    base_url=EIA_OPS_URL,\n",
    "    base_params=ops_params,\n",
    "    api_key=settings.eia_api_key,\n",
    "    page_size=settings.eia_page_size,\n",
    "    timeout_s=settings.eia_timeout_s,\n",
    ")\n",
    "\n",
    "df_ops = ops_res.df\n",
    "{\"rows\": len(df_ops), \"total_reported\": ops_res.total, \"cols\": df_ops.shape[1]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b9e201-bab7-484e-b1b5-b5a5a2bd10f6",
   "metadata": {},
   "source": [
    "## Ops extract sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8745341f-64f3-4884-9ecf-67860a362d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period</th>\n",
       "      <th>location</th>\n",
       "      <th>stateDescription</th>\n",
       "      <th>sectorid</th>\n",
       "      <th>sectorDescription</th>\n",
       "      <th>fueltypeid</th>\n",
       "      <th>fuelTypeDescription</th>\n",
       "      <th>consumption-for-eg</th>\n",
       "      <th>consumption-for-eg-units</th>\n",
       "      <th>consumption-for-eg-btu</th>\n",
       "      <th>consumption-for-eg-btu-units</th>\n",
       "      <th>cost</th>\n",
       "      <th>cost-units</th>\n",
       "      <th>generation</th>\n",
       "      <th>generation-units</th>\n",
       "      <th>total-consumption</th>\n",
       "      <th>total-consumption-units</th>\n",
       "      <th>total-consumption-btu</th>\n",
       "      <th>total-consumption-btu-units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>1</td>\n",
       "      <td>Electric Utility</td>\n",
       "      <td>ALL</td>\n",
       "      <td>all fuels</td>\n",
       "      <td>0</td>\n",
       "      <td>thousand physical units</td>\n",
       "      <td>4.23789</td>\n",
       "      <td>million MMBtu</td>\n",
       "      <td>0</td>\n",
       "      <td>dollars per physical units</td>\n",
       "      <td>569.12167</td>\n",
       "      <td>thousand megawatthours</td>\n",
       "      <td>0</td>\n",
       "      <td>thousand physical units</td>\n",
       "      <td>4.53181</td>\n",
       "      <td>million MMBtu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>1</td>\n",
       "      <td>Electric Utility</td>\n",
       "      <td>AOR</td>\n",
       "      <td>all renewables</td>\n",
       "      <td>0</td>\n",
       "      <td>thousand physical units</td>\n",
       "      <td>.01564</td>\n",
       "      <td>million MMBtu</td>\n",
       "      <td>0</td>\n",
       "      <td>dollars per physical units</td>\n",
       "      <td>4.58276</td>\n",
       "      <td>thousand megawatthours</td>\n",
       "      <td>0</td>\n",
       "      <td>thousand physical units</td>\n",
       "      <td>.01564</td>\n",
       "      <td>million MMBtu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>1</td>\n",
       "      <td>Electric Utility</td>\n",
       "      <td>COL</td>\n",
       "      <td>coal, excluding waste coal</td>\n",
       "      <td>25.983</td>\n",
       "      <td>thousand short tons</td>\n",
       "      <td>.40381</td>\n",
       "      <td>million MMBtu</td>\n",
       "      <td>79.87</td>\n",
       "      <td>dollars per short tons</td>\n",
       "      <td>33.93734</td>\n",
       "      <td>thousand megawatthours</td>\n",
       "      <td>43.817</td>\n",
       "      <td>thousand short tons</td>\n",
       "      <td>.67434</td>\n",
       "      <td>million MMBtu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>1</td>\n",
       "      <td>Electric Utility</td>\n",
       "      <td>COW</td>\n",
       "      <td>all coal products</td>\n",
       "      <td>31.504</td>\n",
       "      <td>thousand short tons</td>\n",
       "      <td>.47116</td>\n",
       "      <td>million MMBtu</td>\n",
       "      <td>70.16</td>\n",
       "      <td>dollars per short tons</td>\n",
       "      <td>39.30983</td>\n",
       "      <td>thousand megawatthours</td>\n",
       "      <td>49.338</td>\n",
       "      <td>thousand short tons</td>\n",
       "      <td>.7417</td>\n",
       "      <td>million MMBtu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>1</td>\n",
       "      <td>Electric Utility</td>\n",
       "      <td>DFO</td>\n",
       "      <td>distillate fuel oil</td>\n",
       "      <td>90.356</td>\n",
       "      <td>thousand short tons</td>\n",
       "      <td>.51183</td>\n",
       "      <td>million MMBtu</td>\n",
       "      <td>133.34</td>\n",
       "      <td>dollars per short tons</td>\n",
       "      <td>48.04845</td>\n",
       "      <td>thousand megawatthours</td>\n",
       "      <td>92.479</td>\n",
       "      <td>thousand short tons</td>\n",
       "      <td>.52418</td>\n",
       "      <td>million MMBtu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>1</td>\n",
       "      <td>Electric Utility</td>\n",
       "      <td>DPV</td>\n",
       "      <td>estimated small scale solar photovoltaic</td>\n",
       "      <td>None</td>\n",
       "      <td>thousand physical units</td>\n",
       "      <td>None</td>\n",
       "      <td>million MMBtu</td>\n",
       "      <td>0</td>\n",
       "      <td>dollars per physical units</td>\n",
       "      <td>0</td>\n",
       "      <td>thousand megawatthours</td>\n",
       "      <td>None</td>\n",
       "      <td>thousand physical units</td>\n",
       "      <td>None</td>\n",
       "      <td>million MMBtu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>1</td>\n",
       "      <td>Electric Utility</td>\n",
       "      <td>FOS</td>\n",
       "      <td>fossil fuels</td>\n",
       "      <td>0</td>\n",
       "      <td>thousand physical units</td>\n",
       "      <td>3.72474</td>\n",
       "      <td>million MMBtu</td>\n",
       "      <td>0</td>\n",
       "      <td>dollars per physical units</td>\n",
       "      <td>419.1309</td>\n",
       "      <td>thousand megawatthours</td>\n",
       "      <td>0</td>\n",
       "      <td>thousand physical units</td>\n",
       "      <td>4.01867</td>\n",
       "      <td>million MMBtu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>1</td>\n",
       "      <td>Electric Utility</td>\n",
       "      <td>HYC</td>\n",
       "      <td>conventional hydroelectric</td>\n",
       "      <td>0</td>\n",
       "      <td>thousand physical units</td>\n",
       "      <td>.49751</td>\n",
       "      <td>million MMBtu</td>\n",
       "      <td>0</td>\n",
       "      <td>dollars per physical units</td>\n",
       "      <td>145.776</td>\n",
       "      <td>thousand megawatthours</td>\n",
       "      <td>0</td>\n",
       "      <td>thousand physical units</td>\n",
       "      <td>.49751</td>\n",
       "      <td>million MMBtu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>1</td>\n",
       "      <td>Electric Utility</td>\n",
       "      <td>LIG</td>\n",
       "      <td>lignite coal</td>\n",
       "      <td>22.417</td>\n",
       "      <td>thousand short tons</td>\n",
       "      <td>.34971</td>\n",
       "      <td>million MMBtu</td>\n",
       "      <td>79.87</td>\n",
       "      <td>dollars per short tons</td>\n",
       "      <td>27.89334</td>\n",
       "      <td>thousand megawatthours</td>\n",
       "      <td>22.417</td>\n",
       "      <td>thousand short tons</td>\n",
       "      <td>.34971</td>\n",
       "      <td>million MMBtu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>1</td>\n",
       "      <td>Electric Utility</td>\n",
       "      <td>NG</td>\n",
       "      <td>natural gas</td>\n",
       "      <td>2439.779</td>\n",
       "      <td>thousand Mcf</td>\n",
       "      <td>2.44204</td>\n",
       "      <td>million MMBtu</td>\n",
       "      <td>5.36</td>\n",
       "      <td>dollars per Mcf</td>\n",
       "      <td>292.56004</td>\n",
       "      <td>thousand megawatthours</td>\n",
       "      <td>2439.779</td>\n",
       "      <td>thousand Mcf</td>\n",
       "      <td>2.44204</td>\n",
       "      <td>million MMBtu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>1</td>\n",
       "      <td>Electric Utility</td>\n",
       "      <td>NGO</td>\n",
       "      <td>natural gas &amp; other gases</td>\n",
       "      <td>2439.779</td>\n",
       "      <td>thousand Mcf</td>\n",
       "      <td>2.44204</td>\n",
       "      <td>million MMBtu</td>\n",
       "      <td>5.36</td>\n",
       "      <td>dollars per Mcf</td>\n",
       "      <td>292.56004</td>\n",
       "      <td>thousand megawatthours</td>\n",
       "      <td>2439.779</td>\n",
       "      <td>thousand Mcf</td>\n",
       "      <td>2.44204</td>\n",
       "      <td>million MMBtu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>1</td>\n",
       "      <td>Electric Utility</td>\n",
       "      <td>OTH</td>\n",
       "      <td>other</td>\n",
       "      <td>.655</td>\n",
       "      <td>thousand physical units</td>\n",
       "      <td>0</td>\n",
       "      <td>million MMBtu</td>\n",
       "      <td>0</td>\n",
       "      <td>dollars per physical units</td>\n",
       "      <td>-.368</td>\n",
       "      <td>thousand megawatthours</td>\n",
       "      <td>.655</td>\n",
       "      <td>thousand physical units</td>\n",
       "      <td>0</td>\n",
       "      <td>million MMBtu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>1</td>\n",
       "      <td>Electric Utility</td>\n",
       "      <td>PEL</td>\n",
       "      <td>petroleum liquids</td>\n",
       "      <td>158.862</td>\n",
       "      <td>thousand barrels</td>\n",
       "      <td>.81154</td>\n",
       "      <td>million MMBtu</td>\n",
       "      <td>133.34</td>\n",
       "      <td>dollars per barrels</td>\n",
       "      <td>87.26103</td>\n",
       "      <td>thousand megawatthours</td>\n",
       "      <td>163.03</td>\n",
       "      <td>thousand barrels</td>\n",
       "      <td>.83493</td>\n",
       "      <td>million MMBtu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>1</td>\n",
       "      <td>Electric Utility</td>\n",
       "      <td>PET</td>\n",
       "      <td>petroleum</td>\n",
       "      <td>158.862</td>\n",
       "      <td>thousand barrels</td>\n",
       "      <td>.81154</td>\n",
       "      <td>million MMBtu</td>\n",
       "      <td>133.34</td>\n",
       "      <td>dollars per barrels</td>\n",
       "      <td>87.26103</td>\n",
       "      <td>thousand megawatthours</td>\n",
       "      <td>163.03</td>\n",
       "      <td>thousand barrels</td>\n",
       "      <td>.83493</td>\n",
       "      <td>million MMBtu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>1</td>\n",
       "      <td>Electric Utility</td>\n",
       "      <td>REN</td>\n",
       "      <td>renewable</td>\n",
       "      <td>0</td>\n",
       "      <td>thousand physical units</td>\n",
       "      <td>.51314</td>\n",
       "      <td>million MMBtu</td>\n",
       "      <td>0</td>\n",
       "      <td>dollars per physical units</td>\n",
       "      <td>150.35876</td>\n",
       "      <td>thousand megawatthours</td>\n",
       "      <td>0</td>\n",
       "      <td>thousand physical units</td>\n",
       "      <td>.51314</td>\n",
       "      <td>million MMBtu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>1</td>\n",
       "      <td>Electric Utility</td>\n",
       "      <td>RFO</td>\n",
       "      <td>residual fuel oil</td>\n",
       "      <td>0</td>\n",
       "      <td>thousand short tons</td>\n",
       "      <td>0</td>\n",
       "      <td>million MMBtu</td>\n",
       "      <td>0</td>\n",
       "      <td>dollars per short tons</td>\n",
       "      <td>0</td>\n",
       "      <td>thousand megawatthours</td>\n",
       "      <td>0</td>\n",
       "      <td>thousand short tons</td>\n",
       "      <td>0</td>\n",
       "      <td>million MMBtu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>1</td>\n",
       "      <td>Electric Utility</td>\n",
       "      <td>SPV</td>\n",
       "      <td>solar photovoltaic</td>\n",
       "      <td>0</td>\n",
       "      <td>thousand physical units</td>\n",
       "      <td>0</td>\n",
       "      <td>million MMBtu</td>\n",
       "      <td>0</td>\n",
       "      <td>dollars per physical units</td>\n",
       "      <td>0</td>\n",
       "      <td>thousand megawatthours</td>\n",
       "      <td>0</td>\n",
       "      <td>thousand physical units</td>\n",
       "      <td>0</td>\n",
       "      <td>million MMBtu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>1</td>\n",
       "      <td>Electric Utility</td>\n",
       "      <td>SUB</td>\n",
       "      <td>subbituminous coal</td>\n",
       "      <td>3.566</td>\n",
       "      <td>thousand short tons</td>\n",
       "      <td>.0541</td>\n",
       "      <td>million MMBtu</td>\n",
       "      <td>0</td>\n",
       "      <td>dollars per short tons</td>\n",
       "      <td>6.044</td>\n",
       "      <td>thousand megawatthours</td>\n",
       "      <td>21.4</td>\n",
       "      <td>thousand short tons</td>\n",
       "      <td>.32464</td>\n",
       "      <td>million MMBtu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>1</td>\n",
       "      <td>Electric Utility</td>\n",
       "      <td>SUN</td>\n",
       "      <td>solar</td>\n",
       "      <td>0</td>\n",
       "      <td>thousand physical units</td>\n",
       "      <td>0</td>\n",
       "      <td>million MMBtu</td>\n",
       "      <td>0</td>\n",
       "      <td>dollars per physical units</td>\n",
       "      <td>0</td>\n",
       "      <td>thousand megawatthours</td>\n",
       "      <td>0</td>\n",
       "      <td>thousand physical units</td>\n",
       "      <td>0</td>\n",
       "      <td>million MMBtu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>1</td>\n",
       "      <td>Electric Utility</td>\n",
       "      <td>TPV</td>\n",
       "      <td>estimated total solar photovoltaic</td>\n",
       "      <td>None</td>\n",
       "      <td>thousand physical units</td>\n",
       "      <td>None</td>\n",
       "      <td>million MMBtu</td>\n",
       "      <td>0</td>\n",
       "      <td>dollars per physical units</td>\n",
       "      <td>0</td>\n",
       "      <td>thousand megawatthours</td>\n",
       "      <td>None</td>\n",
       "      <td>thousand physical units</td>\n",
       "      <td>None</td>\n",
       "      <td>million MMBtu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     period location stateDescription sectorid sectorDescription fueltypeid                       fuelTypeDescription consumption-for-eg  \\\n",
       "0   2023-01       AK           Alaska        1  Electric Utility        ALL                                 all fuels                  0   \n",
       "1   2023-01       AK           Alaska        1  Electric Utility        AOR                            all renewables                  0   \n",
       "2   2023-01       AK           Alaska        1  Electric Utility        COL                coal, excluding waste coal             25.983   \n",
       "3   2023-01       AK           Alaska        1  Electric Utility        COW                         all coal products             31.504   \n",
       "4   2023-01       AK           Alaska        1  Electric Utility        DFO                       distillate fuel oil             90.356   \n",
       "5   2023-01       AK           Alaska        1  Electric Utility        DPV  estimated small scale solar photovoltaic               None   \n",
       "6   2023-01       AK           Alaska        1  Electric Utility        FOS                              fossil fuels                  0   \n",
       "7   2023-01       AK           Alaska        1  Electric Utility        HYC                conventional hydroelectric                  0   \n",
       "8   2023-01       AK           Alaska        1  Electric Utility        LIG                              lignite coal             22.417   \n",
       "9   2023-01       AK           Alaska        1  Electric Utility         NG                               natural gas           2439.779   \n",
       "10  2023-01       AK           Alaska        1  Electric Utility        NGO                 natural gas & other gases           2439.779   \n",
       "11  2023-01       AK           Alaska        1  Electric Utility        OTH                                     other               .655   \n",
       "12  2023-01       AK           Alaska        1  Electric Utility        PEL                         petroleum liquids            158.862   \n",
       "13  2023-01       AK           Alaska        1  Electric Utility        PET                                 petroleum            158.862   \n",
       "14  2023-01       AK           Alaska        1  Electric Utility        REN                                 renewable                  0   \n",
       "15  2023-01       AK           Alaska        1  Electric Utility        RFO                         residual fuel oil                  0   \n",
       "16  2023-01       AK           Alaska        1  Electric Utility        SPV                        solar photovoltaic                  0   \n",
       "17  2023-01       AK           Alaska        1  Electric Utility        SUB                        subbituminous coal              3.566   \n",
       "18  2023-01       AK           Alaska        1  Electric Utility        SUN                                     solar                  0   \n",
       "19  2023-01       AK           Alaska        1  Electric Utility        TPV        estimated total solar photovoltaic               None   \n",
       "\n",
       "   consumption-for-eg-units consumption-for-eg-btu consumption-for-eg-btu-units    cost                  cost-units generation  \\\n",
       "0   thousand physical units                4.23789                million MMBtu       0  dollars per physical units  569.12167   \n",
       "1   thousand physical units                 .01564                million MMBtu       0  dollars per physical units    4.58276   \n",
       "2       thousand short tons                 .40381                million MMBtu   79.87      dollars per short tons   33.93734   \n",
       "3       thousand short tons                 .47116                million MMBtu   70.16      dollars per short tons   39.30983   \n",
       "4       thousand short tons                 .51183                million MMBtu  133.34      dollars per short tons   48.04845   \n",
       "5   thousand physical units                   None                million MMBtu       0  dollars per physical units          0   \n",
       "6   thousand physical units                3.72474                million MMBtu       0  dollars per physical units   419.1309   \n",
       "7   thousand physical units                 .49751                million MMBtu       0  dollars per physical units    145.776   \n",
       "8       thousand short tons                 .34971                million MMBtu   79.87      dollars per short tons   27.89334   \n",
       "9              thousand Mcf                2.44204                million MMBtu    5.36             dollars per Mcf  292.56004   \n",
       "10             thousand Mcf                2.44204                million MMBtu    5.36             dollars per Mcf  292.56004   \n",
       "11  thousand physical units                      0                million MMBtu       0  dollars per physical units      -.368   \n",
       "12         thousand barrels                 .81154                million MMBtu  133.34         dollars per barrels   87.26103   \n",
       "13         thousand barrels                 .81154                million MMBtu  133.34         dollars per barrels   87.26103   \n",
       "14  thousand physical units                 .51314                million MMBtu       0  dollars per physical units  150.35876   \n",
       "15      thousand short tons                      0                million MMBtu       0      dollars per short tons          0   \n",
       "16  thousand physical units                      0                million MMBtu       0  dollars per physical units          0   \n",
       "17      thousand short tons                  .0541                million MMBtu       0      dollars per short tons      6.044   \n",
       "18  thousand physical units                      0                million MMBtu       0  dollars per physical units          0   \n",
       "19  thousand physical units                   None                million MMBtu       0  dollars per physical units          0   \n",
       "\n",
       "          generation-units total-consumption  total-consumption-units total-consumption-btu total-consumption-btu-units  \n",
       "0   thousand megawatthours                 0  thousand physical units               4.53181               million MMBtu  \n",
       "1   thousand megawatthours                 0  thousand physical units                .01564               million MMBtu  \n",
       "2   thousand megawatthours            43.817      thousand short tons                .67434               million MMBtu  \n",
       "3   thousand megawatthours            49.338      thousand short tons                 .7417               million MMBtu  \n",
       "4   thousand megawatthours            92.479      thousand short tons                .52418               million MMBtu  \n",
       "5   thousand megawatthours              None  thousand physical units                  None               million MMBtu  \n",
       "6   thousand megawatthours                 0  thousand physical units               4.01867               million MMBtu  \n",
       "7   thousand megawatthours                 0  thousand physical units                .49751               million MMBtu  \n",
       "8   thousand megawatthours            22.417      thousand short tons                .34971               million MMBtu  \n",
       "9   thousand megawatthours          2439.779             thousand Mcf               2.44204               million MMBtu  \n",
       "10  thousand megawatthours          2439.779             thousand Mcf               2.44204               million MMBtu  \n",
       "11  thousand megawatthours              .655  thousand physical units                     0               million MMBtu  \n",
       "12  thousand megawatthours            163.03         thousand barrels                .83493               million MMBtu  \n",
       "13  thousand megawatthours            163.03         thousand barrels                .83493               million MMBtu  \n",
       "14  thousand megawatthours                 0  thousand physical units                .51314               million MMBtu  \n",
       "15  thousand megawatthours                 0      thousand short tons                     0               million MMBtu  \n",
       "16  thousand megawatthours                 0  thousand physical units                     0               million MMBtu  \n",
       "17  thousand megawatthours              21.4      thousand short tons                .32464               million MMBtu  \n",
       "18  thousand megawatthours                 0  thousand physical units                     0               million MMBtu  \n",
       "19  thousand megawatthours              None  thousand physical units                  None               million MMBtu  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ops.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362ecfeb-d509-4def-9ee5-30e7a6fcc987",
   "metadata": {},
   "source": [
    "## Validate ops dataset structure\n",
    "\n",
    "I enforce required column presence and compute an overview QA summary. This ensures the operational controls I rely on later are complete enough to support modeling and don’t introduce systematic missingness at the state-month level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9fc273a-6da5-4e3a-b780-4cc29b96b489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'eia_power_ops',\n",
       " 'rows': 429522,\n",
       " 'cols': 19,\n",
       " 'duplicate_rows': 0,\n",
       " 'missing_cells': 544561,\n",
       " 'missing_pct': 0.06672790977681678}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "required_ops_cols = [\"period\",\"location\",\"sectorid\",\"fueltypeid\",\"generation\",\"cost\"]\n",
    "assert_required_columns(df_ops, required_ops_cols, \"EIA ops\")\n",
    "\n",
    "overview_ops = qa_overview(df_ops, \"eia_power_ops\")\n",
    "overview_ops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28013a30-e130-4e52-b8d0-64f89946bd54",
   "metadata": {},
   "source": [
    "## Validate state identifiers\n",
    "\n",
    "I validate that state codes conform to the expected two-letter format and are within the intended domain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac70a2ec-6586-413d-bd83-6ab45690088c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: location, dtype: object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_states_ops = check_state_codes(df_ops[\"location\"], \"ops.location\")\n",
    "bad_states_ops.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5eb726-3726-4c70-94dd-4f975321be83",
   "metadata": {},
   "source": [
    "## Confirm ops categorical domains\n",
    "\n",
    "I inspect categorical distributions (fuel type and sector identifiers) to understand the shape of the data I will later roll up into state-month control variables. This step also helps prevent silent category drift from the API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a51cb9cb-a73d-4d6d-8724-be6680701ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(sectorid\n",
       " 99    57856\n",
       " 98    47974\n",
       " 90    46470\n",
       " 94    43426\n",
       " 1     40279\n",
       " 97    35251\n",
       " 2     35242\n",
       " 96    29613\n",
       " 7     27171\n",
       " 5     16784\n",
       " 4     16357\n",
       " 3     15099\n",
       " 6     10934\n",
       " 8      5202\n",
       " 95     1864\n",
       " Name: count, dtype: int64,\n",
       " fueltypeid\n",
       " ALL    20686\n",
       " FOS    19178\n",
       " REN    18412\n",
       " NGO    18246\n",
       " NG     18110\n",
       " AOR    17895\n",
       " PET    16910\n",
       " PEL    16808\n",
       " DFO    16500\n",
       " BIO    14318\n",
       " WAS    13406\n",
       " SPV    13308\n",
       " SUN    13308\n",
       " OTH    12020\n",
       " COW    11125\n",
       " COL    11022\n",
       " HYC    10508\n",
       " TPV    10290\n",
       " TSN    10290\n",
       " DPV    10268\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ops[\"sectorid\"].value_counts(dropna=False).head(20), df_ops[\"fueltypeid\"].value_counts(dropna=False).head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c76fc68-3a01-4d6d-b33a-87f08af23540",
   "metadata": {},
   "source": [
    "## Ops distribution spot checks\n",
    "\n",
    "I run additional frequency checks on key categorical fields to confirm coverage looks reasonable and to anticipate how I will aggregate operational data into stable, interpretable controls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc0a6253-f7ab-401d-bb14-0ae0162a1140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>generation</th>\n",
       "      <th>cost</th>\n",
       "      <th>consumption-for-eg-btu</th>\n",
       "      <th>total-consumption-btu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>410246</td>\n",
       "      <td>83157</td>\n",
       "      <td>384792</td>\n",
       "      <td>384792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>122773</td>\n",
       "      <td>4426</td>\n",
       "      <td>83262</td>\n",
       "      <td>89112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>66572</td>\n",
       "      <td>65785</td>\n",
       "      <td>68305</td>\n",
       "      <td>66256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       generation   cost consumption-for-eg-btu total-consumption-btu\n",
       "count      410246  83157                 384792                384792\n",
       "unique     122773   4426                  83262                 89112\n",
       "top             0      0                      0                     0\n",
       "freq        66572  65785                  68305                 66256"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_ops[[\"generation\",\"cost\",\"consumption-for-eg-btu\",\"total-consumption-btu\"]]\n",
    " .describe(percentiles=[0.01,0.05,0.5,0.95,0.99]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da8b1a4-0c02-4bac-b2b6-6414a93a1037",
   "metadata": {},
   "source": [
    "## Persist ops snapshot + QA evidence\n",
    "\n",
    "I store a parquet snapshot of the raw operational dataset and write QA summaries. This preserves the raw pull and supports repeatable reconstruction of the downstream state-month controls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49045576-8cf3-4d85-80a8-0a007b5b682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ops.to_parquet(paths.data_interim / \"eia_power_ops_raw.parquet\", index=False)\n",
    "qa_column_profile(df_ops).to_csv(paths.reports_qa / \"eia_ops_column_profile.csv\")\n",
    "\n",
    "with open(paths.reports_qa / \"eia_ops_overview.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(overview_ops, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3074853e-5927-428d-875c-c8656e287c14",
   "metadata": {},
   "source": [
    "## Schema-safe loading utilities\n",
    "\n",
    "I define coercion helpers that convert messy numeric and integer-like fields into consistent pandas dtypes compatible with PostgreSQL. This is an engineering upgrade that reduces load failures and prevents silent type drift when the API or CSV formatting varies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "811bf6fe-a44f-4864-be39-d892240a54d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pg_table_coltypes(engine, schema: str, table: str) -> dict[str, str]:\n",
    "    q = sa.text(\"\"\"\n",
    "        SELECT column_name, data_type\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = :schema AND table_name = :table\n",
    "        ORDER BY ordinal_position;\n",
    "    \"\"\")\n",
    "    with engine.connect() as conn:\n",
    "        rows = conn.execute(q, {\"schema\": schema, \"table\": table}).fetchall()\n",
    "    return {r[0]: r[1] for r in rows}\n",
    "\n",
    "def _coerce_numeric_series(s: pd.Series) -> pd.Series:\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        return s\n",
    "    x = s.astype(\"string\").str.strip()\n",
    "    x = x.replace([\"\", \"NA\", \"N/A\", \"na\", \"n/a\", \"null\", \"NULL\", \"None\", \"NONE\", \"-\", \"—\"], pd.NA)\n",
    "    x = x.str.replace(\",\", \"\", regex=False)\n",
    "    x = x.str.replace(r\"[^\\d\\.\\-\\+]\", \"\", regex=True)\n",
    "    x = x.replace(\"\", pd.NA)\n",
    "    return pd.to_numeric(x, errors=\"coerce\")\n",
    "\n",
    "def _coerce_int_series(s: pd.Series, *, colname: str = \"\") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Convert messy integer-like values to pandas nullable Int64.\n",
    "    Handles '6.0' safely. If fractional parts exist, rounds (with warning).\n",
    "    \"\"\"\n",
    "    x = _coerce_numeric_series(s)\n",
    "\n",
    "    # If fractional values exist (e.g., 6.2), warn + round\n",
    "    frac = (x.dropna() - np.floor(x.dropna())).abs()\n",
    "    if len(frac) and (frac > 1e-9).any():\n",
    "        bad_n = int((frac > 1e-9).sum())\n",
    "        print(f\"⚠️  Warning: {colname} has {bad_n} non-integer values; rounding to nearest int for COPY.\")\n",
    "        x = x.round()\n",
    "\n",
    "    return x.astype(\"Int64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974e280a-d22e-4387-99e1-ae34352f92a1",
   "metadata": {},
   "source": [
    "## High-reliability Postgres loaders\n",
    "\n",
    "I add utilities to introspect table schemas and load DataFrames via COPY-based inserts. This approach is both faster and more schema-faithful than naive inserts, and it improves repeatability by enforcing column alignment against the database definitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "816f8101-8dd9-4b36-b47b-bd8fbeffaeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Postgres introspection helpers\n",
    "# -------------------------------\n",
    "def pg_table_columns(engine: sa.Engine, schema: str, table: str) -> list[str]:\n",
    "    q = \"\"\"\n",
    "    SELECT column_name\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_schema=:schema AND table_name=:table\n",
    "    ORDER BY ordinal_position\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        return [r[0] for r in conn.execute(sa.text(q), {\"schema\": schema, \"table\": table}).fetchall()]\n",
    "\n",
    "def pg_table_types(engine: sa.Engine, schema: str, table: str) -> dict[str, str]:\n",
    "    q = \"\"\"\n",
    "    SELECT column_name, data_type\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_schema=:schema AND table_name=:table\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        return {r[0]: r[1] for r in conn.execute(sa.text(q), {\"schema\": schema, \"table\": table}).fetchall()}\n",
    "\n",
    "# -------------------------------\n",
    "# Hashing + safe source_ref\n",
    "# -------------------------------\n",
    "def sanitize_source_ref(s: str) -> str:\n",
    "    # Remove api_key=... if present\n",
    "    return re.sub(r\"([?&])api_key=[^&]+\", r\"\\1api_key=REDACTED\", s)\n",
    "\n",
    "def row_hash(values: list[str | float | int | None]) -> str:\n",
    "    norm = \"|\".join(\"\" if v is None else str(v).strip() for v in values)\n",
    "    return hashlib.sha256(norm.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# -------------------------------\n",
    "# Robust coercion helpers\n",
    "# -------------------------------\n",
    "_NUM_CLEAN_RE = re.compile(r\"[,\\s]\") \n",
    "\n",
    "def _coerce_numeric_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Coerce a series to numeric:\n",
    "    - strips commas (e.g. \"1,200\")\n",
    "    - converts blank/NA-ish to NaN\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return s\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        return pd.to_numeric(s, errors=\"coerce\")\n",
    "    x = s.astype(\"string\").str.strip()\n",
    "    x = x.replace({\"\": pd.NA, \"NA\": pd.NA, \"N/A\": pd.NA, \"None\": pd.NA, \"null\": pd.NA, \"NULL\": pd.NA})\n",
    "    x = x.str.replace(_NUM_CLEAN_RE, \"\", regex=True)\n",
    "    return pd.to_numeric(x, errors=\"coerce\")\n",
    "\n",
    "def _coerce_int_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Coerce to pandas nullable Int64, handling things like \"6.0\" safely.\n",
    "    \"\"\"\n",
    "    x = _coerce_numeric_series(s)\n",
    "    x = np.where(pd.isna(x), np.nan, np.round(x).astype(\"float\"))\n",
    "    return pd.Series(x).astype(\"Int64\")\n",
    "\n",
    "def prepare_df_for_raw_load(\n",
    "    df: pd.DataFrame,\n",
    "    table_cols: list[str],\n",
    "    table_types: dict[str, str],\n",
    "    source_name: str,\n",
    "    source_ref: str,\n",
    "    dedupe_on: list[str] | None = None,\n",
    "    row_hash_cols: list[str] | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Align df to Postgres raw table columns and add ingestion metadata.\n",
    "    - keeps only table_cols\n",
    "    - coerces NUMERIC/INTEGER-ish columns\n",
    "    - adds source_name/source_ref/source_row_hash\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    def _normalize_col(name: str) -> str:\n",
    "        name = str(name).strip().replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "        s = re.sub(r\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", name)\n",
    "        s = re.sub(r\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", s)\n",
    "        s = re.sub(r\"__+\", \"_\", s)\n",
    "        return s.lower()\n",
    "\n",
    "    rename = {c: _normalize_col(c) for c in df2.columns}\n",
    "    df2 = df2.rename(columns=rename)\n",
    "    if table_cols:\n",
    "        alias: dict[str, str] = {}\n",
    "        for c in list(df2.columns):\n",
    "            if c in table_cols:\n",
    "                continue\n",
    "            c_no_us = c.replace(\"_\", \"\")\n",
    "            if c_no_us in table_cols and c_no_us not in df2.columns:\n",
    "                alias[c] = c_no_us\n",
    "        overrides = {\n",
    "            \"customers_units\": \"customerunits\",\n",
    "            \"state_description\": \"statedescription\",\n",
    "            \"sector_name\": \"sectorname\",\n",
    "            \"sector_description\": \"sectordescription\",\n",
    "            \"fuel_type_description\": \"fueltypedescription\",\n",
    "            \"consumption_for_eg\": \"comsumption_for_eg\",\n",
    "            \"consumption_for_eg_units\": \"comsumption_for_eg_units\",\n",
    "            \"consumption_for_eg_btu\": \"comsumption_for_eg_btu\",\n",
    "            \"consumption_for_eg_btu_units\": \"comsumption_for_eg_btu_units\",\n",
    "        }\n",
    "        for src_col, dst_col in overrides.items():\n",
    "            if src_col in df2.columns and dst_col in table_cols and dst_col not in df2.columns:\n",
    "                alias[src_col] = dst_col\n",
    "\n",
    "        if alias:\n",
    "            df2 = df2.rename(columns=alias)\n",
    "    df2[\"source_name\"] = source_name\n",
    "    df2[\"source_ref\"] = source_ref\n",
    "    business_cols = [c for c in df2.columns if c not in (\"ingested_at\", \"source_name\", \"source_ref\", \"source_row_hash\")]\n",
    "    use_hash_cols = row_hash_cols or business_cols\n",
    "    use_hash_cols = [c for c in use_hash_cols if c in df2.columns]\n",
    "    df2[\"source_row_hash\"] = df2[use_hash_cols].astype(\"string\").fillna(\"\").agg(lambda r: row_hash(list(r)), axis=1)\n",
    "\n",
    "    for c in table_cols:\n",
    "        if c not in df2.columns:\n",
    "            df2[c] = pd.NA\n",
    "    df2 = df2[table_cols].copy()\n",
    "    for col, dtype in table_types.items():\n",
    "        if col not in df2.columns:\n",
    "            continue\n",
    "        dt = dtype.lower()\n",
    "        if dt in (\"numeric\", \"double precision\", \"real\", \"decimal\"):\n",
    "            df2[col] = _coerce_numeric_series(df2[col])\n",
    "        elif dt in (\"integer\", \"bigint\", \"smallint\"):\n",
    "            df2[col] = _coerce_int_series(df2[col])\n",
    "    if dedupe_on:\n",
    "        keep = [c for c in dedupe_on if c in df2.columns]\n",
    "        if keep:\n",
    "            df2 = df2.drop_duplicates(subset=keep, keep=\"last\").copy()\n",
    "\n",
    "    return df2\n",
    "\n",
    "# -------------------------------\n",
    "# COPY loader (fast + robust)\n",
    "# -------------------------------\n",
    "def copy_df_to_postgres(\n",
    "    engine: sa.Engine,\n",
    "    schema: str,\n",
    "    table: str,\n",
    "    df: pd.DataFrame,\n",
    "    mode: str = \"append\", \n",
    "    chunk_rows: int = 50_000,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Uses psycopg3 COPY for speed. Expects df columns already aligned to table.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(f\"[COPY] No rows to load into {schema}.{table}\")\n",
    "        return\n",
    "\n",
    "    table_cols = pg_table_columns(engine, schema, table)\n",
    "    missing = [c for c in table_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"DF missing columns for COPY into {schema}.{table}: {missing}\")\n",
    "\n",
    "    # Ensure column order matches table\n",
    "    df = df[table_cols].copy()\n",
    "    def _to_csv_chunk(d: pd.DataFrame) -> str:\n",
    "        return d.to_csv(index=False, header=False, na_rep=\"\", lineterminator=\"\\n\")\n",
    "\n",
    "    raw_conn = engine.raw_connection()\n",
    "    try:\n",
    "        with raw_conn.cursor() as cur:\n",
    "            if mode == \"truncate\":\n",
    "                cur.execute(sa.text(f'TRUNCATE TABLE \"{schema}\".\"{table}\"').text)\n",
    "\n",
    "            col_list = \", \".join([f'\"{c}\"' for c in table_cols])\n",
    "            copy_sql = f'COPY \"{schema}\".\"{table}\" ({col_list}) FROM STDIN WITH (FORMAT CSV, NULL \\'\\')'\n",
    "\n",
    "            with cur.copy(copy_sql) as copy:\n",
    "                for start in range(0, len(df), chunk_rows):\n",
    "                    chunk = df.iloc[start:start+chunk_rows]\n",
    "                    copy.write(_to_csv_chunk(chunk))\n",
    "        raw_conn.commit()\n",
    "        print(f\"[COPY] Loaded {len(df):,} rows into {schema}.{table} ({mode})\")\n",
    "    finally:\n",
    "        raw_conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e6c621-b16f-4a80-8b9d-b4177ad3af91",
   "metadata": {},
   "source": [
    "## Load raw facilities to Postgres\n",
    "\n",
    "I load the facilities dataset into the raw database layer so PostgreSQL becomes the authoritative source for downstream transforms. Landing raw data in the database supports traceability and makes all later aggregation/join logic centralized and testable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "201bc14f-a896-4cc9-87db-a3e83eb0f1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COPY] Loaded 1,260 rows into raw.fractracker_datacenters (truncate)\n",
      "raw.fractracker_datacenters rowcount: 1260\n"
     ]
    }
   ],
   "source": [
    "# --- LOAD: FracTracker -> raw.fractracker_datacenters ---\n",
    "\n",
    "frac_cols  = pg_table_columns(engine, \"raw\", \"fractracker_datacenters\")\n",
    "frac_types = pg_table_types(engine, \"raw\", \"fractracker_datacenters\")\n",
    "\n",
    "assert \"df_dc\" in globals(), \"df_dc not found. Run the FracTracker CSV ingestion cell first.\"\n",
    "assert \"FRAC_PATH\" in globals(), \"FRAC_PATH not found. Add FRAC_PATH = fractracker_csv after setting fractracker_csv.\"\n",
    "\n",
    "df_dc_load = prepare_df_for_raw_load(\n",
    "    df=df_dc,\n",
    "    table_cols=frac_cols,\n",
    "    table_types=frac_types,\n",
    "    source_name=\"fractracker_csv\",\n",
    "    source_ref=str(Path(FRAC_PATH).name),\n",
    "    dedupe_on=[\"facility_id\"],\n",
    "    row_hash_cols=[\"facility_id\", \"name\", \"state\", \"county\", \"city\", \"lat\", \"long\", \"status\", \"mw_high\", \"sizerank\", \"facility_size_sqft\"],\n",
    ")\n",
    "df_dc_load['ingested_at'] = pd.Timestamp.now()\n",
    "copy_df_to_postgres(engine, \"raw\", \"fractracker_datacenters\", df_dc_load, mode=\"truncate\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    n = conn.execute(sa.text('SELECT COUNT(*) FROM raw.fractracker_datacenters')).scalar_one()\n",
    "print(\"raw.fractracker_datacenters rowcount:\", n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39c9006-4897-4c1f-bdcc-1281401bf4f4",
   "metadata": {},
   "source": [
    "## Defensive handling for retail extract naming\n",
    "\n",
    "I add a small compatibility step to ensure the retail DataFrame reference is available even if earlier variable naming differs. This keeps the pipeline resilient to notebook iteration without changing the underlying data logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bc2105d-fa41-4569-85f1-b8b47162a647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ df_retail_raw ready: (10404, 13)\n",
      "Columns: ['period', 'stateid', 'stateDescription', 'sectorid', 'sectorName', 'customers', 'price', 'revenue', 'sales', 'customers-units', 'price-units', 'revenue-units', 'sales-units'] ...\n"
     ]
    }
   ],
   "source": [
    "def _pick_first_existing_df(*names: str):\n",
    "    for n in names:\n",
    "        if n in globals() and isinstance(globals()[n], pd.DataFrame) and not globals()[n].empty:\n",
    "            return globals()[n]\n",
    "    return None\n",
    "\n",
    "def _load_cached_retail_df(raw_dir: Path) -> pd.DataFrame | None:\n",
    "    candidates = [\n",
    "        raw_dir / \"eia_retail_sales.parquet\",\n",
    "        raw_dir / \"eia_retail_sales.csv\",\n",
    "        raw_dir / \"eia_retail_sales.json\",\n",
    "        raw_dir / \"df_retail_raw.parquet\",\n",
    "        raw_dir / \"df_retail_raw.csv\",\n",
    "        raw_dir / \"df_retail_raw.json\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            if p.suffix == \".parquet\":\n",
    "                return pd.read_parquet(p)\n",
    "            if p.suffix == \".csv\":\n",
    "                return pd.read_csv(p)\n",
    "            if p.suffix == \".json\":\n",
    "                return pd.read_json(p)\n",
    "    return None\n",
    "\n",
    "# ---- Ensure df_retail_raw exists ----\n",
    "df_retail_raw = _pick_first_existing_df(\"df_retail_raw\", \"df_retail\", \"df_eia_retail\")\n",
    "\n",
    "if df_retail_raw is None:\n",
    "    cached = _load_cached_retail_df(RAW_DIR)\n",
    "    if cached is not None and not cached.empty:\n",
    "        df_retail_raw = cached\n",
    "\n",
    "if df_retail_raw is None or df_retail_raw.empty:\n",
    "    raise NameError(\n",
    "        \"df_retail_raw is not defined (and no cached retail file found in data/raw/). \"\n",
    "        \"Run your EIA Retail API extraction cell first (the one that fetches the retail endpoint), \"\n",
    "        \"or save the extracted df to data/raw/eia_retail_sales.parquet and rerun this cell.\"\n",
    "    )\n",
    "\n",
    "print(\"✅ df_retail_raw ready:\", df_retail_raw.shape)\n",
    "print(\"Columns:\", list(df_retail_raw.columns)[:20], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df65a9b-9aa6-418b-8ea9-37ca6a145e5e",
   "metadata": {},
   "source": [
    "## Load raw retail outcomes to Postgres\n",
    "\n",
    "I load the EIA retail outcomes into the raw database layer using schema-aware logic. This ensures that the state-month-sector measures are queryable and joinable in SQL, which is especially important when I construct the curated mart.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab0a7a00-e828-466a-a2bd-be3f070b293d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using retail df: df_retail_raw (10404, 13)\n",
      "[COPY] Loaded 10,404 rows into raw.eia_retail_sales (truncate)\n",
      "raw.eia_retail_sales rowcount: 10404\n"
     ]
    }
   ],
   "source": [
    "# --- LOAD: EIA Retail Sales -> raw.eia_retail_sales ---\n",
    "_NUMERIC_RE = re.compile(r\"^\\s*[-+]?(\\d+(\\.\\d+)?|\\.\\d+)\\s*$\")\n",
    "\n",
    "def _coerce_numeric_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Convert common messy numeric strings to floats:\n",
    "    - removes commas: \"1,200\" -> \"1200\"\n",
    "    - removes currency/units chars: \"$12.3\" -> \"12.3\"\n",
    "    - converts blanks/NA-like to NaN\n",
    "    Leaves already-numeric values as-is.\n",
    "    \"\"\"\n",
    "    if s.dtype.kind in \"if\":  # already numeric\n",
    "        return s\n",
    "\n",
    "    # Convert to string, preserve nulls\n",
    "    x = s.astype(\"string\")\n",
    "\n",
    "    # Normalize common null tokens\n",
    "    x = x.str.strip()\n",
    "    x = x.replace(\n",
    "        [\"\", \"NA\", \"N/A\", \"na\", \"n/a\", \"null\", \"NULL\", \"None\", \"NONE\", \"-\", \"—\"],\n",
    "        pd.NA,\n",
    "    )\n",
    "    x = x.str.replace(\",\", \"\", regex=False)\n",
    "    x = x.str.replace(r\"[^\\d\\.\\-\\+]\", \"\", regex=True)\n",
    "\n",
    "    x = x.replace(\"\", pd.NA)\n",
    "\n",
    "    return pd.to_numeric(x, errors=\"coerce\")\n",
    "# --- LOAD: EIA Retail -> raw.eia_retail_sales ---\n",
    "\n",
    "retail_cols  = pg_table_columns(engine, \"raw\", \"eia_retail_sales\")\n",
    "retail_types = pg_table_types(engine, \"raw\", \"eia_retail_sales\")\n",
    "\n",
    "df_retail_in = None\n",
    "for candidate in [\"df_retail_raw\", \"df_retail\", \"df_eia_retail\", \"df_eia_retail_sales\"]:\n",
    "    if candidate in globals():\n",
    "        df_retail_in = globals()[candidate]\n",
    "        print(\"Using retail df:\", candidate, df_retail_in.shape)\n",
    "        break\n",
    "if df_retail_in is None:\n",
    "    raise NameError(\"Could not find a retail dataframe. Expected one of: df_retail_raw, df_retail, df_eia_retail, df_eia_retail_sales\")\n",
    "\n",
    "src_ref = sanitize_source_ref(globals().get(\"EIA_RETAIL_ENDPOINT\", \"eia_api_v2_retail_sales\"))\n",
    "\n",
    "df_retail_load = prepare_df_for_raw_load(\n",
    "    df=df_retail_in,\n",
    "    table_cols=retail_cols,\n",
    "    table_types=retail_types,\n",
    "    source_name=\"eia_api_v2_retail_sales\",\n",
    "    source_ref=src_ref,\n",
    "    dedupe_on=[\"period\", \"stateid\", \"sectorid\"],\n",
    "    row_hash_cols=[\"period\", \"stateid\", \"sectorid\", \"customers\", \"price\", \"revenue\", \"sales\"],\n",
    ")\n",
    "df_retail_load['ingested_at'] = pd.Timestamp.now()\n",
    "copy_df_to_postgres(engine, \"raw\", \"eia_retail_sales\", df_retail_load, mode=\"truncate\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    n = conn.execute(sa.text('SELECT COUNT(*) FROM raw.eia_retail_sales')).scalar_one()\n",
    "print(\"raw.eia_retail_sales rowcount:\", n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad0c312-eeaf-4794-80ac-5a5bfc7429bb",
   "metadata": {},
   "source": [
    "## Load raw operational controls to Postgres\n",
    "\n",
    "I load EIA operational data into the raw layer so fuel/generation measures can be aggregated with SQL into consistent state-month controls. Keeping this logic database-centered improves reproducibility and reduces in-notebook join complexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d3f92d3-8b3b-4607-ab95-f536990f23b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ops df: df_ops (429522, 19)\n",
      "[COPY] Loaded 429,522 rows into raw.eia_power_ops (truncate)\n",
      "raw.eia_power_ops rowcount: 429522\n"
     ]
    }
   ],
   "source": [
    "# --- LOAD: EIA Ops -> raw.eia_power_ops ---\n",
    "\n",
    "ops_cols  = pg_table_columns(engine, \"raw\", \"eia_power_ops\")\n",
    "ops_types = pg_table_types(engine, \"raw\", \"eia_power_ops\")\n",
    "\n",
    "df_ops_in = None\n",
    "for candidate in [\"df_ops_raw\", \"df_ops\", \"df_eia_ops\", \"df_eia_power_ops\"]:\n",
    "    if candidate in globals():\n",
    "        df_ops_in = globals()[candidate]\n",
    "        print(\"Using ops df:\", candidate, df_ops_in.shape)\n",
    "        break\n",
    "if df_ops_in is None:\n",
    "    raise NameError(\"Could not find an ops dataframe. Expected one of: df_ops_raw, df_ops, df_eia_ops, df_eia_power_ops\")\n",
    "\n",
    "src_ref = sanitize_source_ref(globals().get(\"EIA_OPS_ENDPOINT\", \"eia_api_v2_power_ops\"))\n",
    "\n",
    "df_ops_load = prepare_df_for_raw_load(\n",
    "    df=df_ops_in,\n",
    "    table_cols=ops_cols,\n",
    "    table_types=ops_types,\n",
    "    source_name=\"eia_api_v2_power_ops\",\n",
    "    source_ref=src_ref,\n",
    "    dedupe_on=[\"period\", \"location\", \"sectorid\", \"fueltypeid\"],\n",
    "    row_hash_cols=[\"period\", \"location\", \"sectorid\", \"fueltypeid\", \"generation\", \"cost\", \"total_consumption_btu\"],\n",
    ")\n",
    "df_ops_load['ingested_at'] = pd.Timestamp.now()\n",
    "copy_df_to_postgres(engine, \"raw\", \"eia_power_ops\", df_ops_load, mode=\"truncate\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    n = conn.execute(sa.text('SELECT COUNT(*) FROM raw.eia_power_ops')).scalar_one()\n",
    "print(\"raw.eia_power_ops rowcount:\", n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dfe5ae-2d98-4314-8240-c88ef940ee06",
   "metadata": {},
   "source": [
    "## Automated profiling (raw layer)\n",
    "\n",
    "I generate profiling reports for the three raw datasets (facilities, retail, ops). These reports provide a transparent, repeatable record of distributions, missingness, and anomalies that inform the cleaning and modeling decisions downstream.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e352d711-9073-43a1-9cfe-82dd8cf43578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-10 17:20:01,007 | INFO | Pandas backend loaded 2.3.2\n",
      "2026-01-10 17:20:01,062 | INFO | Numpy backend loaded 2.3.3\n",
      "2026-01-10 17:20:01,067 | INFO | Pyspark backend NOT loaded\n",
      "2026-01-10 17:20:01,069 | INFO | Python backend loaded\n",
      "Summarize dataset:  13%|████                          | 6/45 [00:00<00:03, 12.08it/s, Describe variable: info_source_6]\n",
      "Summarize dataset:  49%|███████████▏           | 22/45 [00:00<00:00, 38.65it/s, Describe variable: property_size_acres]\u001b[A\n",
      "Summarize dataset:  64%|███████████████████████▏            | 29/45 [00:00<00:00, 44.34it/s, Describe variable: county]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:00<00:00, 69.65it/s]\u001b[A\n",
      "Summarize dataset: 100%|████████████████████████████████████████████████████| 46/46 [00:01<00:00, 45.13it/s, Completed]\n",
      "Generate report structure: 100%|█████████████████████████████████████████████████████████| 1/1 [00:33<00:00, 33.66s/it]\n",
      "Render HTML: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.51s/it]\n",
      "Export report to file: 100%|█████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 20.31it/s]\n",
      "Summarize dataset:  22%|███████                         | 4/18 [00:00<00:02,  6.78it/s, Describe variable: sales-units]\n",
      "Summarize dataset:  67%|████████████████████▋          | 12/18 [00:00<00:00, 20.20it/s, Describe variable: sales-units]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 61.70it/s]\u001b[A\n",
      "Summarize dataset: 100%|████████████████████████████████████████████████████| 19/19 [00:00<00:00, 28.04it/s, Completed]\n",
      "Generate report structure: 100%|█████████████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.18s/it]\n",
      "Render HTML: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.40it/s]\n",
      "Export report to file: 100%|█████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 23.53it/s]\n",
      "Summarize dataset:   8%|██▊                              | 2/24 [00:04<00:50,  2.31s/it, Describe variable: cost-units]\n",
      "Summarize dataset:  25%|█████▌                | 6/24 [00:08<00:17,  1.04it/s, Describe variable: total-consumption-btu]\u001b[A\n",
      " 16%|█████████████                                                                      | 3/19 [00:02<00:14,  1.11it/s]\u001b[A\n",
      " 21%|█████████████████▍                                                                 | 4/19 [00:03<00:10,  1.42it/s]\u001b[A\n",
      "Summarize dataset:  54%|████████▏      | 13/24 [00:11<00:03,  3.59it/s, Describe variable: total-consumption-btu-units]\u001b[A\n",
      "Summarize dataset:  67%|██████████     | 16/24 [00:12<00:02,  3.46it/s, Describe variable: total-consumption-btu-units]\u001b[A\n",
      "Summarize dataset:  79%|███████████▉   | 19/24 [00:13<00:01,  3.14it/s, Describe variable: total-consumption-btu-units]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:07<00:00,  2.44it/s]\u001b[A\n",
      "Summarize dataset: 100%|████████████████████████████████████████████████████| 25/25 [00:13<00:00,  1.84it/s, Completed]\n",
      "Generate report structure: 100%|█████████████████████████████████████████████████████████| 1/1 [00:18<00:00, 18.25s/it]\n",
      "Render HTML: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.56it/s]\n",
      "Export report to file: 100%|█████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.13it/s]\n"
     ]
    }
   ],
   "source": [
    "write_profile_report(df_dc, paths.reports_eda / \"profiling_fractracker_raw.html\", \"FracTracker Data Centers (Raw)\")\n",
    "write_profile_report(df_retail, paths.reports_eda / \"profiling_eia_retail_raw.html\", \"EIA Retail Sales (Raw)\", sample=300_000)\n",
    "write_profile_report(df_ops, paths.reports_eda / \"profiling_eia_ops_raw.html\", \"EIA Power Ops (Raw)\", sample=300_000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223a95fa-9281-49a4-8287-289a3393dc5f",
   "metadata": {},
   "source": [
    "## Pull raw facilities from Postgres for transformation\n",
    "\n",
    "I read the facilities table from Postgres using a schema-aligned query so the transformation stage starts from the database source of truth. This also helps ensure that the pipeline behavior remains consistent across machines/environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82495084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1260, 28)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>facility_id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>county</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>status</th>\n",
       "      <th>status_detail</th>\n",
       "      <th>expected_date_online</th>\n",
       "      <th>mw_high</th>\n",
       "      <th>sizerank</th>\n",
       "      <th>sizerank_numeric</th>\n",
       "      <th>facility_size_sqft</th>\n",
       "      <th>property_size_acres</th>\n",
       "      <th>number_of_generators</th>\n",
       "      <th>number_of_buildings</th>\n",
       "      <th>power_source</th>\n",
       "      <th>dedicated_power_plant</th>\n",
       "      <th>cooling_type</th>\n",
       "      <th>cooling_source</th>\n",
       "      <th>purpose</th>\n",
       "      <th>operator</th>\n",
       "      <th>tenant</th>\n",
       "      <th>source_row_hash</th>\n",
       "      <th>ingested_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Land Investments LLC developer</td>\n",
       "      <td>Rock Mountain Lake Rd</td>\n",
       "      <td>Bessemer</td>\n",
       "      <td>AL</td>\n",
       "      <td>35022.0</td>\n",
       "      <td>None</td>\n",
       "      <td>33.34200</td>\n",
       "      <td>-87.0341</td>\n",
       "      <td>Proposed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>Mega campus (&gt;1,000 MW)</td>\n",
       "      <td>6.0</td>\n",
       "      <td>None</td>\n",
       "      <td>675</td>\n",
       "      <td>None</td>\n",
       "      <td>18</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>162404f7428395dd34759f3fa26cea0265fc3230254203...</td>\n",
       "      <td>2026-01-10 22:18:16.481519+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>DC Blox</td>\n",
       "      <td>433 6th St S</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>AL</td>\n",
       "      <td>35233.0</td>\n",
       "      <td>None</td>\n",
       "      <td>33.50051</td>\n",
       "      <td>-86.8210</td>\n",
       "      <td>Operating</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>51ff6fa6dd976bae0c1ba4e6d6b4a57cd12c5c9b8d57d2...</td>\n",
       "      <td>2026-01-10 22:18:16.481519+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Google data center</td>\n",
       "      <td>48809 Alabama 277</td>\n",
       "      <td>Bridgeport</td>\n",
       "      <td>AL</td>\n",
       "      <td>35740.0</td>\n",
       "      <td>None</td>\n",
       "      <td>34.92019</td>\n",
       "      <td>-85.7446</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>360</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Google</td>\n",
       "      <td>None</td>\n",
       "      <td>7bf54df3590be2128eb694de6ba30d09a7b6d62ce6fa70...</td>\n",
       "      <td>2026-01-10 22:18:16.481519+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  facility_id                                     name                address        city state      zip county       lat     long  \\\n",
       "0           1  Logistic Land Investments LLC developer  Rock Mountain Lake Rd    Bessemer    AL  35022.0   None  33.34200 -87.0341   \n",
       "1           2                                  DC Blox           433 6th St S  Birmingham    AL  35233.0   None  33.50051 -86.8210   \n",
       "2           3                       Google data center      48809 Alabama 277  Bridgeport    AL  35740.0   None  34.92019 -85.7446   \n",
       "\n",
       "      status status_detail expected_date_online  mw_high                 sizerank  sizerank_numeric facility_size_sqft  \\\n",
       "0   Proposed          None                 None   1200.0  Mega campus (>1,000 MW)               6.0               None   \n",
       "1  Operating          None                 None      NaN                  Unknown               1.0               None   \n",
       "2    Unknown          None                 None      NaN                  Unknown               1.0               None   \n",
       "\n",
       "  property_size_acres number_of_generators number_of_buildings power_source dedicated_power_plant cooling_type cooling_source purpose  \\\n",
       "0                 675                 None                  18         None                  None         None           None    None   \n",
       "1                None                 None                None         None                  None         None           None    None   \n",
       "2                 360                 None                None         None                  None         None           None    None   \n",
       "\n",
       "  operator tenant                                    source_row_hash                      ingested_at  \n",
       "0     None   None  162404f7428395dd34759f3fa26cea0265fc3230254203... 2026-01-10 22:18:16.481519+00:00  \n",
       "1     None   None  51ff6fa6dd976bae0c1ba4e6d6b4a57cd12c5c9b8d57d2... 2026-01-10 22:18:16.481519+00:00  \n",
       "2   Google   None  7bf54df3590be2128eb694de6ba30d09a7b6d62ce6fa70... 2026-01-10 22:18:16.481519+00:00  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- READ: FracTracker raw from DB (for cleaning + imputation) ---\n",
    "\n",
    "cols = set(pg_table_columns(engine, \"raw\", \"fractracker_datacenters\"))\n",
    "\n",
    "wanted = [\n",
    "    \"facility_id\",\n",
    "    \"name\",\n",
    "    \"address\",\n",
    "    \"city\",\n",
    "    \"state\",\n",
    "    \"zip\",\n",
    "    \"county\",\n",
    "    \"lat\",\n",
    "    \"long\",\n",
    "    \"status\",\n",
    "    \"status_detail\",\n",
    "    \"expected_date_online\",\n",
    "    \"mw_high\",\n",
    "    \"sizerank\",\n",
    "    \"sizerank_numeric\",\n",
    "    \"facility_size_sqft\",\n",
    "    \"property_size_acres\",\n",
    "    \"number_of_generators\",\n",
    "    \"number_of_buildings\",\n",
    "    \"power_source\",\n",
    "    \"dedicated_power_plant\",\n",
    "    \"cooling_type\",\n",
    "    \"cooling_source\",\n",
    "    \"purpose\",\n",
    "    \"operator\",  \n",
    "    \"tenant\",\n",
    "    \"source_row_hash\",\n",
    "    \"ingested_at\",\n",
    "]\n",
    "\n",
    "select_exprs = [c for c in wanted if c in cols]\n",
    "\n",
    "RAW_DC_SQL = f\"\"\"\n",
    "SELECT {\", \".join(select_exprs)}\n",
    "FROM raw.fractracker_datacenters;\n",
    "\"\"\"\n",
    "\n",
    "df_dc_raw = pd.read_sql(RAW_DC_SQL, engine)\n",
    "print(df_dc_raw.shape)\n",
    "df_dc_raw.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c1c552-265a-4262-9e0e-4a76c92ac489",
   "metadata": {},
   "source": [
    "## Reusable QA checkpoint helper\n",
    "\n",
    "I define a compact QA overview helper for quick checkpoints during transformation. Keeping this function in the notebook lets me measure row counts, duplicates, and missingness repeatedly as I filter and derive fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e79ece37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== QA: FracTracker raw facilities ===\n",
      "              rows: 1260\n",
      "              cols: 28\n",
      "    duplicate_keys: 0\n",
      "     null_keys_any: 0\n",
      "\n",
      "Top missing (%):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>facility_size_sqft</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_of_generators</th>\n",
       "      <td>99.920635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dedicated_power_plant</th>\n",
       "      <td>99.920635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cooling_type</th>\n",
       "      <td>98.730159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tenant</th>\n",
       "      <td>98.650794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cooling_source</th>\n",
       "      <td>98.492063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>power_source</th>\n",
       "      <td>97.619048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purpose</th>\n",
       "      <td>97.460317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expected_date_online</th>\n",
       "      <td>97.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>status_detail</th>\n",
       "      <td>95.079365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_of_buildings</th>\n",
       "      <td>93.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mw_high</th>\n",
       "      <td>68.095238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>county</th>\n",
       "      <td>61.190476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>operator</th>\n",
       "      <td>47.539683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_size_acres</th>\n",
       "      <td>45.634921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        missing_%\n",
       "facility_size_sqft     100.000000\n",
       "number_of_generators    99.920635\n",
       "dedicated_power_plant   99.920635\n",
       "cooling_type            98.730159\n",
       "tenant                  98.650794\n",
       "cooling_source          98.492063\n",
       "power_source            97.619048\n",
       "purpose                 97.460317\n",
       "expected_date_online    97.142857\n",
       "status_detail           95.079365\n",
       "number_of_buildings     93.888889\n",
       "mw_high                 68.095238\n",
       "county                  61.190476\n",
       "operator                47.539683\n",
       "property_size_acres     45.634921"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def qa_overview(df: pd.DataFrame, name: str, key_cols: list[str] | None = None) -> pd.DataFrame:\n",
    "    out = []\n",
    "    out.append((\"rows\", len(df)))\n",
    "    out.append((\"cols\", df.shape[1]))\n",
    "    if key_cols:\n",
    "        out.append((\"duplicate_keys\", int(df.duplicated(key_cols).sum())))\n",
    "        out.append((\"null_keys_any\", int(df[key_cols].isna().any(axis=1).sum())))\n",
    "    miss = (df.isna().mean() * 100).sort_values(ascending=False).head(15)\n",
    "    print(f\"\\n=== QA: {name} ===\")\n",
    "    for k,v in out:\n",
    "        print(f\"{k:>18}: {v}\")\n",
    "    print(\"\\nTop missing (%):\")\n",
    "    display(miss.to_frame(\"missing_%\"))\n",
    "    return miss.to_frame(\"missing_%\")\n",
    "\n",
    "_ = qa_overview(df_dc_raw, \"FracTracker raw facilities\", key_cols=[\"facility_id\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87af0475-bdcd-474b-83be-1796986ecfd5",
   "metadata": {},
   "source": [
    "## Standardize facility attributes\n",
    "\n",
    "I normalize facility fields (types, text normalization, state formatting) and prepare the operating-facility subset for MW inference. This step converts the raw facility dataset into a clean, modeling-ready input for the imputation stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3b4556d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1260, 28)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_num(x):\n",
    "    if x is None:\n",
    "        return np.nan\n",
    "    if isinstance(x, (int, float, np.number)):\n",
    "        return float(x)\n",
    "    s = str(x).strip()\n",
    "    if s == \"\" or s.lower() in {\"na\", \"n/a\", \"nan\", \"none\", \"null\", \"unknown\"}:\n",
    "        return np.nan\n",
    "    s = s.replace(\",\", \"\")\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df = df_dc_raw.copy()\n",
    "\n",
    "# Normalize core text fields\n",
    "for col in [\"state\",\"status\",\"status_detail\",\"purpose\",\"power_source\",\"cooling_type\",\"cooling_source\",\"dedicated_power_plant\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(\"string\").str.strip()\n",
    "\n",
    "# Numeric conversions for model features\n",
    "num_cols = [\n",
    "    \"lat\",\"long\",\"facility_size_sqft\",\"property_size_acres\",\n",
    "    \"number_of_generators\",\"number_of_buildings\",\"mw_high\",\"sizerank_numeric\"\n",
    "]\n",
    "for c in num_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].map(to_num)\n",
    "\n",
    "# Make state uppercase 2-letter\n",
    "df[\"state\"] = df[\"state\"].str.upper()\n",
    "\n",
    "# Keep only valid-looking states for modeling\n",
    "df = df[df[\"state\"].str.fullmatch(r\"[A-Z]{2}\", na=False)].copy()\n",
    "\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b506661-1b97-4501-b6cb-29d2e80ef6f1",
   "metadata": {},
   "source": [
    "## Define imputation training and prediction sets\n",
    "\n",
    "I filter to operating facilities and partition rows into (a) records with valid MW values for training and (b) records missing or invalid MW values for prediction. This makes the imputation step measurable and auditable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7aaca99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating rows: 494\n",
      "Train rows (MW known): 163\n",
      "Predict rows (MW missing): 331\n",
      "Index(['facility_id', 'name', 'address', 'city', 'state', 'zip', 'county', 'lat', 'long', 'status', 'status_detail',\n",
      "       'expected_date_online', 'mw_high', 'sizerank', 'sizerank_numeric', 'facility_size_sqft', 'property_size_acres',\n",
      "       'number_of_generators', 'number_of_buildings', 'power_source', 'dedicated_power_plant', 'cooling_type', 'cooling_source', 'purpose',\n",
      "       'operator', 'tenant', 'source_row_hash', 'ingested_at'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_op = df[df[\"status\"].str.upper() == \"OPERATING\"].copy()\n",
    "y = df_op[\"mw_high\"].copy()\n",
    "\n",
    "train_mask = y.notna() & (y > 0)\n",
    "pred_mask  = y.isna() | (y <= 0)\n",
    "\n",
    "print(\"Operating rows:\", len(df_op))\n",
    "print(\"Train rows (MW known):\", int(train_mask.sum()))\n",
    "print(\"Predict rows (MW missing):\", int(pred_mask.sum()))\n",
    "print(df_op.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c48ca5-1bce-4d34-a1be-2b01f7295f92",
   "metadata": {},
   "source": [
    "## Build feature matrix for MW inference\n",
    "\n",
    "I select a feature set grounded in facility attributes (size, footprint, generators, geography, and categorical descriptors) and standardize missing categorical values into an explicit “UNKNOWN” bucket. This improves model robustness while preserving transparency about missing source data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59f6464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES = [\n",
    "    \"facility_size_sqft\",\n",
    "    \"property_size_acres\",\n",
    "    \"number_of_generators\",\n",
    "    \"number_of_buildings\",\n",
    "    \"lat\",\n",
    "    \"long\"\n",
    "]\n",
    "\n",
    "CAT_FEATURES = [\n",
    "    \"state\",\n",
    "    \"purpose\",\n",
    "    \"power_source\",\n",
    "    \"cooling_type\",\n",
    "    \"cooling_source\",\n",
    "    \"dedicated_power_plant\",\n",
    "    \"tier_classification\"\n",
    "]\n",
    "\n",
    "NUM_FEATURES = [c for c in NUM_FEATURES if c in df_op.columns]\n",
    "CAT_FEATURES = [c for c in CAT_FEATURES if c in df_op.columns]\n",
    "\n",
    "X = df_op[NUM_FEATURES + CAT_FEATURES].copy()\n",
    "\n",
    "for c in CAT_FEATURES:\n",
    "    X[c] = X[c].fillna(\"UNKNOWN\").astype(\"string\").str.strip().replace({\"\": \"UNKNOWN\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b2a9c7-d863-4bd3-b578-2c52723e00c9",
   "metadata": {},
   "source": [
    "## Evaluate MW inference model (validated imputation)\n",
    "\n",
    "Instead of relying on a single heuristic, I evaluate an imputation model using cross-validation and standard regression metrics (RMSE/MAE/R²). This is an intentional upgrade: it keeps the imputation defensible by measuring expected error on facilities where MW is known.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98ae84b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost CV: {'rmse': 274.5759108811738, 'mae': 97.88754730821715, 'r2': 0.06878641341973857}\n",
      "sklearn HGBR CV: {'rmse': 292.1591435248276, 'mae': 109.5730466014038, 'r2': -0.05429804981514352}\n",
      "\n",
      "BEST: catboost {'rmse': 274.5759108811738, 'mae': 97.88754730821715, 'r2': 0.06878641341973857}\n"
     ]
    }
   ],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def eval_cv_predictions(y_true, y_pred):\n",
    "    return {\n",
    "        \"rmse\": rmse(y_true, y_pred),\n",
    "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
    "        \"r2\": float(r2_score(y_true, y_pred)),\n",
    "    }\n",
    "\n",
    "X_train = X.loc[train_mask].copy()\n",
    "y_train = y.loc[train_mask].copy()\n",
    "\n",
    "y_train_log = np.log1p(y_train)\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "candidates = []\n",
    "\n",
    "\n",
    "catboost_available = False\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    catboost_available = True\n",
    "\n",
    "    cat_idx = [X_train.columns.get_loc(c) for c in CAT_FEATURES]\n",
    "\n",
    "    oof = np.zeros(len(X_train), dtype=float)\n",
    "    for fold, (tr, va) in enumerate(cv.split(X_train), start=1):\n",
    "        model = CatBoostRegressor(\n",
    "            loss_function=\"RMSE\",\n",
    "            depth=8,\n",
    "            learning_rate=0.05,\n",
    "            iterations=2000,\n",
    "            random_seed=RANDOM_SEED,\n",
    "            verbose=False\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train.iloc[tr], y_train_log.iloc[tr],\n",
    "            cat_features=cat_idx,\n",
    "            eval_set=(X_train.iloc[va], y_train_log.iloc[va]),\n",
    "            use_best_model=True\n",
    "        )\n",
    "        oof[va] = model.predict(X_train.iloc[va])\n",
    "\n",
    "    oof_mw = np.expm1(oof)\n",
    "    metrics = eval_cv_predictions(y_train.values, oof_mw)\n",
    "    candidates.append((\"catboost\", metrics))\n",
    "    print(\"CatBoost CV:\", metrics)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"CatBoost not available or failed import:\", repr(e))\n",
    "\n",
    "# --- Candidate B: sklearn HistGradientBoosting (robust baseline) ---\n",
    "\n",
    "\n",
    "numeric_tf = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "])\n",
    "\n",
    "categorical_tf = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_tf, NUM_FEATURES),\n",
    "        (\"cat\", categorical_tf, CAT_FEATURES),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "sk_model = HistGradientBoostingRegressor(\n",
    "    random_state=RANDOM_SEED,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    max_iter=800\n",
    ")\n",
    "\n",
    "sk_pipe = Pipeline(steps=[(\"pre\", pre), (\"model\", sk_model)])\n",
    "\n",
    "oof = np.zeros(len(X_train), dtype=float)\n",
    "for fold, (tr, va) in enumerate(cv.split(X_train), start=1):\n",
    "    sk_pipe.fit(X_train.iloc[tr], y_train_log.iloc[tr])\n",
    "    oof[va] = sk_pipe.predict(X_train.iloc[va])\n",
    "\n",
    "oof_mw = np.expm1(oof)\n",
    "metrics = eval_cv_predictions(y_train.values, oof_mw)\n",
    "candidates.append((\"sklearn_hgbr\", metrics))\n",
    "print(\"sklearn HGBR CV:\", metrics)\n",
    "\n",
    "best_algo, best_metrics = sorted(candidates, key=lambda x: x[1][\"rmse\"])[0]\n",
    "print(\"\\nBEST:\", best_algo, best_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ec4605-bda0-4339-85a6-2b05facb2cd6",
   "metadata": {},
   "source": [
    "## Impute MW and enforce reasonable bounds\n",
    "\n",
    "I train the final model on known-MW facilities, generate predictions for all operating facilities, and construct a single MW_final field that uses the observed value when available and the predicted value otherwise. I also cap extreme predictions using a high-quantile rule to reduce the impact of outliers on state-level aggregation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48ad72e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mw_high</th>\n",
       "      <th>mw_pred</th>\n",
       "      <th>mw_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>163.000000</td>\n",
       "      <td>494.000000</td>\n",
       "      <td>494.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>113.119632</td>\n",
       "      <td>52.081817</td>\n",
       "      <td>68.099149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>285.413214</td>\n",
       "      <td>72.256656</td>\n",
       "      <td>169.560215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.019715</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>36.110608</td>\n",
       "      <td>36.041937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>300.000000</td>\n",
       "      <td>104.833588</td>\n",
       "      <td>112.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>575.400000</td>\n",
       "      <td>155.223835</td>\n",
       "      <td>182.686825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99%</th>\n",
       "      <td>1400.000000</td>\n",
       "      <td>333.733588</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2200.000000</td>\n",
       "      <td>787.621667</td>\n",
       "      <td>2200.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           mw_high     mw_pred     mw_final\n",
       "count   163.000000  494.000000   494.000000\n",
       "mean    113.119632   52.081817    68.099149\n",
       "std     285.413214   72.256656   169.560215\n",
       "min       0.100000    1.019715     0.100000\n",
       "50%      16.000000   36.110608    36.041937\n",
       "90%     300.000000  104.833588   112.100000\n",
       "95%     575.400000  155.223835   182.686825\n",
       "99%    1400.000000  333.733588  1000.000000\n",
       "max    2200.000000  787.621667  2200.000000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all = X.copy()\n",
    "\n",
    "if best_algo == \"catboost\":\n",
    "    cat_idx = [X_train.columns.get_loc(c) for c in CAT_FEATURES]\n",
    "    final_model = CatBoostRegressor(\n",
    "        loss_function=\"RMSE\",\n",
    "        depth=8,\n",
    "        learning_rate=0.05,\n",
    "        iterations=2000,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        verbose=False\n",
    "    )\n",
    "    final_model.fit(X_train, y_train_log, cat_features=cat_idx, verbose=False)\n",
    "    pred_log = final_model.predict(X_all)\n",
    "else:\n",
    "    # sklearn pipeline\n",
    "    final_model = sk_pipe\n",
    "    final_model.fit(X_train, y_train_log)\n",
    "    pred_log = final_model.predict(X_all)\n",
    "\n",
    "pred_mw = np.expm1(pred_log)\n",
    "\n",
    "q995 = float(np.nanquantile(y_train.values, 0.995))\n",
    "upper_cap = max(q995 * 1.5, q995)  # allow some headroom\n",
    "pred_mw = np.clip(pred_mw, 0, upper_cap)\n",
    "\n",
    "df_op[\"mw_pred\"] = pred_mw\n",
    "df_op[\"mw_final\"] = np.where(train_mask.values, df_op[\"mw_high\"], df_op[\"mw_pred\"])\n",
    "\n",
    "df_op[[\"mw_high\",\"mw_pred\",\"mw_final\"]].describe(percentiles=[0.5,0.9,0.95,0.99])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e354d729-b98c-4562-9f36-2bb28b029dca",
   "metadata": {},
   "source": [
    "## Derive size rank consistently from MW_final\n",
    "\n",
    "I map MW_final into an interpretable size-rank scale (small → hyperscale → mega campus) and reconcile it with any existing size rank fields. This guarantees that downstream “intensity” measures can be summarized consistently even when the source dataset marks size as unknown.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d80f7d95-8730-4681-a8f4-f4c60d4d3458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sizerank</th>\n",
       "      <th>sizerank_final</th>\n",
       "      <th>sizerank_numeric</th>\n",
       "      <th>sizerank_numeric_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>Medium (11–50 MW)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>Medium (11–50 MW)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>Medium (11–50 MW)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>Hyperscale (101–999 MW)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>Hyperscale (101–999 MW)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Small (0-10 MW)</td>\n",
       "      <td>Small (0-10 MW)</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>Hyperscale (101–999 MW)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Small (0-10 MW)</td>\n",
       "      <td>Small (0-10 MW)</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>Hyperscale (101–999 MW)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>Hyperscale (101–999 MW)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sizerank           sizerank_final  sizerank_numeric  sizerank_numeric_final\n",
       "1           Unknown        Medium (11–50 MW)               1.0                     3.0\n",
       "3           Unknown        Medium (11–50 MW)               1.0                     3.0\n",
       "4           Unknown        Medium (11–50 MW)               1.0                     3.0\n",
       "14          Unknown  Hyperscale (101–999 MW)               1.0                     5.0\n",
       "19          Unknown  Hyperscale (101–999 MW)               1.0                     5.0\n",
       "26  Small (0-10 MW)          Small (0-10 MW)               2.0                     2.0\n",
       "39          Unknown  Hyperscale (101–999 MW)               1.0                     5.0\n",
       "40  Small (0-10 MW)          Small (0-10 MW)               2.0                     2.0\n",
       "43          Unknown  Hyperscale (101–999 MW)               1.0                     5.0\n",
       "44          Unknown  Hyperscale (101–999 MW)               1.0                     5.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sizerank_from_mw(mw: float) -> tuple[str, int]:\n",
    "    if mw is None or np.isnan(mw) or mw <= 0:\n",
    "        return (\"Unknown\", 1)\n",
    "    if mw <= 10:\n",
    "        return (\"Small (0–10 MW)\", 2)\n",
    "    if mw <= 50:\n",
    "        return (\"Medium (11–50 MW)\", 3)\n",
    "    if mw <= 100:\n",
    "        return (\"Large (51–100 MW)\", 4)\n",
    "    if mw <= 999:\n",
    "        return (\"Hyperscale (101–999 MW)\", 5)\n",
    "    return (\"Mega campus (>1,000 MW)\", 6)\n",
    "\n",
    "sr = df_op[\"mw_final\"].apply(sizerank_from_mw)\n",
    "df_op[\"sizerank_inferred\"] = sr.apply(lambda x: x[0])\n",
    "df_op[\"sizerank_numeric_inferred\"] = sr.apply(lambda x: x[1])\n",
    "\n",
    "df_op[\"sizerank_final\"] = np.where(\n",
    "    df_op[\"sizerank\"].fillna(\"Unknown\").str.contains(\"Unknown\", case=False),\n",
    "    df_op[\"sizerank_inferred\"],\n",
    "    df_op[\"sizerank\"].fillna(df_op[\"sizerank_inferred\"])\n",
    ")\n",
    "\n",
    "df_op[\"sizerank_numeric_final\"] = np.where(\n",
    "    df_op[\"sizerank_numeric\"].fillna(1).astype(float) == 1,\n",
    "    df_op[\"sizerank_numeric_inferred\"],\n",
    "    df_op[\"sizerank_numeric\"]\n",
    ")\n",
    "\n",
    "df_op[[\"sizerank\",\"sizerank_final\",\"sizerank_numeric\",\"sizerank_numeric_final\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b819dc-4273-4ad1-b4e2-40c7e793681f",
   "metadata": {},
   "source": [
    "## Persist imputation as a first-class, auditable run\n",
    "\n",
    "I write the imputation run metadata (algorithm, parameters, metrics) and the per-facility imputed MW values into a dedicated staging schema with a run_id. Treating imputation as versioned data—rather than a hidden transformation—supports reproducibility, lineage, and future re-runs with improved models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a1bb131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted imputation run: f45cdb3a-68da-45f5-a8d3-4d654fbbc71b rows: 331\n"
     ]
    }
   ],
   "source": [
    "def pg_conn():\n",
    "    return psycopg.connect(\n",
    "        host=os.environ.get(\"PGHOST\", \"localhost\"),\n",
    "        port=os.environ.get(\"PGPORT\", \"5432\"),\n",
    "        dbname=os.environ[\"PGDATABASE\"],\n",
    "        user=os.environ[\"PGUSER\"],\n",
    "        password=os.environ[\"PGPASSWORD\"],\n",
    "        row_factory=dict_row\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fetch_df(query: str, params: tuple | None = None) -> pd.DataFrame:\n",
    "    with pg_conn() as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(query, params or ())\n",
    "            rows = cur.fetchall()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def fetch_scalar(query: str, params: tuple | None = None):\n",
    "    with pg_conn() as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(query, params or ())\n",
    "            row = cur.fetchone()\n",
    "    if not row:\n",
    "        return None\n",
    "    return list(row.values())[0]\n",
    "run_id = str(uuid.uuid4())\n",
    "run_ts = datetime.now(timezone.utc)\n",
    "\n",
    "algo_params = {\n",
    "    \"algorithm\": best_algo,\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"num_features\": NUM_FEATURES,\n",
    "    \"cat_features\": CAT_FEATURES,\n",
    "    \"target_transform\": \"log1p(mw_high)\",\n",
    "    \"prediction_cap_rule\": \"cap at 1.5 * q99.5 of training MW\"\n",
    "}\n",
    "run_metrics = best_metrics\n",
    "\n",
    "df_imp = df_op.loc[\n",
    "    pred_mask.values,\n",
    "    [\"facility_id\", \"mw_pred\", \"mw_final\", \"sizerank_final\", \"sizerank_numeric_final\"]\n",
    "].copy()\n",
    "\n",
    "missing_final = df_imp[\"mw_final\"].isna()\n",
    "if missing_final.any():\n",
    "    donor = df_op.loc[df_op[\"mw_high\"].notna(), [\"mw_high\", \"sizerank_final\"]].copy()\n",
    "\n",
    "    by_rank_median = donor.groupby(\"sizerank_final\")[\"mw_high\"].median()\n",
    "    overall_median = donor[\"mw_high\"].median()\n",
    "\n",
    "    df_imp.loc[missing_final, \"mw_final\"] = (\n",
    "        df_imp.loc[missing_final, \"sizerank_final\"]\n",
    "        .map(by_rank_median)\n",
    "        .fillna(overall_median)\n",
    "    )\n",
    "\n",
    "    df_imp.loc[missing_final, \"mw_pred\"] = df_imp.loc[missing_final, \"mw_final\"]\n",
    "\n",
    "if df_imp[\"mw_final\"].isna().any():\n",
    "    bad = df_imp[df_imp[\"mw_final\"].isna()].head(10)\n",
    "    raise ValueError(f\"mw_final still has NULLs after fallback. Sample:\\n{bad}\")\n",
    "\n",
    "df_imp[\"diagnostics\"] = df_imp.apply(lambda r: {\n",
    "    \"run_id\": run_id,\n",
    "    \"mw_pred\": float(r[\"mw_pred\"]) if pd.notna(r[\"mw_pred\"]) else None,\n",
    "    \"mw_final\": float(r[\"mw_final\"]) if pd.notna(r[\"mw_final\"]) else None,\n",
    "    \"sizerank_final\": r[\"sizerank_final\"],\n",
    "    \"sizerank_numeric_final\": int(r[\"sizerank_numeric_final\"]) if pd.notna(r[\"sizerank_numeric_final\"]) else None,\n",
    "}, axis=1)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# B) Persist run + results (schema-safe, NOT NULL-safe)\n",
    "# ----------------------------\n",
    "with pg_conn() as conn:\n",
    "    with conn.cursor() as cur:\n",
    "\n",
    "        cur.execute(\"CREATE SCHEMA IF NOT EXISTS stage;\")\n",
    "\n",
    "        cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS stage.dc_mw_imputation_run (\n",
    "          run_id UUID PRIMARY KEY,\n",
    "          run_ts TIMESTAMPTZ NOT NULL,\n",
    "          algorithm TEXT NOT NULL,\n",
    "          params JSONB NOT NULL,\n",
    "          metrics JSONB NOT NULL,\n",
    "          is_current BOOLEAN NOT NULL DEFAULT FALSE\n",
    "        );\n",
    "        \"\"\")\n",
    "\n",
    "        cur.execute(\"\"\"ALTER TABLE stage.dc_mw_imputation_run ADD COLUMN IF NOT EXISTS run_ts TIMESTAMPTZ;\"\"\")\n",
    "        cur.execute(\"\"\"ALTER TABLE stage.dc_mw_imputation_run ADD COLUMN IF NOT EXISTS algorithm TEXT;\"\"\")\n",
    "        cur.execute(\"\"\"ALTER TABLE stage.dc_mw_imputation_run ADD COLUMN IF NOT EXISTS params JSONB;\"\"\")\n",
    "        cur.execute(\"\"\"ALTER TABLE stage.dc_mw_imputation_run ADD COLUMN IF NOT EXISTS metrics JSONB;\"\"\")\n",
    "        cur.execute(\"\"\"ALTER TABLE stage.dc_mw_imputation_run ADD COLUMN IF NOT EXISTS is_current BOOLEAN;\"\"\")\n",
    "\n",
    "        cur.execute(\"UPDATE stage.dc_mw_imputation_run SET run_ts = COALESCE(run_ts, now()) WHERE run_ts IS NULL;\")\n",
    "        cur.execute(\"UPDATE stage.dc_mw_imputation_run SET algorithm = COALESCE(algorithm, 'unknown') WHERE algorithm IS NULL;\")\n",
    "        cur.execute(\"UPDATE stage.dc_mw_imputation_run SET params = COALESCE(params, '{}'::jsonb) WHERE params IS NULL;\")\n",
    "        cur.execute(\"UPDATE stage.dc_mw_imputation_run SET metrics = COALESCE(metrics, '{}'::jsonb) WHERE metrics IS NULL;\")\n",
    "        cur.execute(\"UPDATE stage.dc_mw_imputation_run SET is_current = COALESCE(is_current, FALSE) WHERE is_current IS NULL;\")\n",
    "\n",
    "        cur.execute(\"ALTER TABLE stage.dc_mw_imputation_run ALTER COLUMN run_ts SET NOT NULL;\")\n",
    "        cur.execute(\"ALTER TABLE stage.dc_mw_imputation_run ALTER COLUMN algorithm SET NOT NULL;\")\n",
    "        cur.execute(\"ALTER TABLE stage.dc_mw_imputation_run ALTER COLUMN params SET NOT NULL;\")\n",
    "        cur.execute(\"ALTER TABLE stage.dc_mw_imputation_run ALTER COLUMN metrics SET NOT NULL;\")\n",
    "        cur.execute(\"ALTER TABLE stage.dc_mw_imputation_run ALTER COLUMN is_current SET NOT NULL;\")\n",
    "        cur.execute(\"ALTER TABLE stage.dc_mw_imputation_run ALTER COLUMN is_current SET DEFAULT FALSE;\")\n",
    "\n",
    "        cur.execute(\"\"\"\n",
    "        CREATE UNIQUE INDEX IF NOT EXISTS ux_dc_mw_imputation_run_current\n",
    "          ON stage.dc_mw_imputation_run (is_current) WHERE is_current;\n",
    "        \"\"\")\n",
    "\n",
    "        cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS stage.dc_mw_imputation_result (\n",
    "          run_id UUID NOT NULL REFERENCES stage.dc_mw_imputation_run(run_id) ON DELETE CASCADE,\n",
    "          facility_id TEXT NOT NULL,\n",
    "          mw_imputed NUMERIC NOT NULL,\n",
    "          mw_pred NUMERIC,\n",
    "          diagnostics JSONB NOT NULL DEFAULT '{}'::jsonb,\n",
    "          inserted_at TIMESTAMPTZ NOT NULL DEFAULT now(),\n",
    "          PRIMARY KEY (run_id, facility_id)\n",
    "        );\n",
    "        \"\"\")\n",
    "\n",
    "        cur.execute(\"\"\"ALTER TABLE stage.dc_mw_imputation_result ADD COLUMN IF NOT EXISTS mw_imputed NUMERIC;\"\"\")\n",
    "        cur.execute(\"\"\"ALTER TABLE stage.dc_mw_imputation_result ADD COLUMN IF NOT EXISTS mw_pred NUMERIC;\"\"\")\n",
    "        cur.execute(\"\"\"ALTER TABLE stage.dc_mw_imputation_result ADD COLUMN IF NOT EXISTS diagnostics JSONB;\"\"\")\n",
    "        cur.execute(\"\"\"ALTER TABLE stage.dc_mw_imputation_result ADD COLUMN IF NOT EXISTS inserted_at TIMESTAMPTZ;\"\"\")\n",
    "\n",
    "        cur.execute(\"\"\"\n",
    "        UPDATE stage.dc_mw_imputation_result\n",
    "        SET\n",
    "          diagnostics = COALESCE(diagnostics, '{}'::jsonb),\n",
    "          inserted_at = COALESCE(inserted_at, now()),\n",
    "          mw_imputed = COALESCE(\n",
    "              mw_imputed,\n",
    "              mw_pred,\n",
    "              NULLIF((diagnostics->>'mw_final'), '')::numeric,\n",
    "              NULLIF((diagnostics->>'mw_pred'), '')::numeric\n",
    "          )\n",
    "        WHERE mw_imputed IS NULL\n",
    "           OR diagnostics IS NULL\n",
    "           OR inserted_at IS NULL;\n",
    "        \"\"\")\n",
    "\n",
    "        # Enforce constraints\n",
    "        cur.execute(\"ALTER TABLE stage.dc_mw_imputation_result ALTER COLUMN mw_imputed SET NOT NULL;\")\n",
    "        cur.execute(\"ALTER TABLE stage.dc_mw_imputation_result ALTER COLUMN diagnostics SET NOT NULL;\")\n",
    "        cur.execute(\"ALTER TABLE stage.dc_mw_imputation_result ALTER COLUMN inserted_at SET NOT NULL;\")\n",
    "        cur.execute(\"ALTER TABLE stage.dc_mw_imputation_result ALTER COLUMN inserted_at SET DEFAULT now();\")\n",
    "\n",
    "        # ----------------------------\n",
    "        # 1) Insert as the current run\n",
    "        # ----------------------------\n",
    "        cur.execute(\"UPDATE stage.dc_mw_imputation_run SET is_current = FALSE WHERE is_current = TRUE;\")\n",
    "\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO stage.dc_mw_imputation_run (run_id, run_ts, algorithm, params, metrics, is_current)\n",
    "            VALUES (%s, %s, %s, %s::jsonb, %s::jsonb, TRUE);\n",
    "            \"\"\",\n",
    "            (run_id, run_ts, best_algo, json.dumps(algo_params), json.dumps(run_metrics)),\n",
    "        )\n",
    "\n",
    "        # ----------------------------\n",
    "        # 2) Upsert imputed results (INCLUDE mw_imputed)\n",
    "        # ----------------------------\n",
    "        rows = [\n",
    "            (\n",
    "                run_id,\n",
    "                str(r[\"facility_id\"]),\n",
    "                float(r[\"mw_final\"]),  # <- satisfies mw_imputed NOT NULL\n",
    "                float(r[\"mw_pred\"]) if pd.notna(r[\"mw_pred\"]) else None,\n",
    "                json.dumps(r[\"diagnostics\"]),\n",
    "            )\n",
    "            for _, r in df_imp.iterrows()\n",
    "        ]\n",
    "\n",
    "        cur.executemany(\n",
    "            \"\"\"\n",
    "            INSERT INTO stage.dc_mw_imputation_result\n",
    "              (run_id, facility_id, mw_imputed, mw_pred, diagnostics)\n",
    "            VALUES (%s, %s, %s, %s, %s::jsonb)\n",
    "            ON CONFLICT (run_id, facility_id) DO UPDATE\n",
    "            SET mw_imputed = EXCLUDED.mw_imputed,\n",
    "                mw_pred    = EXCLUDED.mw_pred,\n",
    "                diagnostics= EXCLUDED.diagnostics,\n",
    "                inserted_at= now();\n",
    "            \"\"\",\n",
    "            rows,\n",
    "        )\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "print(\"Inserted imputation run:\", run_id, \"rows:\", len(df_imp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402cebe6-76c5-4fb3-aca4-03cca8a6268a",
   "metadata": {},
   "source": [
    "## Build staging tables from raw sources (SQL transforms)\n",
    "\n",
    "I execute staged SQL transforms to convert raw landing tables into clean, analysis-aligned staging tables (operating facilities, retail state-month-sector, and operational controls). Keeping these transforms in SQL makes the logic centralized, testable, and easier to refresh end-to-end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45b32809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: C:\\Users\\amand\\OneDrive\\Documents\\School\\Assignments\\WGU\\Data-Engineering\\Capstone\\datacenter-electricity-prices-us\\sql\\transforms\\06_stage__datacenters_operating.sql\n",
      "Running: C:\\Users\\amand\\OneDrive\\Documents\\School\\Assignments\\WGU\\Data-Engineering\\Capstone\\datacenter-electricity-prices-us\\sql\\transforms\\07_stage__retail_state_month_sector.sql\n",
      "Running: C:\\Users\\amand\\OneDrive\\Documents\\School\\Assignments\\WGU\\Data-Engineering\\Capstone\\datacenter-electricity-prices-us\\sql\\transforms\\08_stage__ops_fuel_and_controls.sql\n",
      "Stage refresh complete.\n"
     ]
    }
   ],
   "source": [
    "SQL_TRANSFORMS_DIR = SQL_DIR / \"transforms\"\n",
    "stage_sql_files = [\n",
    "    \"06_stage__datacenters_operating.sql\",\n",
    "    \"07_stage__retail_state_month_sector.sql\",\n",
    "    \"08_stage__ops_fuel_and_controls.sql\",\n",
    "]\n",
    "\n",
    "def run_sql_file(path: Path):\n",
    "    sql_text = path.read_text(encoding=\"utf-8\")\n",
    "    sql_text = re.sub(r\"(?im)^\\s*BEGIN;\\s*$\", \"\", sql_text)\n",
    "    sql_text = re.sub(r\"(?im)^\\s*COMMIT;\\s*$\", \"\", sql_text)\n",
    "    sql_text = re.sub(r\"%(?![sbt%])\", \"%%\", sql_text)\n",
    "\n",
    "    with engine.begin() as conn:\n",
    "        conn.exec_driver_sql(sql_text)\n",
    "\n",
    "for f in stage_sql_files:\n",
    "    p = SQL_TRANSFORMS_DIR / f\n",
    "    print(\"Running:\", p)\n",
    "    run_sql_file(p)\n",
    "\n",
    "print(\"Stage refresh complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe32dba7-383e-4fa1-86c2-34ec917a6191",
   "metadata": {},
   "source": [
    "## Stage validation: facility counts and MW totals\n",
    "\n",
    "I compute state-level aggregates (facility counts and total MW) from the staged operating facilities table as a reasonableness check. This is a quick “does it pass the smell test?” checkpoint before I generate the curated mart and modeling dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0375308c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_code</th>\n",
       "      <th>facilities</th>\n",
       "      <th>mw_final_nulls</th>\n",
       "      <th>total_mw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VA</td>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>10575.357169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TX</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>6833.246667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GA</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>4638.988180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IN</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2239.275342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TN</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2028.833780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OH</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1547.522475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CA</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>855.772646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MS</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>819.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>OK</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>770.056653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PA</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>660.420047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state_code  facilities  mw_final_nulls      total_mw\n",
       "0         VA         202               0  10575.357169\n",
       "1         TX          53               0   6833.246667\n",
       "2         GA          82               0   4638.988180\n",
       "3         IN           3               0   2239.275342\n",
       "4         TN           5               0   2028.833780\n",
       "5         OH          11               0   1547.522475\n",
       "6         CA           7               0    855.772646\n",
       "7         MS           1               0    819.000000\n",
       "8         OK           3               0    770.056653\n",
       "9         PA          42               0    660.420047"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc_stage = pd.read_sql(\"\"\"\n",
    "SELECT\n",
    "  state_code,\n",
    "  COUNT(*) AS facilities,\n",
    "  SUM(CASE WHEN mw_final IS NULL THEN 1 ELSE 0 END) AS mw_final_nulls,\n",
    "  SUM(COALESCE(mw_final, 0)) AS total_mw\n",
    "FROM stage.data_centers_operating\n",
    "GROUP BY 1\n",
    "ORDER BY total_mw DESC NULLS LAST;\n",
    "\"\"\", engine)\n",
    "\n",
    "dc_stage.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee62106b-1ce5-4650-b5c2-10d0417c3075",
   "metadata": {},
   "source": [
    "## QA checkpoint output structure\n",
    "\n",
    "I create a timestamped QA output directory for this run so all validation tables and summaries are preserved as run artifacts. This run-isolated structure makes debugging and reporting much easier because outputs never overwrite prior checkpoints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19bf6566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA checkpoint outputs -> reports\\qa\\checkpoint_20260110_174056\n"
     ]
    }
   ],
   "source": [
    "REPORT_RUN_TS = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "QA_OUT_DIR = Path(\"reports\") / \"qa\" / f\"checkpoint_{REPORT_RUN_TS}\"\n",
    "QA_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"QA checkpoint outputs ->\", QA_OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1dd0c0-726f-4d49-bcae-65bcb7baa2e3",
   "metadata": {},
   "source": [
    "## Curated mart build (dimensions, facts, and analysis view)\n",
    "\n",
    "I execute the mart SQL in a controlled order to load dimensions, load facts, and refresh the analysis materialized view. This produces a curated, analytics-ready layer designed for consistent joins and stable modeling at the state-month grain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6be431e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09_mart__load_dimensions.sql\n",
      "Running: 09_mart__load_dimensions.sql  (statements: 13)\n",
      "10_mart__load_facts.sql\n",
      "Running: 10_mart__load_facts.sql  (statements: 9)\n",
      "11_mart__mv_analysis_refresh.sql\n",
      "Running: 11_mart__mv_analysis_refresh.sql  (statements: 8)\n",
      "✅ MART load complete (dimensions, facts, analysis MV).\n"
     ]
    }
   ],
   "source": [
    "marts_dir = SQL_DIR / \"transforms\"\n",
    "\n",
    "mart_sql_files = [\n",
    "    \"09_mart__load_dimensions.sql\",\n",
    "    \"10_mart__load_facts.sql\",\n",
    "    \"11_mart__mv_analysis_refresh.sql\",\n",
    "]\n",
    "\n",
    "_PCT = re.compile(r\"%(?![sbt%])\", flags=re.IGNORECASE)\n",
    "\n",
    "def run_sql_file(path: Path):\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing SQL file: {path.resolve()}\")\n",
    "\n",
    "    sql_text = path.read_text(encoding=\"utf-8-sig\")\n",
    "\n",
    "    statements = _split_sql_statements(sql_text)\n",
    "\n",
    "    print(f\"Running: {path.name}  (statements: {len(statements)})\")\n",
    "    with engine.begin() as conn:\n",
    "        for k, stmt in enumerate(statements, start=1):\n",
    "            try:\n",
    "                conn.exec_driver_sql(stmt)\n",
    "            except Exception as e:\n",
    "                preview = (stmt[:400] + \" ...\") if len(stmt) > 400 else stmt\n",
    "                raise RuntimeError(\n",
    "                    f\"SQL failed in {path.name} (statement {k}/{len(statements)}):\\n{preview}\"\n",
    "                ) from e\n",
    "\n",
    "\n",
    "# Run mart scripts in the required order\n",
    "for fname in mart_sql_files:\n",
    "    print(fname)\n",
    "    run_sql_file(marts_dir / fname)\n",
    "\n",
    "print(\"✅ MART load complete (dimensions, facts, analysis MV).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dddb89-c42b-4f9a-8d7b-e4906001047a",
   "metadata": {},
   "source": [
    "## QA assertion framework\n",
    "\n",
    "I define reusable QA assertions (row counts, uniqueness, null-rate thresholds, non-negativity) plus a helper to write outputs to the checkpoint folder. These checks formalize data quality expectations and turn silent failures into explicit, actionable errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ff9e32d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qdf(sql: str, params: dict | None = None) -> pd.DataFrame:\n",
    "    return pd.read_sql(text(sql), engine, params=params or {})\n",
    "\n",
    "def scalar(sql: str, params: dict | None = None):\n",
    "    df = qdf(sql, params)\n",
    "    return df.iat[0, 0] if not df.empty else None\n",
    "\n",
    "def assert_rowcount(table: str, min_rows: int = 1, max_rows: int | None = None):\n",
    "    n = scalar(f\"SELECT COUNT(*) FROM {table};\")\n",
    "    if n is None:\n",
    "        raise AssertionError(f\"{table}: unable to read rowcount\")\n",
    "    if n < min_rows:\n",
    "        raise AssertionError(f\"{table}: rowcount {n} < min {min_rows}\")\n",
    "    if max_rows is not None and n > max_rows:\n",
    "        raise AssertionError(f\"{table}: rowcount {n} > max {max_rows}\")\n",
    "    return int(n)\n",
    "\n",
    "def assert_unique_key(table: str, key_cols: list[str]):\n",
    "    cols = \", \".join(key_cols)\n",
    "    dup = scalar(f\"\"\"\n",
    "        SELECT COUNT(*) FROM (\n",
    "          SELECT {cols}, COUNT(*) c\n",
    "          FROM {table}\n",
    "          GROUP BY {cols}\n",
    "          HAVING COUNT(*) > 1\n",
    "        ) d;\n",
    "    \"\"\")\n",
    "    if dup and int(dup) > 0:\n",
    "        raise AssertionError(f\"{table}: found {dup} duplicate key rows on ({cols})\")\n",
    "    return 0\n",
    "\n",
    "def assert_null_rate(table: str, col: str, max_null_pct: float):\n",
    "    pct = scalar(f\"\"\"\n",
    "        SELECT 100.0 * AVG(CASE WHEN {col} IS NULL THEN 1 ELSE 0 END) FROM {table};\n",
    "    \"\"\")\n",
    "    pct = float(pct) if pct is not None else 0.0\n",
    "    if pct > max_null_pct:\n",
    "        raise AssertionError(f\"{table}.{col}: null% {pct:.2f} > allowed {max_null_pct:.2f}\")\n",
    "    return pct\n",
    "\n",
    "def assert_nonnegative(table: str, col: str):\n",
    "    bad = scalar(f\"SELECT COUNT(*) FROM {table} WHERE {col} < 0;\")\n",
    "    bad = int(bad or 0)\n",
    "    if bad > 0:\n",
    "        raise AssertionError(f\"{table}.{col}: {bad} rows are negative\")\n",
    "    return 0\n",
    "\n",
    "def save_df(df: pd.DataFrame, filename: str):\n",
    "    out = QA_OUT_DIR / filename\n",
    "    df.to_csv(out, index=False)\n",
    "    print(\"Wrote:\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ff46af-0a92-487b-879a-cce21300a4c9",
   "metadata": {},
   "source": [
    "## Dimension QA validation\n",
    "\n",
    "I validate dimension tables (state, month, sector, and optional fuel group) for expected row counts and key uniqueness. These checks ensure the star schema behaves correctly and prevents downstream duplication in fact joins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6c99c38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: reports\\qa\\checkpoint_20260110_174056\\dim_month_bounds.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_month</th>\n",
       "      <th>max_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2025-10-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    min_month   max_month\n",
       "0  2023-01-01  2025-10-01"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: mart.dim_fuel_group check skipped: (psycopg.errors.UndefinedTable) relation \"mart.dim_fuel_group\" does not exist\n",
      "LINE 1: SELECT COUNT(*) FROM mart.dim_fuel_group;\n",
      "                             ^\n",
      "[SQL: SELECT COUNT(*) FROM mart.dim_fuel_group;]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n",
      "Wrote: reports\\qa\\checkpoint_20260110_174056\\qa_dim_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table</th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mart.dim_state</td>\n",
       "      <td>rowcount</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mart.dim_month</td>\n",
       "      <td>rowcount</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mart.dim_retail_sector</td>\n",
       "      <td>rowcount</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    table    metric  value\n",
       "0          mart.dim_state  rowcount     51\n",
       "1          mart.dim_month  rowcount     34\n",
       "2  mart.dim_retail_sector  rowcount      6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qa_rows = []\n",
    "\n",
    "# dim_state\n",
    "n_states = assert_rowcount(\"mart.dim_state\", min_rows=40, max_rows=60)\n",
    "assert_unique_key(\"mart.dim_state\", [\"state_code\"])\n",
    "qa_rows.append((\"mart.dim_state\", \"rowcount\", n_states))\n",
    "\n",
    "# dim_month\n",
    "n_months = assert_rowcount(\"mart.dim_month\", min_rows=12, max_rows=120)\n",
    "assert_unique_key(\"mart.dim_month\", [\"month_date\"])\n",
    "qa_rows.append((\"mart.dim_month\", \"rowcount\", n_months))\n",
    "\n",
    "# Check month coverage\n",
    "month_bounds = qdf(\"\"\"\n",
    "SELECT MIN(month_date) AS min_month, MAX(month_date) AS max_month\n",
    "FROM mart.dim_month;\n",
    "\"\"\")\n",
    "save_df(month_bounds, \"dim_month_bounds.csv\")\n",
    "display(month_bounds)\n",
    "\n",
    "# dim_retail_sector\n",
    "n_sectors = assert_rowcount(\"mart.dim_retail_sector\", min_rows=3, max_rows=10)\n",
    "assert_unique_key(\"mart.dim_retail_sector\", [\"sector_code\"])\n",
    "qa_rows.append((\"mart.dim_retail_sector\", \"rowcount\", n_sectors))\n",
    "\n",
    "# dim_fuel_group \n",
    "try:\n",
    "    n_fuel_groups = assert_rowcount(\"mart.dim_fuel_group\", min_rows=4, max_rows=20)\n",
    "    assert_unique_key(\"mart.dim_fuel_group\", [\"fuel_group\"])\n",
    "    qa_rows.append((\"mart.dim_fuel_group\", \"rowcount\", n_fuel_groups))\n",
    "except Exception as e:\n",
    "    print(\"Note: mart.dim_fuel_group check skipped:\", e)\n",
    "\n",
    "qa_summary = pd.DataFrame(qa_rows, columns=[\"table\", \"metric\", \"value\"])\n",
    "save_df(qa_summary, \"qa_dim_summary.csv\")\n",
    "display(qa_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e2c20a-fd3e-4b41-8599-339f689d506d",
   "metadata": {},
   "source": [
    "## Fact QA validation\n",
    "\n",
    "I validate core fact tables for row counts, uniqueness at the intended grain, and numeric constraints (e.g., non-negative sales/revenue/customers). This confirms that my curated measures can safely support modeling without structural join artifacts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "96742bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: reports\\qa\\checkpoint_20260110_174056\\qa_fact_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mart.fact_retail_price</td>\n",
       "      <td>rowcount</td>\n",
       "      <td>10404.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mart.fact_retail_price.price_cents_kwh</td>\n",
       "      <td>null_pct</td>\n",
       "      <td>16.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mart.fact_retail_price.sales_mkwh</td>\n",
       "      <td>null_pct</td>\n",
       "      <td>16.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mart.fact_dc_state</td>\n",
       "      <td>rowcount</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     item    metric         value\n",
       "0                  mart.fact_retail_price  rowcount  10404.000000\n",
       "1  mart.fact_retail_price.price_cents_kwh  null_pct     16.666667\n",
       "2       mart.fact_retail_price.sales_mkwh  null_pct     16.666667\n",
       "3                      mart.fact_dc_state  rowcount     30.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qa_fact = []\n",
    "\n",
    "# fact_retail_price\n",
    "n_fr = assert_rowcount(\"mart.fact_retail_price\", min_rows=500)\n",
    "assert_unique_key(\"mart.fact_retail_price\", [\"state_sk\", \"month_sk\", \"sector_sk\"])\n",
    "\n",
    "# sanity checks\n",
    "assert_nonnegative(\"mart.fact_retail_price\", \"price_cents_kwh\")\n",
    "assert_nonnegative(\"mart.fact_retail_price\", \"sales_mkwh\")\n",
    "assert_nonnegative(\"mart.fact_retail_price\", \"revenue_musd\")\n",
    "assert_nonnegative(\"mart.fact_retail_price\", \"customers\")\n",
    "\n",
    "# null thresholds\n",
    "qa_fact.append((\"mart.fact_retail_price\", \"rowcount\", n_fr))\n",
    "qa_fact.append((\"mart.fact_retail_price.price_cents_kwh\", \"null_pct\", assert_null_rate(\"mart.fact_retail_price\", \"price_cents_kwh\", max_null_pct=20.0)))\n",
    "qa_fact.append((\"mart.fact_retail_price.sales_mkwh\", \"null_pct\", assert_null_rate(\"mart.fact_retail_price\", \"sales_mkwh\", max_null_pct=20.0)))\n",
    "\n",
    "# fact_dc_state\n",
    "n_dc = assert_rowcount(\"mart.fact_dc_state\", min_rows=1, max_rows=100)\n",
    "assert_unique_key(\"mart.fact_dc_state\", [\"state_sk\"])\n",
    "assert_nonnegative(\"mart.fact_dc_state\", \"dc_count\")\n",
    "assert_nonnegative(\"mart.fact_dc_state\", \"total_mw\")\n",
    "assert_nonnegative(\"mart.fact_dc_state\", \"total_sqft\")\n",
    "qa_fact.append((\"mart.fact_dc_state\", \"rowcount\", n_dc))\n",
    "\n",
    "qa_fact_df = pd.DataFrame(qa_fact, columns=[\"item\", \"metric\", \"value\"])\n",
    "save_df(qa_fact_df, \"qa_fact_summary.csv\")\n",
    "display(qa_fact_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb06746b-55de-40eb-b0eb-9c11c582e694",
   "metadata": {},
   "source": [
    "## Control-variable QA (generation mix shares)\n",
    "\n",
    "I validate that derived generation mix shares are well-formed (bounded between 0 and 1) and that the controls table has sufficient coverage. This protects the regression stage from subtle math errors that can occur during aggregation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19bfac4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: reports\\qa\\checkpoint_20260110_174056\\qa_controls_share_checks.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>null_share_coal_pct</th>\n",
       "      <th>null_share_gas_pct</th>\n",
       "      <th>null_share_nuclear_pct</th>\n",
       "      <th>null_share_renew_pct</th>\n",
       "      <th>out_of_range_coal</th>\n",
       "      <th>out_of_range_gas</th>\n",
       "      <th>out_of_range_nuclear</th>\n",
       "      <th>out_of_range_renew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.633218</td>\n",
       "      <td>3.633218</td>\n",
       "      <td>3.633218</td>\n",
       "      <td>3.633218</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   null_share_coal_pct  null_share_gas_pct  null_share_nuclear_pct  null_share_renew_pct  out_of_range_coal  out_of_range_gas  \\\n",
       "0             3.633218            3.633218                3.633218              3.633218                  0                 0   \n",
       "\n",
       "   out_of_range_nuclear  out_of_range_renew  \n",
       "0                     0                   0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Controls shares look valid. Rows: 1734\n"
     ]
    }
   ],
   "source": [
    "n_controls = assert_rowcount(\"stage.ops_state_month_controls\", min_rows=500)\n",
    "\n",
    "# shares should be between 0 and 1\n",
    "share_checks = qdf(\"\"\"\n",
    "SELECT\n",
    "  100.0 * AVG(CASE WHEN share_coal   IS NULL THEN 1 ELSE 0 END) AS null_share_coal_pct,\n",
    "  100.0 * AVG(CASE WHEN share_gas    IS NULL THEN 1 ELSE 0 END) AS null_share_gas_pct,\n",
    "  100.0 * AVG(CASE WHEN share_nuclear IS NULL THEN 1 ELSE 0 END) AS null_share_nuclear_pct,\n",
    "  100.0 * AVG(CASE WHEN share_renew  IS NULL THEN 1 ELSE 0 END) AS null_share_renew_pct,\n",
    "  SUM(CASE WHEN share_coal   < 0 OR share_coal   > 1 THEN 1 ELSE 0 END) AS out_of_range_coal,\n",
    "  SUM(CASE WHEN share_gas    < 0 OR share_gas    > 1 THEN 1 ELSE 0 END) AS out_of_range_gas,\n",
    "  SUM(CASE WHEN share_nuclear < 0 OR share_nuclear > 1 THEN 1 ELSE 0 END) AS out_of_range_nuclear,\n",
    "  SUM(CASE WHEN share_renew  < 0 OR share_renew  > 1 THEN 1 ELSE 0 END) AS out_of_range_renew\n",
    "FROM stage.ops_state_month_controls;\n",
    "\"\"\")\n",
    "save_df(share_checks, \"qa_controls_share_checks.csv\")\n",
    "display(share_checks)\n",
    "\n",
    "# Assert no out-of-range shares\n",
    "for col in [\"out_of_range_coal\",\"out_of_range_gas\",\"out_of_range_nuclear\",\"out_of_range_renew\"]:\n",
    "    if int(share_checks.at[0, col]) > 0:\n",
    "        raise AssertionError(f\"stage.ops_state_month_controls: {col} = {share_checks.at[0, col]}\")\n",
    "\n",
    "print(\"✅ Controls shares look valid. Rows:\", n_controls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946a6491-f931-4b20-8785-0e3b99c7feb8",
   "metadata": {},
   "source": [
    "## Analysis view QA (panel structure)\n",
    "\n",
    "I validate that the analysis view has the expected state-month uniqueness and sufficient coverage to behave like a proper panel dataset. I also check null thresholds on key modeling fields so the regression dataset remains stable and defensible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9743946c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_mw null%: 41.1764705882353\n",
      "generation_mwh_total null%: 0.0\n",
      "fuel_cost_proxy null%: 0.0\n",
      "share_gas null%: 3.633217993079585\n",
      "share_coal null%: 3.633217993079585\n",
      "Wrote: reports\\qa\\checkpoint_20260110_174056\\qa_mv_coverage.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_rows</th>\n",
       "      <th>states_present</th>\n",
       "      <th>months_present</th>\n",
       "      <th>expected_state_month</th>\n",
       "      <th>coverage_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1734</td>\n",
       "      <td>51</td>\n",
       "      <td>34</td>\n",
       "      <td>1734</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   actual_rows  states_present  months_present  expected_state_month  coverage_pct\n",
       "0         1734              51              34                  1734         100.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MV analysis rowcount: 1734\n"
     ]
    }
   ],
   "source": [
    "# 1) MV exists and has rows\n",
    "n_mv = assert_rowcount(\"mart.mv_analysis_state_month\", min_rows=200)\n",
    "\n",
    "# 2) Uniqueness at analysis grain\n",
    "assert_unique_key(\"mart.mv_analysis_state_month\", [\"state_code\", \"month_date\"])\n",
    "\n",
    "# 3) Column null thresholds for modeling fields\n",
    "assert_null_rate(\"mart.mv_analysis_state_month\", \"res_price_cents_kwh\", max_null_pct=5.0)\n",
    "\n",
    "# Exposure: total_mw may be null if no DC rows exist in fact_dc_state for that state\n",
    "null_total_mw_pct = scalar(\"\"\"\n",
    "SELECT 100.0 * AVG(CASE WHEN total_mw IS NULL THEN 1 ELSE 0 END)\n",
    "FROM mart.mv_analysis_state_month;\n",
    "\"\"\")\n",
    "print(\"total_mw null%:\", float(null_total_mw_pct))\n",
    "\n",
    "# Controls: allow some null% (EIA ops might be missing in rare rows)\n",
    "for c in [\"generation_mwh_total\", \"fuel_cost_proxy\", \"share_gas\", \"share_coal\"]:\n",
    "    pct = scalar(f\"\"\"\n",
    "    SELECT 100.0 * AVG(CASE WHEN {c} IS NULL THEN 1 ELSE 0 END)\n",
    "    FROM mart.mv_analysis_state_month;\n",
    "    \"\"\")\n",
    "    print(f\"{c} null%:\", float(pct))\n",
    "\n",
    "# 4) Coverage vs expected state-month panel (diagnostic)\n",
    "expected_panel = n_states * n_months\n",
    "\n",
    "coverage = qdf(\"\"\"\n",
    "SELECT\n",
    "  COUNT(*) AS actual_rows,\n",
    "  COUNT(DISTINCT state_code) AS states_present,\n",
    "  COUNT(DISTINCT month_date) AS months_present\n",
    "FROM mart.mv_analysis_state_month;\n",
    "\"\"\")\n",
    "coverage[\"expected_state_month\"] = expected_panel\n",
    "coverage[\"coverage_pct\"] = 100.0 * coverage[\"actual_rows\"] / expected_panel\n",
    "save_df(coverage, \"qa_mv_coverage.csv\")\n",
    "display(coverage)\n",
    "\n",
    "print(\"✅ MV analysis rowcount:\", n_mv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e53905e-34b2-4cf9-8c76-14a888850af6",
   "metadata": {},
   "source": [
    "## Create a model-ready dataset snapshot\n",
    "\n",
    "I pull the full analysis dataset from the materialized view, coerce numeric fields, and write a timestamped parquet snapshot. This locks the exact modeling input for the run and makes the regression step reproducible even if I later refresh the mart.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ca97ea7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: data\\interim\\mv_analysis_state_month_20260110_174056.parquet shape: (1734, 18)\n"
     ]
    }
   ],
   "source": [
    "df_model = qdf(\"\"\"\n",
    "SELECT *\n",
    "FROM mart.mv_analysis_state_month\n",
    "ORDER BY state_code, month_date;\n",
    "\"\"\")\n",
    "\n",
    "numeric_cols = [\n",
    "    \"res_price_cents_kwh\",\"res_sales_mkwh\",\"res_customers\",\n",
    "    \"dc_count\",\"total_mw\",\"total_sqft\",\"avg_mw\",\"mw_per_100k_sqft\",\n",
    "    \"generation_mwh_total\",\"share_coal\",\"share_gas\",\"share_nuclear\",\"share_renew\",\n",
    "    \"fuel_cost_proxy\",\"dc_mw_per_res_mkwh\",\"ln_total_mw\"\n",
    "]\n",
    "for c in numeric_cols:\n",
    "    if c in df_model.columns:\n",
    "        df_model[c] = pd.to_numeric(df_model[c], errors=\"coerce\")\n",
    "\n",
    "out_path = Path(\"data\") / \"interim\" / f\"mv_analysis_state_month_{REPORT_RUN_TS}.parquet\"\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_model.to_parquet(out_path, index=False)\n",
    "print(\"Wrote:\", out_path, \"shape:\", df_model.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3525b925-8023-463e-ab15-124c019683e7",
   "metadata": {},
   "source": [
    "## Regression run orchestration and provenance\n",
    "\n",
    "I set up a run-specific output folder structure (tables/figures/text/data/logs) and write a run manifest that captures timestamp and (when available) the current git commit SHA. This makes the statistical results traceable to the exact code + dataset snapshot used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "813f58e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-10 17:40:58,945 | INFO | Regression run directory: C:\\Users\\amand\\OneDrive\\Documents\\School\\Assignments\\WGU\\Data-Engineering\\Capstone\\datacenter-electricity-prices-us\\notebooks\\outputs\\regression\\run_20260110_174058\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(\"capstone.regression\")\n",
    "if not logger.handlers:\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "\n",
    "# --- Paths (run-isolated) ---\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "OUT_ROOT = PROJECT_ROOT / \"outputs\" / \"regression\"\n",
    "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_DIR = OUT_ROOT / f\"run_{RUN_ID}\"\n",
    "\n",
    "DIRS = {\n",
    "    \"tables\": RUN_DIR / \"tables\",\n",
    "    \"figures\": RUN_DIR / \"figures\",\n",
    "    \"text\": RUN_DIR / \"text\",\n",
    "    \"data\": RUN_DIR / \"data\",\n",
    "    \"logs\": RUN_DIR / \"logs\",\n",
    "}\n",
    "for p in DIRS.values():\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logger.info(f\"Regression run directory: {RUN_DIR}\")\n",
    "\n",
    "# --- DB engine expectation ---\n",
    "try:\n",
    "    engine  # noqa: F821\n",
    "except NameError as e:\n",
    "    raise RuntimeError(\n",
    "        \"SQLAlchemy `engine` is not defined yet. Ensure your earlier DB setup cell ran \"\n",
    "        \"(the one that creates `engine = sa.create_engine(...)`).\"\n",
    "    ) from e\n",
    "\n",
    "def assert_true(cond: bool, msg: str) -> None:\n",
    "    if not cond:\n",
    "        raise AssertionError(msg)\n",
    "\n",
    "def safe_write_text(path: Path, content: str) -> None:\n",
    "    path.write_text(content, encoding=\"utf-8\")\n",
    "\n",
    "def safe_to_parquet(df: pd.DataFrame, path: Path) -> Path:\n",
    "    \"\"\"Write parquet if available; otherwise fall back to CSV and return the written path.\"\"\"\n",
    "    try:\n",
    "        df.to_parquet(path, index=False)\n",
    "        return path\n",
    "    except Exception:\n",
    "        csv_path = path.with_suffix(\".csv\")\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        return csv_path\n",
    "\n",
    "def git_head_sha() -> str | None:\n",
    "    try:\n",
    "        sha = subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"], cwd=PROJECT_ROOT, text=True).strip()\n",
    "        return sha\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "manifest = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"project_root\": str(PROJECT_ROOT),\n",
    "    \"git_sha\": git_head_sha(),\n",
    "}\n",
    "safe_write_text(DIRS[\"text\"] / \"run_manifest.json\", json.dumps(manifest, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b06a038-7d13-4b29-be0d-5d520f2a8bc5",
   "metadata": {},
   "source": [
    "## Optional mart refresh control\n",
    "\n",
    "I include an explicit switch to refresh the mart/materialized view from SQL using psql when needed. This gives me a clean separation between “data build” and “modeling run,” while keeping refresh behavior intentional rather than implicit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2b65dc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-10 17:40:59,166 | INFO | REFRESH_MART=False → skipping mart refresh.\n"
     ]
    }
   ],
   "source": [
    "REFRESH_MART = False  \n",
    "SQL_DIR = PROJECT_ROOT / \"sql\"\n",
    "sql_files = [\n",
    "    SQL_DIR / \"09_mart__load_dimensions.sql\",\n",
    "    SQL_DIR / \"10_mart__load_facts.sql\",\n",
    "    SQL_DIR / \"11_mart__mv_analysis_refresh.sql\",\n",
    "]\n",
    "\n",
    "def run_psql_file(path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Executes a SQL file with psql, stopping on first error.\n",
    "    Uses PG* env vars (do not print secrets).\n",
    "    \"\"\"\n",
    "    assert_true(path.exists(), f\"SQL file not found: {path}\")\n",
    "\n",
    "    cmd = [\n",
    "        \"psql\",\n",
    "        \"-h\", PGHOST,\n",
    "        \"-p\", str(PGPORT),\n",
    "        \"-U\", PGUSER,\n",
    "        \"-d\", PGDATABASE,\n",
    "        \"-v\", \"ON_ERROR_STOP=1\",\n",
    "        \"-f\", str(path),\n",
    "    ]\n",
    "    env = os.environ.copy()\n",
    "    env[\"PGPASSWORD\"] = PGPASSWORD\n",
    "\n",
    "    logger.info(f\"psql -f {path.name}\")\n",
    "    proc = subprocess.run(cmd, env=env, capture_output=True, text=True)\n",
    "    if proc.returncode != 0:\n",
    "        raise RuntimeError(f\"psql failed for {path.name}:\\n{proc.stderr}\")\n",
    "    if proc.stderr.strip():\n",
    "        logger.warning(proc.stderr.strip())\n",
    "\n",
    "if REFRESH_MART:\n",
    "    for f in sql_files:\n",
    "        run_psql_file(f)\n",
    "    logger.info(\"✅ Mart load + MV refresh complete.\")\n",
    "else:\n",
    "    logger.info(\"REFRESH_MART=False → skipping mart refresh.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa06d317-1c20-431f-9680-4ef9d4d0b4e3",
   "metadata": {},
   "source": [
    "## Load regression input from the curated mart\n",
    "\n",
    "I query the analysis view for the specific modeling fields I need and write a run-local parquet snapshot. I also include a defensive fallback for the exposure measure to ensure the regression can proceed even if the view structure evolves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f67b650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-10 17:40:59,334 | INFO | Wrote dataset snapshot: C:\\Users\\amand\\OneDrive\\Documents\\School\\Assignments\\WGU\\Data-Engineering\\Capstone\\datacenter-electricity-prices-us\\notebooks\\outputs\\regression\\run_20260110_174058\\data\\mv_analysis_state_month.parquet\n",
      "2026-01-10 17:40:59,336 | INFO | mv_analysis_state_month shape: (1734, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_code</th>\n",
       "      <th>month_date</th>\n",
       "      <th>ym</th>\n",
       "      <th>res_price_cents_kwh</th>\n",
       "      <th>res_sales_mkwh</th>\n",
       "      <th>res_customers</th>\n",
       "      <th>dc_count</th>\n",
       "      <th>total_mw</th>\n",
       "      <th>total_sqft</th>\n",
       "      <th>avg_mw</th>\n",
       "      <th>mw_per_100k_sqft</th>\n",
       "      <th>generation_mwh_total</th>\n",
       "      <th>share_coal</th>\n",
       "      <th>share_gas</th>\n",
       "      <th>share_nuclear</th>\n",
       "      <th>share_renew</th>\n",
       "      <th>fuel_cost_proxy</th>\n",
       "      <th>dc_mw_per_res_mkwh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AK</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>21.73</td>\n",
       "      <td>230.49790</td>\n",
       "      <td>295151.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>569121.67</td>\n",
       "      <td>0.069071</td>\n",
       "      <td>0.514055</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.264194</td>\n",
       "      <td>0.426905</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AK</td>\n",
       "      <td>2023-02-01</td>\n",
       "      <td>2023-02</td>\n",
       "      <td>22.92</td>\n",
       "      <td>178.92844</td>\n",
       "      <td>294835.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>517944.72</td>\n",
       "      <td>0.044791</td>\n",
       "      <td>0.540090</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.286934</td>\n",
       "      <td>0.826461</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AK</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>2023-03</td>\n",
       "      <td>23.45</td>\n",
       "      <td>196.24384</td>\n",
       "      <td>295298.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>533502.11</td>\n",
       "      <td>0.087022</td>\n",
       "      <td>0.526931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.270481</td>\n",
       "      <td>0.825519</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AK</td>\n",
       "      <td>2023-04-01</td>\n",
       "      <td>2023-04</td>\n",
       "      <td>23.44</td>\n",
       "      <td>166.71102</td>\n",
       "      <td>294846.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>487187.82</td>\n",
       "      <td>0.079070</td>\n",
       "      <td>0.524473</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.240792</td>\n",
       "      <td>0.826277</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AK</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>2023-05</td>\n",
       "      <td>24.91</td>\n",
       "      <td>151.96408</td>\n",
       "      <td>295873.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>441835.35</td>\n",
       "      <td>0.081966</td>\n",
       "      <td>0.488562</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.259393</td>\n",
       "      <td>0.762815</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state_code month_date       ym  res_price_cents_kwh  res_sales_mkwh  res_customers  dc_count  total_mw total_sqft  avg_mw  \\\n",
       "0         AK 2023-01-01  2023-01                21.73       230.49790       295151.0       NaN       NaN       None     NaN   \n",
       "1         AK 2023-02-01  2023-02                22.92       178.92844       294835.0       NaN       NaN       None     NaN   \n",
       "2         AK 2023-03-01  2023-03                23.45       196.24384       295298.0       NaN       NaN       None     NaN   \n",
       "3         AK 2023-04-01  2023-04                23.44       166.71102       294846.0       NaN       NaN       None     NaN   \n",
       "4         AK 2023-05-01  2023-05                24.91       151.96408       295873.0       NaN       NaN       None     NaN   \n",
       "\n",
       "  mw_per_100k_sqft  generation_mwh_total  share_coal  share_gas  share_nuclear  share_renew  fuel_cost_proxy  dc_mw_per_res_mkwh  \n",
       "0             None             569121.67    0.069071   0.514055            0.0     0.264194         0.426905                 0.0  \n",
       "1             None             517944.72    0.044791   0.540090            0.0     0.286934         0.826461                 0.0  \n",
       "2             None             533502.11    0.087022   0.526931            0.0     0.270481         0.825519                 0.0  \n",
       "3             None             487187.82    0.079070   0.524473            0.0     0.240792         0.826277                 0.0  \n",
       "4             None             441835.35    0.081966   0.488562            0.0     0.259393         0.762815                 0.0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# Pull model-ready dataset from mart.mv_analysis_state_month\n",
    "# =========================\n",
    "\n",
    "QUERY = \"\"\"\n",
    "SELECT\n",
    "  state_code,\n",
    "  month_date,\n",
    "  ym,\n",
    "  res_price_cents_kwh,\n",
    "  res_sales_mkwh,\n",
    "  res_customers,\n",
    "  dc_count,\n",
    "  total_mw,\n",
    "  total_sqft,\n",
    "  avg_mw,\n",
    "  mw_per_100k_sqft,\n",
    "  generation_mwh_total,\n",
    "  share_coal,\n",
    "  share_gas,\n",
    "  share_nuclear,\n",
    "  share_renew,\n",
    "  fuel_cost_proxy,\n",
    "  dc_mw_per_res_mkwh\n",
    "FROM mart.mv_analysis_state_month\n",
    "ORDER BY state_code, month_date;\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(text(QUERY), engine, parse_dates=[\"month_date\"])\n",
    "\n",
    "# Robust fallback if MV ever lacks exposure\n",
    "if \"dc_mw_per_res_mkwh\" not in df.columns and {\"total_mw\", \"res_sales_mkwh\"}.issubset(df.columns):\n",
    "    df[\"dc_mw_per_res_mkwh\"] = np.where(\n",
    "        (df[\"res_sales_mkwh\"].isna()) | (df[\"res_sales_mkwh\"] == 0),\n",
    "        np.nan,\n",
    "        df[\"total_mw\"] / df[\"res_sales_mkwh\"]\n",
    "    )\n",
    "\n",
    "written = safe_to_parquet(df, DIRS[\"data\"] / \"mv_analysis_state_month.parquet\")\n",
    "logger.info(f\"Wrote dataset snapshot: {written}\")\n",
    "logger.info(f\"mv_analysis_state_month shape: {df.shape}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd5e1a8-0490-4af1-b5c7-cf878b4fe6e6",
   "metadata": {},
   "source": [
    "## Regression dataset QA\n",
    "\n",
    "Before modeling, I enforce required columns, confirm uniqueness at the state-month grain, and apply null-rate thresholds for the outcome, exposure, and controls. This checkpoint ensures the regression results reflect data issues transparently rather than masking them in the estimator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5cff56-6896-4d97-b352-0445c62bbddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Checkpoint QA: grain, uniqueness, null thresholds, and domain checks\n",
    "# =========================\n",
    "\n",
    "required_cols = [\n",
    "    \"state_code\", \"month_date\",\n",
    "    \"res_price_cents_kwh\",\n",
    "    \"dc_mw_per_res_mkwh\",\n",
    "    \"generation_mwh_total\",\n",
    "    \"share_coal\", \"share_gas\", \"share_nuclear\",\n",
    "    \"fuel_cost_proxy\",\n",
    "]\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "assert_true(not missing, f\"Missing required columns in MV: {missing}\")\n",
    "\n",
    "dup = int(df.duplicated(subset=[\"state_code\", \"month_date\"]).sum())\n",
    "assert_true(dup == 0, f\"Duplicate (state_code, month_date) rows in MV: {dup}\")\n",
    "\n",
    "assert_true(df[\"state_code\"].astype(str).str.fullmatch(r\"[A-Z]{2}\").all(), \"state_code invalid.\")\n",
    "assert_true(pd.to_datetime(df[\"month_date\"], errors=\"coerce\").notna().all(), \"month_date unparsable.\")\n",
    "assert_true((df[\"res_price_cents_kwh\"].dropna() > 0).all(), \"Found non-positive prices.\")\n",
    "\n",
    "for c in [\"share_coal\", \"share_gas\", \"share_nuclear\", \"share_renew\"]:\n",
    "    s = df[c].dropna()\n",
    "    if len(s):\n",
    "        bad = ((s < 0) | (s > 1)).sum()\n",
    "        assert_true(bad == 0, f\"{c} has {bad} values outside [0,1].\")\n",
    "\n",
    "null_rates = df.isna().mean().sort_values(ascending=False)\n",
    "null_rates.to_csv(DIRS[\"tables\"] / \"qa_null_rates.csv\")\n",
    "\n",
    "NULL_THRESH = {\n",
    "    \"res_price_cents_kwh\": 0.05,\n",
    "    \"dc_mw_per_res_mkwh\": 0.25,\n",
    "    \"generation_mwh_total\": 0.10,\n",
    "    \"fuel_cost_proxy\": 0.25,\n",
    "    \"share_coal\": 0.25,\n",
    "    \"share_gas\": 0.25,\n",
    "    \"share_nuclear\": 0.25,\n",
    "}\n",
    "\n",
    "for col, thresh in NULL_THRESH.items():\n",
    "    rate = float(df[col].isna().mean())\n",
    "    assert_true(rate <= thresh, f\"Null rate too high for {col}: {rate:.2%} > {thresh:.2%}\")\n",
    "\n",
    "qa_summary = {\n",
    "    \"rows\": int(df.shape[0]),\n",
    "    \"cols\": int(df.shape[1]),\n",
    "    \"states\": int(df[\"state_code\"].nunique()),\n",
    "    \"months\": int(df[\"month_date\"].nunique()),\n",
    "    \"null_thresholds\": NULL_THRESH,\n",
    "    \"null_rates\": {k: float(df[k].isna().mean()) for k in NULL_THRESH},\n",
    "}\n",
    "safe_write_text(DIRS[\"text\"] / \"qa_summary.json\", json.dumps(qa_summary, indent=2))\n",
    "\n",
    "logger.info(\"✅ QA checks passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aeb3e0-d9cc-4e39-9fe1-de1bed1274d0",
   "metadata": {},
   "source": [
    "## Descriptive summary for the analysis story (Table 1)\n",
    "\n",
    "I produce a Table 1 style descriptive summary of the model variables, both overall and split by whether a state has any operating data centers. This sets context for the regression by showing baseline distributions and helps readers interpret effect sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8deeb5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-10 17:40:59,570 | INFO | Wrote Table 1: C:\\Users\\amand\\OneDrive\\Documents\\School\\Assignments\\WGU\\Data-Engineering\\Capstone\\datacenter-electricity-prices-us\\notebooks\\outputs\\regression\\run_20260110_174058\\tables\\table1_model_ready.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>group</th>\n",
       "      <th>n</th>\n",
       "      <th>pct_missing</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>p25</th>\n",
       "      <th>median</th>\n",
       "      <th>p75</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>res_price_cents_kwh</td>\n",
       "      <td>Overall</td>\n",
       "      <td>1734.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.244769</td>\n",
       "      <td>6.442070</td>\n",
       "      <td>9.270000</td>\n",
       "      <td>13.152500</td>\n",
       "      <td>14.910000</td>\n",
       "      <td>18.397500</td>\n",
       "      <td>45.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dc_mw_per_res_mkwh</td>\n",
       "      <td>Overall</td>\n",
       "      <td>1734.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.167903</td>\n",
       "      <td>0.459301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009027</td>\n",
       "      <td>0.114525</td>\n",
       "      <td>4.101417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ln_generation_mwh_total</td>\n",
       "      <td>Overall</td>\n",
       "      <td>1734.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.392357</td>\n",
       "      <td>3.726264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.085443</td>\n",
       "      <td>14.895369</td>\n",
       "      <td>15.562025</td>\n",
       "      <td>17.078796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>share_coal</td>\n",
       "      <td>Overall</td>\n",
       "      <td>1671.0</td>\n",
       "      <td>3.633218</td>\n",
       "      <td>0.220587</td>\n",
       "      <td>0.258375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127965</td>\n",
       "      <td>0.358885</td>\n",
       "      <td>0.981543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>share_gas</td>\n",
       "      <td>Overall</td>\n",
       "      <td>1671.0</td>\n",
       "      <td>3.633218</td>\n",
       "      <td>0.357086</td>\n",
       "      <td>0.268646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131545</td>\n",
       "      <td>0.327905</td>\n",
       "      <td>0.527069</td>\n",
       "      <td>0.999086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>share_nuclear</td>\n",
       "      <td>Overall</td>\n",
       "      <td>1671.0</td>\n",
       "      <td>3.633218</td>\n",
       "      <td>0.110718</td>\n",
       "      <td>0.167583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.226358</td>\n",
       "      <td>0.638601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fuel_cost_proxy</td>\n",
       "      <td>Overall</td>\n",
       "      <td>1734.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.010775</td>\n",
       "      <td>0.512534</td>\n",
       "      <td>-1.164549</td>\n",
       "      <td>-0.196730</td>\n",
       "      <td>-0.003175</td>\n",
       "      <td>0.111120</td>\n",
       "      <td>11.390130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>res_price_cents_kwh</td>\n",
       "      <td>No DC in state</td>\n",
       "      <td>714.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.856611</td>\n",
       "      <td>7.964229</td>\n",
       "      <td>9.750000</td>\n",
       "      <td>13.050000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>23.917500</td>\n",
       "      <td>45.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dc_mw_per_res_mkwh</td>\n",
       "      <td>No DC in state</td>\n",
       "      <td>714.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ln_generation_mwh_total</td>\n",
       "      <td>No DC in state</td>\n",
       "      <td>714.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.493560</td>\n",
       "      <td>4.794280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.132993</td>\n",
       "      <td>13.550820</td>\n",
       "      <td>14.839594</td>\n",
       "      <td>17.078796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>share_coal</td>\n",
       "      <td>No DC in state</td>\n",
       "      <td>651.0</td>\n",
       "      <td>8.823529</td>\n",
       "      <td>0.237297</td>\n",
       "      <td>0.300990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070715</td>\n",
       "      <td>0.429573</td>\n",
       "      <td>0.981543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>share_gas</td>\n",
       "      <td>No DC in state</td>\n",
       "      <td>651.0</td>\n",
       "      <td>8.823529</td>\n",
       "      <td>0.266106</td>\n",
       "      <td>0.273366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>0.156746</td>\n",
       "      <td>0.472510</td>\n",
       "      <td>0.965764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   variable           group       n  pct_missing       mean       std       min        p25     median        p75  \\\n",
       "0       res_price_cents_kwh         Overall  1734.0     0.000000  17.244769  6.442070  9.270000  13.152500  14.910000  18.397500   \n",
       "1        dc_mw_per_res_mkwh         Overall  1734.0     0.000000   0.167903  0.459301  0.000000   0.000000   0.009027   0.114525   \n",
       "2   ln_generation_mwh_total         Overall  1734.0     0.000000  13.392357  3.726264  0.000000  13.085443  14.895369  15.562025   \n",
       "3                share_coal         Overall  1671.0     3.633218   0.220587  0.258375  0.000000   0.000000   0.127965   0.358885   \n",
       "4                 share_gas         Overall  1671.0     3.633218   0.357086  0.268646  0.000000   0.131545   0.327905   0.527069   \n",
       "5             share_nuclear         Overall  1671.0     3.633218   0.110718  0.167583  0.000000   0.000000   0.000000   0.226358   \n",
       "6           fuel_cost_proxy         Overall  1734.0     0.000000  -0.010775  0.512534 -1.164549  -0.196730  -0.003175   0.111120   \n",
       "7       res_price_cents_kwh  No DC in state   714.0     0.000000  18.856611  7.964229  9.750000  13.050000  15.500000  23.917500   \n",
       "8        dc_mw_per_res_mkwh  No DC in state   714.0     0.000000   0.000000  0.000000  0.000000   0.000000   0.000000   0.000000   \n",
       "9   ln_generation_mwh_total  No DC in state   714.0     0.000000  11.493560  4.794280  0.000000   9.132993  13.550820  14.839594   \n",
       "10               share_coal  No DC in state   651.0     8.823529   0.237297  0.300990  0.000000   0.000000   0.070715   0.429573   \n",
       "11                share_gas  No DC in state   651.0     8.823529   0.266106  0.273366  0.000000   0.005172   0.156746   0.472510   \n",
       "\n",
       "          max  \n",
       "0   45.390000  \n",
       "1    4.101417  \n",
       "2   17.078796  \n",
       "3    0.981543  \n",
       "4    0.999086  \n",
       "5    0.638601  \n",
       "6   11.390130  \n",
       "7   45.390000  \n",
       "8    0.000000  \n",
       "9   17.078796  \n",
       "10   0.981543  \n",
       "11   0.965764  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# Table 1: model-ready descriptive summary (overall + by DC presence)\n",
    "# =========================\n",
    "\n",
    "outcome = \"res_price_cents_kwh\"\n",
    "exposure = \"dc_mw_per_res_mkwh\"\n",
    "\n",
    "df1 = df.copy()\n",
    "df1[\"ln_generation_mwh_total\"] = np.log1p(df1[\"generation_mwh_total\"])\n",
    "df1[\"has_dc_state\"] = (df1[\"dc_count\"].fillna(0) > 0).astype(int)\n",
    "\n",
    "model_vars = [\n",
    "    outcome,\n",
    "    exposure,\n",
    "    \"ln_generation_mwh_total\",\n",
    "    \"share_coal\", \"share_gas\", \"share_nuclear\",\n",
    "    \"fuel_cost_proxy\",\n",
    "]\n",
    "\n",
    "def _summarize_series(x: pd.Series) -> pd.Series:\n",
    "    return pd.Series({\n",
    "        \"n\": int(x.notna().sum()),\n",
    "        \"pct_missing\": float(100.0 * x.isna().mean()),\n",
    "        \"mean\": float(x.mean()) if x.notna().any() else np.nan,\n",
    "        \"std\": float(x.std()) if x.notna().any() else np.nan,\n",
    "        \"min\": float(x.min()) if x.notna().any() else np.nan,\n",
    "        \"p25\": float(x.quantile(0.25)) if x.notna().any() else np.nan,\n",
    "        \"median\": float(x.median()) if x.notna().any() else np.nan,\n",
    "        \"p75\": float(x.quantile(0.75)) if x.notna().any() else np.nan,\n",
    "        \"max\": float(x.max()) if x.notna().any() else np.nan,\n",
    "    })\n",
    "\n",
    "def build_table1(dfin: pd.DataFrame, vars_: list[str], group_col: str) -> pd.DataFrame:\n",
    "    blocks = []\n",
    "    overall = pd.concat({v: _summarize_series(dfin[v]) for v in vars_}, axis=1).T\n",
    "    overall.insert(0, \"group\", \"Overall\")\n",
    "    blocks.append(overall)\n",
    "\n",
    "    for gval, gname in [(0, \"No DC in state\"), (1, \"Has DC in state\")]:\n",
    "        sub = dfin[dfin[group_col] == gval]\n",
    "        tab = pd.concat({v: _summarize_series(sub[v]) for v in vars_}, axis=1).T\n",
    "        tab.insert(0, \"group\", gname)\n",
    "        blocks.append(tab)\n",
    "\n",
    "    out = pd.concat(blocks, axis=0)\n",
    "    out.insert(0, \"variable\", out.index)\n",
    "    out.reset_index(drop=True, inplace=True)\n",
    "    return out\n",
    "\n",
    "table1 = build_table1(df1, model_vars, \"has_dc_state\")\n",
    "table1_path = DIRS[\"tables\"] / \"table1_model_ready.csv\"\n",
    "table1.to_csv(table1_path, index=False)\n",
    "logger.info(f\"Wrote Table 1: {table1_path}\")\n",
    "\n",
    "table1.head(12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15674f8d-9b47-4968-9ca6-3919e58ba948",
   "metadata": {},
   "source": [
    "## Prepare panel structure for fixed-effects regression\n",
    "\n",
    "I construct the panel dataset, clean numeric fields, drop rows missing required modeling variables, and winsorize the exposure to reduce sensitivity to extreme values. I then set a MultiIndex (state, month) so fixed effects can be absorbed cleanly by the estimator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea7ccd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-10 17:40:59,634 | INFO | Rows kept for regression: 1,671 / 1,734\n",
      "2026-01-10 17:40:59,684 | INFO | Wrote modeling dataset snapshot: C:\\Users\\amand\\OneDrive\\Documents\\School\\Assignments\\WGU\\Data-Engineering\\Capstone\\datacenter-electricity-prices-us\\notebooks\\outputs\\regression\\run_20260110_174058\\data\\panel_regression_dataset.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>res_price_cents_kwh</th>\n",
       "      <th>dc_mw_per_res_mkwh</th>\n",
       "      <th>ln_generation_mwh_total</th>\n",
       "      <th>share_coal</th>\n",
       "      <th>share_gas</th>\n",
       "      <th>share_nuclear</th>\n",
       "      <th>fuel_cost_proxy</th>\n",
       "      <th>dc_mw_per_res_mkwh_w</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state_code</th>\n",
       "      <th>month_date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">AK</th>\n",
       "      <th>2023-01-01</th>\n",
       "      <td>21.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.251851</td>\n",
       "      <td>0.069071</td>\n",
       "      <td>0.514055</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.426905</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-01</th>\n",
       "      <td>22.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.157626</td>\n",
       "      <td>0.044791</td>\n",
       "      <td>0.540090</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.826461</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-01</th>\n",
       "      <td>23.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.187220</td>\n",
       "      <td>0.087022</td>\n",
       "      <td>0.526931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.825519</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-01</th>\n",
       "      <td>23.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.096407</td>\n",
       "      <td>0.079070</td>\n",
       "      <td>0.524473</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.826277</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-01</th>\n",
       "      <td>24.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.998695</td>\n",
       "      <td>0.081966</td>\n",
       "      <td>0.488562</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.762815</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       res_price_cents_kwh  dc_mw_per_res_mkwh  ln_generation_mwh_total  share_coal  share_gas  share_nuclear  \\\n",
       "state_code month_date                                                                                                           \n",
       "AK         2023-01-01                21.73                 0.0                13.251851    0.069071   0.514055            0.0   \n",
       "           2023-02-01                22.92                 0.0                13.157626    0.044791   0.540090            0.0   \n",
       "           2023-03-01                23.45                 0.0                13.187220    0.087022   0.526931            0.0   \n",
       "           2023-04-01                23.44                 0.0                13.096407    0.079070   0.524473            0.0   \n",
       "           2023-05-01                24.91                 0.0                12.998695    0.081966   0.488562            0.0   \n",
       "\n",
       "                       fuel_cost_proxy  dc_mw_per_res_mkwh_w  \n",
       "state_code month_date                                         \n",
       "AK         2023-01-01         0.426905                   0.0  \n",
       "           2023-02-01         0.826461                   0.0  \n",
       "           2023-03-01         0.825519                   0.0  \n",
       "           2023-04-01         0.826277                   0.0  \n",
       "           2023-05-01         0.762815                   0.0  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# Prepare panel dataset (MultiIndex) + mild winsorization of exposure\n",
    "# =========================\n",
    "\n",
    "reg = df1[[\"state_code\", \"month_date\"] + model_vars].copy()\n",
    "\n",
    "for c in model_vars:\n",
    "    reg[c] = pd.to_numeric(reg[c], errors=\"coerce\")\n",
    "reg = reg.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "reg = reg.dropna(subset=[outcome])\n",
    "\n",
    "def winsorize(s: pd.Series, p: float = 0.01) -> pd.Series:\n",
    "    lo = s.quantile(p)\n",
    "    hi = s.quantile(1 - p)\n",
    "    return s.clip(lower=lo, upper=hi)\n",
    "\n",
    "reg[exposure + \"_w\"] = winsorize(reg[exposure], p=0.01)\n",
    "\n",
    "needed = [exposure + \"_w\", \"ln_generation_mwh_total\", \"share_coal\", \"share_gas\", \"share_nuclear\", \"fuel_cost_proxy\"]\n",
    "before = len(reg)\n",
    "reg = reg.dropna(subset=needed)\n",
    "logger.info(f\"Rows kept for regression: {len(reg):,} / {before:,}\")\n",
    "\n",
    "within_sd = reg.groupby(\"state_code\")[exposure + \"_w\"].std()\n",
    "assert_true((within_sd.fillna(0) > 0).any(), \"Exposure constant within every state → will be absorbed by FE.\")\n",
    "\n",
    "reg = reg.set_index([\"state_code\", \"month_date\"]).sort_index()\n",
    "\n",
    "written = safe_to_parquet(reg.reset_index(), DIRS[\"data\"] / \"panel_regression_dataset.parquet\")\n",
    "logger.info(f\"Wrote modeling dataset snapshot: {written}\")\n",
    "\n",
    "reg.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bbf60d-0992-4a83-8ca2-e3fc68a87ab1",
   "metadata": {},
   "source": [
    "## Primary model: two-way fixed effects panel regression\n",
    "\n",
    "I fit a PanelOLS model with state fixed effects and month fixed effects, using robust clustering as my primary standard-error specification. I export a clean coefficient table (with confidence intervals) so results can be reported consistently and reused in the automated PDF output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e92904e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-10 17:40:59,727 | INFO | PanelOLS formula: res_price_cents_kwh ~ 1 + dc_mw_per_res_mkwh_w + ln_generation_mwh_total + share_coal + share_gas + share_nuclear + fuel_cost_proxy + EntityEffects + TimeEffects\n",
      "2026-01-10 17:41:00,198 | WARNING | Kernel covariance fit failed (skipping): Covariance estimator DriscollKraay only supports the keyword arguments: kernel, bandwidth. Unknown keyword arguments were passed to the estimator. The unknown keyword argument(s) are: cov_config \n",
      "2026-01-10 17:41:00,210 | INFO | Wrote coefficients: C:\\Users\\amand\\OneDrive\\Documents\\School\\Assignments\\WGU\\Data-Engineering\\Capstone\\datacenter-electricity-prices-us\\notebooks\\outputs\\regression\\run_20260110_174058\\tables\\panelols_coefficients_clustered.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PanelOLS Estimation Summary                           \n",
      "=================================================================================\n",
      "Dep. Variable:     res_price_cents_kwh   R-squared:                        0.0209\n",
      "Estimator:                    PanelOLS   R-squared (Between):             -0.1883\n",
      "No. Observations:                 1671   R-squared (Within):               0.0087\n",
      "Date:                 Sat, Jan 10 2026   R-squared (Overall):             -0.1356\n",
      "Time:                         17:40:59   Log-likelihood                   -2451.9\n",
      "Cov. Estimator:              Clustered                                           \n",
      "                                         F-statistic:                      5.6315\n",
      "Entities:                           51   P-value                           0.0000\n",
      "Avg Obs:                        32.765   Distribution:                  F(6,1581)\n",
      "Min Obs:                        6.0000                                           \n",
      "Max Obs:                        34.000   F-statistic (robust):             1.1335\n",
      "                                         P-value                           0.3402\n",
      "Time periods:                       34   Distribution:                  F(6,1581)\n",
      "Avg Obs:                        49.147                                           \n",
      "Min Obs:                        47.000                                           \n",
      "Max Obs:                        51.000                                           \n",
      "                                                                                 \n",
      "                                    Parameter Estimates                                    \n",
      "===========================================================================================\n",
      "                         Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "-------------------------------------------------------------------------------------------\n",
      "Intercept                   12.453     3.2755     3.8019     0.0001      6.0285      18.878\n",
      "dc_mw_per_res_mkwh_w        0.8675     0.5408     1.6040     0.1089     -0.1933      1.9284\n",
      "ln_generation_mwh_total     0.3512     0.2587     1.3576     0.1748     -0.1562      0.8585\n",
      "share_coal                 -0.4226     1.4467    -0.2921     0.7702     -3.2603      2.4151\n",
      "share_gas                  -1.0273     0.9335    -1.1004     0.2713     -2.8584      0.8038\n",
      "share_nuclear              -0.9960     1.1134    -0.8945     0.3712     -3.1799      1.1879\n",
      "fuel_cost_proxy             0.1496     0.1586     0.9434     0.3456     -0.1615      0.4607\n",
      "===========================================================================================\n",
      "\n",
      "F-test for Poolability: 462.78\n",
      "P-value: 0.0000\n",
      "Distribution: F(83,1581)\n",
      "\n",
      "Included effects: Entity, Time\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param</th>\n",
       "      <th>coef</th>\n",
       "      <th>std_err</th>\n",
       "      <th>t_stat</th>\n",
       "      <th>p_value</th>\n",
       "      <th>ci_95_low</th>\n",
       "      <th>ci_95_high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>12.453402</td>\n",
       "      <td>3.275539</td>\n",
       "      <td>3.801940</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>6.028546</td>\n",
       "      <td>18.878259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dc_mw_per_res_mkwh_w</td>\n",
       "      <td>0.867527</td>\n",
       "      <td>0.540843</td>\n",
       "      <td>1.604028</td>\n",
       "      <td>0.108908</td>\n",
       "      <td>-0.193317</td>\n",
       "      <td>1.928371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ln_generation_mwh_total</td>\n",
       "      <td>0.351158</td>\n",
       "      <td>0.258651</td>\n",
       "      <td>1.357650</td>\n",
       "      <td>0.174769</td>\n",
       "      <td>-0.156178</td>\n",
       "      <td>0.858494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>share_coal</td>\n",
       "      <td>-0.422601</td>\n",
       "      <td>1.446734</td>\n",
       "      <td>-0.292107</td>\n",
       "      <td>0.770243</td>\n",
       "      <td>-3.260321</td>\n",
       "      <td>2.415118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>share_gas</td>\n",
       "      <td>-1.027313</td>\n",
       "      <td>0.933540</td>\n",
       "      <td>-1.100449</td>\n",
       "      <td>0.271304</td>\n",
       "      <td>-2.858420</td>\n",
       "      <td>0.803794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>share_nuclear</td>\n",
       "      <td>-0.995976</td>\n",
       "      <td>1.113411</td>\n",
       "      <td>-0.894527</td>\n",
       "      <td>0.371176</td>\n",
       "      <td>-3.179893</td>\n",
       "      <td>1.187941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fuel_cost_proxy</td>\n",
       "      <td>0.149625</td>\n",
       "      <td>0.158596</td>\n",
       "      <td>0.943438</td>\n",
       "      <td>0.345601</td>\n",
       "      <td>-0.161455</td>\n",
       "      <td>0.460706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     param       coef   std_err    t_stat   p_value  ci_95_low  ci_95_high\n",
       "0                Intercept  12.453402  3.275539  3.801940  0.000149   6.028546   18.878259\n",
       "1     dc_mw_per_res_mkwh_w   0.867527  0.540843  1.604028  0.108908  -0.193317    1.928371\n",
       "2  ln_generation_mwh_total   0.351158  0.258651  1.357650  0.174769  -0.156178    0.858494\n",
       "3               share_coal  -0.422601  1.446734 -0.292107  0.770243  -3.260321    2.415118\n",
       "4                share_gas  -1.027313  0.933540 -1.100449  0.271304  -2.858420    0.803794\n",
       "5            share_nuclear  -0.995976  1.113411 -0.894527  0.371176  -3.179893    1.187941\n",
       "6          fuel_cost_proxy   0.149625  0.158596  0.943438  0.345601  -0.161455    0.460706"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# Fit PanelOLS: state FE + month FE (primary: 2-way clustered SE)\n",
    "# =========================\n",
    "\n",
    "formula = (\n",
    "    f\"{outcome} ~ 1 \"\n",
    "    f\"+ {exposure}_w \"\n",
    "    \"+ ln_generation_mwh_total \"\n",
    "    \"+ share_coal + share_gas + share_nuclear \"\n",
    "    \"+ fuel_cost_proxy \"\n",
    "    \"+ EntityEffects + TimeEffects\"\n",
    ")\n",
    "\n",
    "logger.info(f\"PanelOLS formula: {formula}\")\n",
    "\n",
    "mod = PanelOLS.from_formula(\n",
    "    formula,\n",
    "    data=reg,\n",
    "    drop_absorbed=True,\n",
    "    check_rank=True,\n",
    ")\n",
    "\n",
    "res_cluster = mod.fit(cov_type=\"clustered\", cluster_entity=True, cluster_time=True)\n",
    "\n",
    "print(res_cluster.summary)\n",
    "safe_write_text(DIRS[\"text\"] / \"panelols_summary_clustered.txt\", str(res_cluster.summary))\n",
    "\n",
    "try:\n",
    "    res_kernel = mod.fit(cov_type=\"kernel\", cov_config={\"kernel\": \"bartlett\", \"bandwidth\": 6})\n",
    "    safe_write_text(DIRS[\"text\"] / \"panelols_summary_kernel_bartlett_bw6.txt\", str(res_kernel.summary))\n",
    "except Exception as e:\n",
    "    res_kernel = None\n",
    "    logger.warning(f\"Kernel covariance fit failed (skipping): {e}\")\n",
    "\n",
    "coef_tbl = pd.DataFrame({\n",
    "    \"param\": res_cluster.params.index,\n",
    "    \"coef\": res_cluster.params.astype(float).values,\n",
    "    \"std_err\": res_cluster.std_errors.astype(float).values,\n",
    "    \"t_stat\": res_cluster.tstats.astype(float).values,\n",
    "    \"p_value\": res_cluster.pvalues.astype(float).values,\n",
    "})\n",
    "ci = res_cluster.conf_int().astype(float)\n",
    "coef_tbl[\"ci_95_low\"] = ci.iloc[:, 0].values\n",
    "coef_tbl[\"ci_95_high\"] = ci.iloc[:, 1].values\n",
    "\n",
    "coef_path = DIRS[\"tables\"] / \"panelols_coefficients_clustered.csv\"\n",
    "coef_tbl.to_csv(coef_path, index=False)\n",
    "logger.info(f\"Wrote coefficients: {coef_path}\")\n",
    "\n",
    "coef_tbl.head(12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4604f-f594-4e2b-a7bd-163d17dda87d",
   "metadata": {},
   "source": [
    "## Hypothesis test and interpretable effect size\n",
    "\n",
    "I evaluate the one-sided hypothesis on the exposure coefficient (β > 0) and record the decision at my chosen alpha level. To make the result actionable, I translate the coefficient into an interpretable scenario (e.g., moving from the 25th to 75th percentile exposure and computing the implied change in cents/kWh with a 95% CI).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a42dfc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-10 17:41:00,262 | INFO | Decision: FAIL TO REJECT | p_one=0.05445 | beta=0.867527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.05,\n",
       " 'hypothesis': 'H0: beta <= 0 vs H1: beta > 0',\n",
       " 'param': 'dc_mw_per_res_mkwh_w',\n",
       " 'beta': 0.8675269341103441,\n",
       " 'std_error': 0.5408426001594229,\n",
       " 't_stat': 1.604028480475882,\n",
       " 'p_two_sided': 0.1089075064893541,\n",
       " 'p_one_sided_beta_gt_0': 0.05445375324467705,\n",
       " 'reject_H0': False,\n",
       " 'nobs': 1671,\n",
       " 'states': 51,\n",
       " 'months': 34,\n",
       " 'exposure_p25': 0.0,\n",
       " 'exposure_p75': 0.11924634016614011,\n",
       " 'exposure_delta_p75_minus_p25': 0.11924634016614011,\n",
       " 'implied_delta_price_cents_per_kwh': 0.1034494118882107,\n",
       " 'implied_delta_price_ci95_low': -0.023052371250940774,\n",
       " 'implied_delta_price_ci95_high': 0.2299511950273622}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# Automated hypothesis test (one-sided) + interpretable effect size\n",
    "# =========================\n",
    "\n",
    "ALPHA = 0.05\n",
    "param = f\"{exposure}_w\"\n",
    "assert_true(param in res_cluster.params.index, f\"Exposure param '{param}' not found.\")\n",
    "\n",
    "beta = float(res_cluster.params[param])\n",
    "se = float(res_cluster.std_errors[param])\n",
    "t = float(res_cluster.tstats[param])\n",
    "p_two = float(res_cluster.pvalues[param])\n",
    "\n",
    "p_one = (p_two / 2.0) if beta >= 0 else (1.0 - p_two / 2.0)\n",
    "reject = bool((beta > 0) and (p_one < ALPHA))\n",
    "\n",
    "x = reg[param].dropna()\n",
    "p25, p75 = float(x.quantile(0.25)), float(x.quantile(0.75))\n",
    "delta_x = p75 - p25\n",
    "delta_price = beta * delta_x\n",
    "\n",
    "ci = res_cluster.conf_int().loc[param].astype(float)\n",
    "beta_lo, beta_hi = float(ci.iloc[0]), float(ci.iloc[1])\n",
    "delta_lo, delta_hi = beta_lo * delta_x, beta_hi * delta_x\n",
    "\n",
    "hyp = {\n",
    "    \"alpha\": ALPHA,\n",
    "    \"hypothesis\": \"H0: beta <= 0 vs H1: beta > 0\",\n",
    "    \"param\": param,\n",
    "    \"beta\": beta,\n",
    "    \"std_error\": se,\n",
    "    \"t_stat\": t,\n",
    "    \"p_two_sided\": p_two,\n",
    "    \"p_one_sided_beta_gt_0\": p_one,\n",
    "    \"reject_H0\": reject,\n",
    "    \"nobs\": int(res_cluster.nobs),\n",
    "    \"states\": int(reg.reset_index()[\"state_code\"].nunique()),\n",
    "    \"months\": int(reg.reset_index()[\"month_date\"].nunique()),\n",
    "    \"exposure_p25\": p25,\n",
    "    \"exposure_p75\": p75,\n",
    "    \"exposure_delta_p75_minus_p25\": delta_x,\n",
    "    \"implied_delta_price_cents_per_kwh\": delta_price,\n",
    "    \"implied_delta_price_ci95_low\": delta_lo,\n",
    "    \"implied_delta_price_ci95_high\": delta_hi,\n",
    "}\n",
    "\n",
    "safe_write_text(DIRS[\"tables\"] / \"hypothesis_test.json\", json.dumps(hyp, indent=2))\n",
    "\n",
    "logger.info(f\"Decision: {'REJECT' if reject else 'FAIL TO REJECT'} | p_one={p_one:.4g} | beta={beta:.6g}\")\n",
    "hyp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d601a20-82d7-4a23-879a-b668e73d8862",
   "metadata": {},
   "source": [
    "## Residual diagnostics (quantitative checks)\n",
    "\n",
    "I compute fitted values and residuals and summarize residual behavior numerically, including lag-1 autocorrelation patterns across states. This gives me quick evidence about whether error structure might motivate clustered or kernel/HAC-style standard errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "55f151ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-10 17:41:00,327 | INFO | Wrote diagnostics dataset: C:\\Users\\amand\\OneDrive\\Documents\\School\\Assignments\\WGU\\Data-Engineering\\Capstone\\datacenter-electricity-prices-us\\notebooks\\outputs\\regression\\run_20260110_174058\\data\\diagnostics_fitted_resid.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "resid_mean                            1.103127e-14\n",
       "resid_std                             1.049913e+00\n",
       "lag1_autocorr_mean_across_states      6.706805e-01\n",
       "lag1_autocorr_median_across_states    6.946309e-01\n",
       "n_states_with_lag1_corr               5.100000e+01\n",
       "dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# Residual diagnostics (figures + numeric summaries)\n",
    "# =========================\n",
    "\n",
    "fitted = res_cluster.fitted_values\n",
    "resid = res_cluster.resids\n",
    "\n",
    "fitted_s = fitted.iloc[:, 0] if isinstance(fitted, pd.DataFrame) else fitted\n",
    "resid_s = resid.iloc[:, 0] if isinstance(resid, pd.DataFrame) else resid\n",
    "\n",
    "diag = pd.DataFrame({\"fitted\": fitted_s.astype(float), \"resid\": resid_s.astype(float)}).reset_index()\n",
    "diag = diag.rename(columns={\"entity\": \"state_code\", \"time\": \"month_date\"})\n",
    "\n",
    "actual = reg[[outcome]].reset_index().rename(columns={outcome: \"actual\"})\n",
    "diag = diag.merge(actual, on=[\"state_code\", \"month_date\"], how=\"left\")\n",
    "\n",
    "written = safe_to_parquet(diag, DIRS[\"data\"] / \"diagnostics_fitted_resid.parquet\")\n",
    "logger.info(f\"Wrote diagnostics dataset: {written}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(diag[\"resid\"].dropna(), bins=40)\n",
    "plt.title(\"Residual distribution (PanelOLS)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(DIRS[\"figures\"] / \"residual_hist.png\", dpi=200)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "stats.probplot(diag[\"resid\"].dropna(), dist=\"norm\", plot=plt)\n",
    "plt.title(\"QQ plot of residuals\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(DIRS[\"figures\"] / \"residual_qq.png\", dpi=200)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(diag[\"fitted\"], diag[\"resid\"], s=10)\n",
    "plt.axhline(0, linewidth=1)\n",
    "plt.title(\"Residuals vs fitted\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(DIRS[\"figures\"] / \"residual_vs_fitted.png\", dpi=200)\n",
    "plt.show()\n",
    "\n",
    "by_month = diag.groupby(\"month_date\", as_index=False)[\"resid\"].mean().sort_values(\"month_date\")\n",
    "plt.figure()\n",
    "plt.plot(by_month[\"month_date\"], by_month[\"resid\"])\n",
    "plt.axhline(0, linewidth=1)\n",
    "plt.title(\"Mean residual over time\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(DIRS[\"figures\"] / \"mean_residual_over_time.png\", dpi=200)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "monthly_mean_resid = by_month.set_index(\"month_date\")[\"resid\"].dropna()\n",
    "\n",
    "if len(monthly_mean_resid) >= 3:\n",
    "    lags = min(24, len(monthly_mean_resid) - 1)  # e.g., up to 2 years of monthly lags\n",
    "    fig, ax = plt.subplots()\n",
    "    plot_acf(monthly_mean_resid, lags=lags, zero=False, ax=ax)\n",
    "    ax.axhline(0, linewidth=1)\n",
    "    ax.set_title(\"ACF: monthly mean residuals\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(DIRS[\"figures\"] / \"acf_monthly_mean_residual.png\", dpi=200)\n",
    "    plt.show()\n",
    "else:\n",
    "    logger.warning(\"Skipping ACF plot: not enough monthly residual points.\")\n",
    "\n",
    "\n",
    "diag_sorted = diag.sort_values([\"state_code\", \"month_date\"]).copy()\n",
    "diag_sorted[\"resid_lag1\"] = diag_sorted.groupby(\"state_code\")[\"resid\"].shift(1)\n",
    "\n",
    "def safe_corr(g: pd.DataFrame) -> float:\n",
    "    g2 = g.dropna(subset=[\"resid\", \"resid_lag1\"])\n",
    "    if len(g2) < 3:\n",
    "        return np.nan\n",
    "    return float(np.corrcoef(g2[\"resid\"], g2[\"resid_lag1\"])[0, 1])\n",
    "\n",
    "ac1 = diag_sorted.groupby(\"state_code\").apply(safe_corr).dropna()\n",
    "diag_numeric = {\n",
    "    \"resid_mean\": float(diag[\"resid\"].mean()),\n",
    "    \"resid_std\": float(diag[\"resid\"].std()),\n",
    "    \"lag1_autocorr_mean_across_states\": float(ac1.mean()) if len(ac1) else None,\n",
    "    \"lag1_autocorr_median_across_states\": float(ac1.median()) if len(ac1) else None,\n",
    "    \"n_states_with_lag1_corr\": int(len(ac1)),\n",
    "}\n",
    "safe_write_text(DIRS[\"tables\"] / \"residual_diagnostics_numeric.json\", json.dumps(diag_numeric, indent=2))\n",
    "\n",
    "pd.Series(diag_numeric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1534247-4150-4de9-a3d6-8efec31f8405",
   "metadata": {},
   "source": [
    "## Run narrative for reporting\n",
    "\n",
    "I automatically generate a concise narrative summary of the hypothesis test result, key coefficient values, and the practical effect-size translation. Writing this to disk as an artifact ensures the final report language matches the exact run outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c5f1fefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression & Hypothesis Testing (PanelOLS)\n",
      "\n",
      "Model:\n",
      "Two-way fixed effects panel regression with state fixed effects and month fixed effects.\n",
      "Outcome: residential price (cents/kWh).\n",
      "Exposure: data center load intensity (MW per million kWh of residential sales).\n",
      "Controls: log(1+generation), fuel mix shares (renew omitted baseline), fuel cost proxy.\n",
      "\n",
      "Estimation:\n",
      "- PanelOLS with EntityEffects + TimeEffects\n",
      "- SEs: two-way clustered by state and month\n",
      "- N=1671 across 51 states and 34 months\n",
      "\n",
      "Result:\n",
      "beta=0.867527 (SE=0.540843)\n",
      "two-sided p=0.108908\n",
      "one-sided p=0.054454 → fail to reject H0 at alpha=0.05\n",
      "\n",
      "Effect size (P25→P75 exposure):\n",
      "Δx=0.119246 implies Δprice≈0.1034 cents/kWh\n",
      "(95% CI≈[-0.0231, 0.2300])\n",
      "\n",
      "Artifacts written to:\n",
      "C:\\Users\\amand\\OneDrive\\Documents\\School\\Assignments\\WGU\\Data-Engineering\\Capstone\\datacenter-electricity-prices-us\\notebooks\\outputs\\regression\\run_20260110_174058\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Auto-generated narrative block\n",
    "# =========================\n",
    "\n",
    "def fmt(x, digits=4):\n",
    "    if x is None:\n",
    "        return \"NA\"\n",
    "    try:\n",
    "        xf = float(x)\n",
    "        if np.isnan(xf):\n",
    "            return \"NA\"\n",
    "        return f\"{xf:.{digits}f}\"\n",
    "    except Exception:\n",
    "        return str(x)\n",
    "\n",
    "decision_txt = \"reject\" if hyp[\"reject_H0\"] else \"fail to reject\"\n",
    "\n",
    "narrative = f\"\"\"\n",
    "Regression & Hypothesis Testing (PanelOLS)\n",
    "\n",
    "Model:\n",
    "Two-way fixed effects panel regression with state fixed effects and month fixed effects.\n",
    "Outcome: residential price (cents/kWh).\n",
    "Exposure: data center load intensity (MW per million kWh of residential sales).\n",
    "Controls: log(1+generation), fuel mix shares (renew omitted baseline), fuel cost proxy.\n",
    "\n",
    "Estimation:\n",
    "- PanelOLS with EntityEffects + TimeEffects\n",
    "- SEs: two-way clustered by state and month\n",
    "- N={hyp[\"nobs\"]} across {hyp[\"states\"]} states and {hyp[\"months\"]} months\n",
    "\n",
    "Result:\n",
    "beta={fmt(hyp[\"beta\"], 6)} (SE={fmt(hyp[\"std_error\"], 6)})\n",
    "two-sided p={fmt(hyp[\"p_two_sided\"], 6)}\n",
    "one-sided p={fmt(hyp[\"p_one_sided_beta_gt_0\"], 6)} → {decision_txt} H0 at alpha={hyp[\"alpha\"]}\n",
    "\n",
    "Effect size (P25→P75 exposure):\n",
    "Δx={fmt(hyp[\"exposure_delta_p75_minus_p25\"], 6)} implies Δprice≈{fmt(hyp[\"implied_delta_price_cents_per_kwh\"], 4)} cents/kWh\n",
    "(95% CI≈[{fmt(hyp[\"implied_delta_price_ci95_low\"], 4)}, {fmt(hyp[\"implied_delta_price_ci95_high\"], 4)}])\n",
    "\n",
    "Artifacts written to:\n",
    "{RUN_DIR}\n",
    "\"\"\".strip()\n",
    "\n",
    "safe_write_text(DIRS[\"text\"] / \"results_narrative.txt\", narrative)\n",
    "print(narrative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f7a88c-ae78-4a96-b48a-982b4b14a33d",
   "metadata": {},
   "source": [
    "## Sensitivity checks (robustness of inference)\n",
    "\n",
    "I re-fit the same model under multiple covariance specifications (robust, clustered, kernel/HAC-style) and export a comparison table. This is a targeted robustness step to show whether inference about the exposure coefficient is stable to reasonable assumptions about the error structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9daf8694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-10 17:41:03,394 | INFO | Wrote sensitivity table: C:\\Users\\amand\\OneDrive\\Documents\\School\\Assignments\\WGU\\Data-Engineering\\Capstone\\datacenter-electricity-prices-us\\notebooks\\outputs\\regression\\run_20260110_174058\\tables\\panelols_sensitivity.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spec</th>\n",
       "      <th>beta</th>\n",
       "      <th>std_err</th>\n",
       "      <th>p_value_two_sided</th>\n",
       "      <th>nobs</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robust</td>\n",
       "      <td>0.867527</td>\n",
       "      <td>0.230868</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>1671.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cluster_state</td>\n",
       "      <td>0.867527</td>\n",
       "      <td>0.550125</td>\n",
       "      <td>0.115004</td>\n",
       "      <td>1671.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cluster_state_month</td>\n",
       "      <td>0.867527</td>\n",
       "      <td>0.540843</td>\n",
       "      <td>0.108908</td>\n",
       "      <td>1671.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kernel_bartlett_bw6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Covariance estimator DriscollKraay only suppor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  spec      beta   std_err  p_value_two_sided    nobs                                              error\n",
       "0               robust  0.867527  0.230868           0.000178  1671.0                                                NaN\n",
       "1        cluster_state  0.867527  0.550125           0.115004  1671.0                                                NaN\n",
       "2  cluster_state_month  0.867527  0.540843           0.108908  1671.0                                                NaN\n",
       "3  kernel_bartlett_bw6       NaN       NaN                NaN     NaN  Covariance estimator DriscollKraay only suppor..."
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# Sensitivity specs\n",
    "# =========================\n",
    "\n",
    "specs = [\n",
    "    (\"robust\", dict(cov_type=\"robust\")),\n",
    "    (\"cluster_state\", dict(cov_type=\"clustered\", cluster_entity=True, cluster_time=False)),\n",
    "    (\"cluster_state_month\", dict(cov_type=\"clustered\", cluster_entity=True, cluster_time=True)),\n",
    "    (\"kernel_bartlett_bw6\", dict(cov_type=\"kernel\", cov_config={\"kernel\": \"bartlett\", \"bandwidth\": 6})),\n",
    "]\n",
    "\n",
    "rows = []\n",
    "for name, kwargs in specs:\n",
    "    try:\n",
    "        r = mod.fit(**kwargs)\n",
    "        rows.append({\n",
    "            \"spec\": name,\n",
    "            \"beta\": float(r.params.get(f\"{exposure}_w\", np.nan)),\n",
    "            \"std_err\": float(r.std_errors.get(f\"{exposure}_w\", np.nan)),\n",
    "            \"p_value_two_sided\": float(r.pvalues.get(f\"{exposure}_w\", np.nan)),\n",
    "            \"nobs\": int(r.nobs),\n",
    "        })\n",
    "    except Exception as e:\n",
    "        rows.append({\"spec\": name, \"beta\": np.nan, \"std_err\": np.nan, \"p_value_two_sided\": np.nan, \"nobs\": np.nan, \"error\": str(e)})\n",
    "\n",
    "sens = pd.DataFrame(rows)\n",
    "sens_path = DIRS[\"tables\"] / \"panelols_sensitivity.csv\"\n",
    "sens.to_csv(sens_path, index=False)\n",
    "logger.info(f\"Wrote sensitivity table: {sens_path}\")\n",
    "\n",
    "sens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646207ca-7212-43a8-a404-2fbdbca708c7",
   "metadata": {},
   "source": [
    "## Automated PDF report assembly (setup)\n",
    "\n",
    "I locate the most recent regression run folder and define the paths to the tables, figures, text artifacts, and dataset snapshot. This design keeps reporting decoupled from modeling: the report is assembled from saved artifacts, not from in-memory state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7bfc6aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPORT_RUN_DIR: C:\\Users\\amand\\OneDrive\\Documents\\School\\Assignments\\WGU\\Data-Engineering\\Capstone\\datacenter-electricity-prices-us\\notebooks\\outputs\\regression\\run_20260110_174058\n",
      "OUT_PDF: C:\\Users\\amand\\OneDrive\\Documents\\School\\Assignments\\WGU\\Data-Engineering\\Capstone\\datacenter-electricity-prices-us\\notebooks\\outputs\\regression\\run_20260110_174058\\report_regression_results.pdf\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PDF Builder \n",
    "# =============================================================================\n",
    "_outputs_root = Path.cwd() / \"outputs\" / \"regression\"\n",
    "\n",
    "def _pick_latest_run_dir(root: Path) -> Path:\n",
    "    if not root.exists():\n",
    "        raise FileNotFoundError(f\"Outputs root not found: {root}\")\n",
    "    runs = sorted([p for p in root.glob(\"run_*\") if p.is_dir()])\n",
    "    if not runs:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No run_* folders found under: {root}. \"\n",
    "            \"Run the regression section first to generate artifacts.\"\n",
    "        )\n",
    "    return runs[-1]\n",
    "\n",
    "try:\n",
    "    REPORT_RUN_DIR = Path(RUN_DIR)\n",
    "except NameError:\n",
    "    REPORT_RUN_DIR = _pick_latest_run_dir(_outputs_root)\n",
    "\n",
    "REPORT_RUN_DIR = Path(REPORT_RUN_DIR).resolve()\n",
    "if not REPORT_RUN_DIR.exists():\n",
    "    raise FileNotFoundError(f\"REPORT_RUN_DIR does not exist: {REPORT_RUN_DIR}\")\n",
    "\n",
    "REPORT_TABLES = REPORT_RUN_DIR / \"tables\"\n",
    "REPORT_FIGS   = REPORT_RUN_DIR / \"figures\"\n",
    "REPORT_TEXT   = REPORT_RUN_DIR / \"text\"\n",
    "REPORT_DATA   = REPORT_RUN_DIR / \"data\"\n",
    "\n",
    "# Ensure expected folders exist\n",
    "for _p in [REPORT_TABLES, REPORT_FIGS, REPORT_TEXT, REPORT_DATA]:\n",
    "    _p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_PDF = REPORT_RUN_DIR / \"report_regression_results.pdf\"\n",
    "\n",
    "print(\"REPORT_RUN_DIR:\", REPORT_RUN_DIR)\n",
    "print(\"OUT_PDF:\", OUT_PDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7754fd-7077-47b5-ae51-67de35c09dbc",
   "metadata": {},
   "source": [
    "## Generate the final regression PDF (artifact-driven)\n",
    "\n",
    "I compile the run artifacts into a polished PDF report: executive summary narrative, Table 1, coefficient table, model summary, and diagnostic figures. Building the report from saved tables/figures/text makes the output reproducible and aligns the final narrative with the exact results generated by the run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bba9ee6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PDF generated: C:\\Users\\amand\\OneDrive\\Documents\\School\\Assignments\\WGU\\Data-Engineering\\Capstone\\datacenter-electricity-prices-us\\notebooks\\outputs\\regression\\run_20260110_174058\\report_regression_results.pdf\n",
      "PDF size (MB): 0.31\n"
     ]
    }
   ],
   "source": [
    "def safe_read_text(path: Path, default: str = \"\") -> str:\n",
    "    try:\n",
    "        return path.read_text(encoding=\"utf-8\")\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def safe_read_csv(path: Path) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def fmt_num(x):\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    if isinstance(x, (int, np.integer)):\n",
    "        return f\"{int(x):,}\"\n",
    "    if isinstance(x, (float, np.floating)):\n",
    "        ax = abs(float(x))\n",
    "        if ax >= 1000:\n",
    "            return f\"{float(x):,.2f}\"\n",
    "        if ax >= 10:\n",
    "            return f\"{float(x):.3f}\"\n",
    "        return f\"{float(x):.4f}\"\n",
    "    return str(x)\n",
    "\n",
    "def df_to_rl_table(df: pd.DataFrame, doc_width: float, max_rows: int = 60) -> Table:\n",
    "    \"\"\"Convert a DataFrame to a ReportLab Table with safe sizing.\"\"\"\n",
    "    if df.empty:\n",
    "        return Table([[\"(No data found)\"]])\n",
    "\n",
    "    df2 = df.copy()\n",
    "\n",
    "    # Trim long tables so the PDF stays readable\n",
    "    if len(df2) > max_rows:\n",
    "        df2 = df2.iloc[:max_rows].copy()\n",
    "        df2.loc[len(df2)] = [\"…\"] + [\"…\"] * (df2.shape[1] - 1)\n",
    "\n",
    "    data = [list(df2.columns)]\n",
    "    for _, row in df2.iterrows():\n",
    "        data.append([fmt_num(v) for v in row.values])\n",
    "\n",
    "    ncols = len(df2.columns)\n",
    "    col_widths = [doc_width / ncols] * ncols\n",
    "\n",
    "    tbl = Table(data, repeatRows=1, colWidths=col_widths)\n",
    "    tbl.hAlign = \"LEFT\"\n",
    "\n",
    "    tbl.setStyle(TableStyle([\n",
    "        (\"FONTNAME\", (0,0), (-1,0), \"Helvetica-Bold\"),\n",
    "        (\"FONTSIZE\", (0,0), (-1,0), 8),\n",
    "        (\"BACKGROUND\", (0,0), (-1,0), colors.HexColor(\"#f1f5f9\")),\n",
    "        (\"TEXTCOLOR\", (0,0), (-1,0), colors.HexColor(\"#0f172a\")),\n",
    "        (\"LINEBELOW\", (0,0), (-1,0), 1, colors.HexColor(\"#cbd5e1\")),\n",
    "\n",
    "        (\"FONTNAME\", (0,1), (-1,-1), \"Helvetica\"),\n",
    "        (\"FONTSIZE\", (0,1), (-1,-1), 7),\n",
    "        (\"TEXTCOLOR\", (0,1), (-1,-1), colors.HexColor(\"#111827\")),\n",
    "\n",
    "        (\"GRID\", (0,0), (-1,-1), 0.25, colors.HexColor(\"#e2e8f0\")),\n",
    "        (\"VALIGN\", (0,0), (-1,-1), \"TOP\"),\n",
    "        (\"LEFTPADDING\", (0,0), (-1,-1), 3),\n",
    "        (\"RIGHTPADDING\", (0,0), (-1,-1), 3),\n",
    "        (\"TOPPADDING\", (0,0), (-1,-1), 2),\n",
    "        (\"BOTTOMPADDING\", (0,0), (-1,-1), 2),\n",
    "    ]))\n",
    "\n",
    "    # Right-align numeric-looking columns (heuristic)\n",
    "    for c in range(1, ncols):\n",
    "        col_vals = [str(v) for v in df2.iloc[:, c].head(15).values]\n",
    "        numericish = sum(bool(re.match(r\"^\\s*[-+]?\\d[\\d,]*\\.?\\d*\\s*$\", v)) for v in col_vals) >= max(5, len(col_vals)//2)\n",
    "        if numericish:\n",
    "            tbl.setStyle(TableStyle([(\"ALIGN\", (c,1), (c,-1), \"RIGHT\")]))\n",
    "\n",
    "    return tbl\n",
    "\n",
    "def add_fig(story, img_path: Path, caption: str, max_width_in: float = 6.7):\n",
    "    if not img_path.exists():\n",
    "        story.append(Paragraph(f\"<b>Figure:</b> Missing image: {img_path.name}\", styles[\"Body\"]))\n",
    "        story.append(Spacer(1, 0.15*inch))\n",
    "        return\n",
    "    img = Image(str(img_path))\n",
    "    max_w = max_width_in * inch\n",
    "    if img.drawWidth > max_w:\n",
    "        scale = max_w / img.drawWidth\n",
    "        img.drawWidth *= scale\n",
    "        img.drawHeight *= scale\n",
    "    story.append(img)\n",
    "    story.append(Spacer(1, 0.08*inch))\n",
    "    story.append(Paragraph(f\"<i>{caption}</i>\", styles[\"Caption\"]))\n",
    "    story.append(Spacer(1, 0.20*inch))\n",
    "\n",
    "# ---------- Styles ----------\n",
    "base = getSampleStyleSheet()\n",
    "styles = {\n",
    "    \"Title\": ParagraphStyle(\"Title\", parent=base[\"Title\"], fontName=\"Helvetica-Bold\", fontSize=20, leading=24,\n",
    "                           textColor=colors.HexColor(\"#0f172a\"), spaceAfter=12),\n",
    "    \"H1\": ParagraphStyle(\"H1\", parent=base[\"Heading1\"], fontName=\"Helvetica-Bold\", fontSize=14, leading=18,\n",
    "                        textColor=colors.HexColor(\"#0f172a\"), spaceBefore=10, spaceAfter=8),\n",
    "    \"Body\": ParagraphStyle(\"Body\", parent=base[\"BodyText\"], fontName=\"Helvetica\", fontSize=9.5, leading=13,\n",
    "                          textColor=colors.HexColor(\"#111827\"), spaceAfter=6),\n",
    "    \"Caption\": ParagraphStyle(\"Caption\", parent=base[\"BodyText\"], fontName=\"Helvetica-Oblique\", fontSize=8.5,\n",
    "                             leading=11, textColor=colors.HexColor(\"#334155\"), spaceAfter=8),\n",
    "    \"Mono\": ParagraphStyle(\"Mono\", parent=base[\"BodyText\"], fontName=\"Courier\", fontSize=7.5, leading=9.5,\n",
    "                          textColor=colors.HexColor(\"#0b1220\")),\n",
    "    \"Meta\": ParagraphStyle(\"Meta\", parent=base[\"BodyText\"], fontName=\"Helvetica\", fontSize=9, leading=12,\n",
    "                          textColor=colors.HexColor(\"#334155\")),\n",
    "}\n",
    "\n",
    "PROJECT_TITLE = \"Quantifying the Association Between Data Center Presence and Retail Electricity Prices\"\n",
    "SUBTITLE = \"Regression & Hypothesis Testing Report (Automated Output)\"\n",
    "CREATED_AT = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# ---------- Load artifacts ----------\n",
    "table1_path = REPORT_TABLES / \"table1_model_ready.csv\"\n",
    "coef_path   = REPORT_TABLES / \"panelols_coefficients_clustered.csv\"\n",
    "narr_path   = REPORT_TEXT   / \"results_narrative.txt\"\n",
    "sum_path    = REPORT_TEXT   / \"panelols_summary_clustered.txt\"\n",
    "\n",
    "df_table1 = safe_read_csv(table1_path)\n",
    "df_coef   = safe_read_csv(coef_path)\n",
    "narrative = safe_read_text(narr_path, default=\"(results narrative not found)\")\n",
    "summary_txt = safe_read_text(sum_path, default=\"(model summary not found)\")\n",
    "\n",
    "# ---------- Header/Footer ----------\n",
    "def _header_footer(canvas, doc):\n",
    "    canvas.saveState()\n",
    "    canvas.setFont(\"Helvetica\", 8)\n",
    "    canvas.setFillColor(colors.HexColor(\"#475569\"))\n",
    "    canvas.drawString(0.75*inch, 0.55*inch, f\"{PROJECT_TITLE}  •  Generated {CREATED_AT}\")\n",
    "    canvas.drawRightString(7.75*inch, 0.55*inch, f\"Page {canvas.getPageNumber()}\")\n",
    "    canvas.restoreState()\n",
    "\n",
    "# ---------- Build PDF ----------\n",
    "OUT_PDF.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "doc = SimpleDocTemplate(\n",
    "    str(OUT_PDF),\n",
    "    pagesize=LETTER,\n",
    "    leftMargin=0.85*inch,\n",
    "    rightMargin=0.85*inch,\n",
    "    topMargin=0.85*inch,\n",
    "    bottomMargin=0.85*inch,\n",
    "    title=PROJECT_TITLE,\n",
    "    author=\"WGU Capstone (Automated Notebook Output)\",\n",
    ")\n",
    "\n",
    "story = []\n",
    "\n",
    "# Cover\n",
    "story.append(Paragraph(PROJECT_TITLE, styles[\"Title\"]))\n",
    "story.append(Paragraph(SUBTITLE, styles[\"Meta\"]))\n",
    "story.append(Spacer(1, 0.15*inch))\n",
    "story.append(Paragraph(f\"<b>Artifact folder:</b> {REPORT_RUN_DIR}\", styles[\"Meta\"]))\n",
    "story.append(Paragraph(\n",
    "    \"This PDF is generated automatically from the notebook run artifacts (tables, figures, and model output). \"\n",
    "    \"It is intended to be a polished, reproducible input to the final written capstone report.\",\n",
    "    styles[\"Body\"]\n",
    "))\n",
    "story.append(PageBreak())\n",
    "\n",
    "# Executive Summary / Narrative\n",
    "story.append(Paragraph(\"Executive Summary\", styles[\"H1\"]))\n",
    "for para in [p.strip() for p in narrative.split(\"\\n\\n\") if p.strip()]:\n",
    "    story.append(Paragraph(para.replace(\"\\n\", \"<br/>\"), styles[\"Body\"]))\n",
    "story.append(PageBreak())\n",
    "\n",
    "# Table 1\n",
    "story.append(Paragraph(\"Table 1. Model-Ready Descriptive Summary\", styles[\"H1\"]))\n",
    "story.append(Spacer(1, 0.10*inch))\n",
    "story.append(df_to_rl_table(df_table1, doc.width, max_rows=60))\n",
    "story.append(Spacer(1, 0.15*inch))\n",
    "story.append(Paragraph(f\"<i>Source:</i> {table1_path.name}\", styles[\"Caption\"]))\n",
    "story.append(PageBreak())\n",
    "\n",
    "# Regression coefficients\n",
    "story.append(Paragraph(\"Panel Regression Results (Coefficients)\", styles[\"H1\"]))\n",
    "story.append(Spacer(1, 0.10*inch))\n",
    "story.append(df_to_rl_table(df_coef, doc.width, max_rows=40))\n",
    "story.append(Spacer(1, 0.15*inch))\n",
    "story.append(Paragraph(f\"<i>Source:</i> {coef_path.name}\", styles[\"Caption\"]))\n",
    "story.append(PageBreak())\n",
    "\n",
    "# Model summary\n",
    "story.append(Paragraph(\"Model Summary (PanelOLS)\", styles[\"H1\"]))\n",
    "story.append(Spacer(1, 0.10*inch))\n",
    "story.append(Preformatted(summary_txt, styles[\"Mono\"]))\n",
    "story.append(PageBreak())\n",
    "\n",
    "# Figures\n",
    "story.append(Paragraph(\"Diagnostics Figures\", styles[\"H1\"]))\n",
    "story.append(Spacer(1, 0.10*inch))\n",
    "\n",
    "fig_items = [\n",
    "    (\"residual_hist.png\", \"Figure 1. Histogram of regression residuals.\"),\n",
    "    (\"residual_qq.png\", \"Figure 2. Q-Q plot of residuals (normality check).\"),\n",
    "    (\"residual_vs_fitted.png\", \"Figure 3. Residuals vs fitted values.\"),\n",
    "    (\"mean_residual_over_time.png\", \"Figure 4. Mean residual over time.\"),\n",
    "    (\"acf_monthly_mean_residual.png\", \"Figure 5. ACF of monthly mean residuals.\"),\n",
    "]\n",
    "\n",
    "for fname, cap in fig_items:\n",
    "    add_fig(story, REPORT_FIGS / fname, cap)\n",
    "\n",
    "doc.build(story, onFirstPage=_header_footer, onLaterPages=_header_footer)\n",
    "\n",
    "# Post-build verification\n",
    "assert OUT_PDF.exists(), f\"PDF build did not produce output file: {OUT_PDF}\"\n",
    "print(f\"✅ PDF generated: {OUT_PDF}\")\n",
    "print(\"PDF size (MB):\", round(OUT_PDF.stat().st_size / (1024**2), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c7002-08cf-4925-8dbb-39cf5a7db727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WGU Capstone (dc-capstone)",
   "language": "python",
   "name": "dc-capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
